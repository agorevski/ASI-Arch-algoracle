"""
DeltaNet: Gated Renormalized Linear Attention with Chunked Streaming
Evolved architecture implementing content-adaptive retention, S/Z normalization,
RoPE positional encoding, pre-LN RMSNorm, and SwiGLU FFN.
Maintains sub-quadratic complexity and strict causality with chunk-wise processing.
"""

from typing import Optional

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange

# Safe torch.compile import
try:
    torch_compile = torch.compile  # type: ignore[attr-defined]
except Exception:  # pragma: no cover
    def torch_compile(fn=None, **kwargs):
        if fn is None:
            def wrapper(f):
                return f
            return wrapper
        return fn


class RMSNorm(nn.Module):
    """Root Mean Square Layer Normalization (pre-LN style).
    Keeps mean-free scaling without centering, good for deep networks.
    """
    def __init__(self, dim: int, eps: float = 1e-6, **kwargs):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: [*, dim]
        dim = x.shape[-1]
        rms = torch.rsqrt(torch.mean(x.pow(2), dim=-1, keepdim=True) + self.eps)
        x_norm = x * rms
        return x_norm * self.weight


def rotate_half(x: torch.Tensor) -> torch.Tensor:
    # Split last dim into two halves and rotate
    x1, x2 = torch.chunk(x, 2, dim=-1)
    return torch.cat((-x2, x1), dim=-1)


def apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:
    # x: [b, h, t, d], cos/sin: [1, 1, t, d]
    # Broadcast cos/sin across batch and heads
    return (x * cos) + (rotate_half(x) * sin)


def build_rope_cache(t: int, d: int, device: torch.device, base: float = 10000.0) -> tuple:
    # Build cos/sin cache for RoPE. d must be even.
    half = d // 2
    # frequencies
    inv_freq = 1.0 / (base ** (torch.arange(0, half, device=device, dtype=torch.float32) / half))
    # positions
    positions = torch.arange(t, device=device, dtype=torch.float32)
    freqs = torch.einsum('t,d->td', positions, inv_freq)  # [t, half]
    # Cast to the computation dtype later
    cos = torch.cos(freqs).unsqueeze(1)  # [t, 1, half]
    sin = torch.sin(freqs).unsqueeze(1)  # [t, 1, half]
    # Expand to full d using interleaving convention
    cos = torch.stack([cos, cos], dim=-1).reshape(t, 1, d)
    sin = torch.stack([sin, sin], dim=-1).reshape(t, 1, d)
    return cos, sin


def phi_feature_map(x: torch.Tensor) -> torch.Tensor:
    """Positive feature map for linear attention stability.
    We use ELU + 1 as in Favor+ to ensure positivity.
    x: [..., d]
    returns: [..., d]
    """
    return F.elu(x, alpha=1.0) + 1.0


class DeltaNet(nn.Module):
    """Gated, Renormalized Linear Attention Layer with Chunked Streaming.

    Key features:
    - Content- and channel-wise retention r_t in [0,1] per head and per feature
    - S/Z renormalization accumulators for numerical stability
    - RoPE positional encoding on q/k
    - Chunk-wise causal processing
    - Batch-size agnostic, sub-quadratic complexity
    """

    def __init__(self, hidden_size: int, num_heads: int = 8, dropout: float = 0.1, **kwargs):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        assert self.head_dim * num_heads == hidden_size, (
            f"hidden_size {hidden_size} not divisible by num_heads {num_heads}")

        # Projections
        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.k_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.v_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.out_proj = nn.Linear(hidden_size, hidden_size, bias=True)

        # Content-adaptive retention gate r_t in [0,1], per-head/per-channel
        # Implemented as sigmoid(W_g x + b), initialized to retain (~0.95)
        self.gate_proj = nn.Linear(hidden_size, hidden_size, bias=True)

        # Norms and dropout
        self.pre_norm = RMSNorm(hidden_size)
        self.dropout = nn.Dropout(dropout)

        # Chunking and numerical stability
        self.chunk_size = kwargs.get('chunk_size', 128)
        self.eps = 1e-6

        self._reset_parameters()

    def _reset_parameters(self):
        nn.init.xavier_uniform_(self.q_proj.weight)
        nn.init.xavier_uniform_(self.k_proj.weight)
        nn.init.xavier_uniform_(self.v_proj.weight)
        nn.init.xavier_uniform_(self.out_proj.weight)
        nn.init.zeros_(self.out_proj.bias)
        # Gate bias to logit(0.95) â‰ˆ 2.944439; weight small to start near constant retention
        nn.init.zeros_(self.gate_proj.weight)
        nn.init.constant_(self.gate_proj.bias, 2.944439)

    @torch_compile()
    def _scan_chunks(self,
                     phi_q: torch.Tensor,
                     phi_k: torch.Tensor,
                     v: torch.Tensor,
                     r: torch.Tensor,
                     attn_mask: Optional[torch.Tensor]) -> torch.Tensor:
        """Causal chunked scan over time.
        Inputs are shaped [b, h, t, d]. Output is [b, h, t, d].
        attn_mask: [b, 1, t, 1] with 1 for valid, 0 for masked/pad (or None)
        """
        b, h, t, d = phi_q.shape
        y = torch.zeros((b, h, t, d), dtype=phi_q.dtype, device=phi_q.device)

        # Initialize states S and Z
        S = torch.zeros((b, h, d, d), dtype=phi_q.dtype, device=phi_q.device)
        Z = torch.zeros((b, h, d), dtype=phi_q.dtype, device=phi_q.device)

        # Process in chunks
        cs = self.chunk_size
        for start in range(0, t, cs):
            end = min(start + cs, t)
            # Slice views
            phi_q_c = phi_q[:, :, start:end, :]
            phi_k_c = phi_k[:, :, start:end, :]
            v_c = v[:, :, start:end, :]
            r_c = r[:, :, start:end, :]
            if attn_mask is not None:
                m_c = attn_mask[:, :, start:end, :]  # [b,1,c,1] broadcastable to [b,h,c,1]
            else:
                m_c = None

            # Step scan within chunk
            steps = end - start
            for i in range(steps):
                k_t = phi_k_c[:, :, i, :]  # [b,h,d]
                v_t = v_c[:, :, i, :]      # [b,h,d]
                q_t = phi_q_c[:, :, i, :]  # [b,h,d]
                r_t = r_c[:, :, i, :]      # [b,h,d] retention per channel

                if m_c is not None:
                    valid = m_c[:, :, i, :]  # [b,1,1]
                    # Broadcast to [b,h,1] for safe multiplication across feature dim
                    valid_bh1 = valid.expand(-1, h, -1)  # [b,h,1]
                    # Apply masking to k and v (zero out padded positions)
                    k_t = k_t * valid_bh1
                    v_t = v_t * valid_bh1
                    # For masked steps, set r_t=1 (no decay across padding)
                    r_t = torch.where(valid_bh1 > 0, r_t, torch.ones_like(r_t))

                # Update S and Z with retention r_t
                # S: [b,h,d,d] = r_t:[b,h,d] on row-dim
                S = S * r_t.unsqueeze(-1) + torch.einsum('bhd,bhe->bhde', k_t, v_t)
                Z = Z * r_t + k_t

                # Compute output y_t = (q_t @ S) / (q_t @ Z + eps)
                num_t = torch.einsum('bhd,bhde->bhe', q_t, S)  # [b,h,d]
                den_t = torch.sum(q_t * Z, dim=-1, keepdim=True)  # [b,h,1]
                den_t = den_t + self.eps
                y[:, :, start + i, :] = num_t / den_t

        return y

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """Forward pass of the attention layer.
        Args:
            x: [batch, seq_len, hidden_size]
            mask: Optional attention mask [batch, seq_len], 1 for valid, 0 for pad
        Returns:
            [batch, seq_len, hidden_size]
        """
        # Pre-norm
        x_norm = self.pre_norm(x)

        # Projections
        q = self.q_proj(x_norm)
        k = self.k_proj(x_norm)
        v = self.v_proj(x_norm)
        g = self.gate_proj(x_norm)

        # Reshape to multi-head: [b, h, t, d]
        # Use einops rearrange to maintain robustness
        q = rearrange(q, 'b t (h d) -> b h t d', h=self.num_heads)
        k = rearrange(k, 'b t (h d) -> b h t d', h=self.num_heads)
        v = rearrange(v, 'b t (h d) -> b h t d', h=self.num_heads)
        g = rearrange(g, 'b t (h d) -> b h t d', h=self.num_heads)

        b, h, t, d = q.shape

        # Build RoPE cache and apply to q,k (requires even d)
        if d % 2 != 0:
            # If odd, skip RoPE to preserve robustness.
            pass
        else:
            cos, sin = build_rope_cache(t=t, d=d, device=x.device)
            cos = cos.to(dtype=q.dtype)
            sin = sin.to(dtype=q.dtype)
            # reshape to [1,1,t,d] for broadcasting across [b,h,t,d]
            cos = rearrange(cos, 't 1 d -> 1 1 t d')
            sin = rearrange(sin, 't 1 d -> 1 1 t d')
            q = apply_rope(q, cos, sin)
            k = apply_rope(k, cos, sin)

        # Positive features for linear attention
        phi_q = phi_feature_map(q)
        phi_k = phi_feature_map(k)

        # Retention gate r_t in [0,1]
        r = torch.sigmoid(g)

        # Prepare mask for scanning: [b,1,t,1]
        if mask is not None:
            # Ensure mask dtype and device match compute tensor for multiplies
            m = mask.to(device=phi_q.device, dtype=phi_q.dtype)
            m = rearrange(m, 'b t -> b 1 t 1')
        else:
            m = None

        # Causal chunked scan
        y = self._scan_chunks(phi_q=phi_q, phi_k=phi_k, v=v, r=r, attn_mask=m)

        # Merge heads and output projection
        y = rearrange(y, 'b h t d -> b t (h d)')
        y = self.out_proj(y)
        y = self.dropout(y)

        # Residual connection (pre-LN style)
        return x + y


class SwiGLU(nn.Module):
    def __init__(self, dim: int, hidden_mult: float = 2.0, dropout: float = 0.1):
        super().__init__()
        inner = int(dim * hidden_mult)
        self.w1 = nn.Linear(dim, inner, bias=False)
        self.w2 = nn.Linear(dim, inner, bias=False)
        self.out = nn.Linear(inner, dim, bias=True)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        a = self.w1(x)
        b = self.w2(x)
        x = F.silu(a) * b
        x = self.out(self.dropout(x))
        return x


class DeltaNetBlock(nn.Module):
    """Complete Transformer block with DeltaNet attention and SwiGLU FFN (pre-LN)."""

    def __init__(self, hidden_size: int, num_heads: int = 8,
                 ffn_hidden_size: Optional[int] = None, dropout: float = 0.1, **kwargs):
        super().__init__()
        if ffn_hidden_size is None:
            # Use 2x SwiGLU instead of 4x GELU MLP for parameter efficiency
            ffn_hidden_size = int(hidden_size * 2.0)

        # Keep a norm here if needed in future extensions, but DeltaNet does its own pre-LN
        self.attn_norm = RMSNorm(hidden_size)
        self.attention = DeltaNet(hidden_size, num_heads, dropout, **kwargs)

        self.ffn_norm = RMSNorm(hidden_size)
        self.ffn = SwiGLU(hidden_size, hidden_mult=2.0, dropout=dropout)
        self.ffn_dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # Attention sublayer handles its own pre-LN and residual
        x = self.attention(x, mask)

        # Pre-LN FFN
        ffn_in = self.ffn_norm(x)
        x = x + self.ffn_dropout(self.ffn(ffn_in))
        return x


class DeltaNetModel(nn.Module):
    """Complete model using evolved DeltaNet attention blocks."""

    def __init__(self, vocab_size: int, hidden_size: int = 512, num_layers: int = 6,
        num_heads: int = 8, max_seq_len: int = 2048, dropout: float = 0.1, **kwargs):
        super().__init__()
        self.hidden_size = hidden_size
        self.max_seq_len = max_seq_len

        # Embeddings
        self.token_embedding = nn.Embedding(vocab_size, hidden_size)
        self.position_embedding = nn.Embedding(max_seq_len, hidden_size)
        self.emb_dropout = nn.Dropout(dropout)

        # Stacked blocks
        self.layers = nn.ModuleList([
            DeltaNetBlock(hidden_size, num_heads, dropout=dropout, **kwargs)
            for _ in range(num_layers)
        ])

        # Final normalization and LM head
        self.final_norm = RMSNorm(hidden_size)
        self.lm_head = nn.Linear(hidden_size, vocab_size, bias=False)
        self.lm_head.weight = self.token_embedding.weight

        self._reset_parameters()

    def _reset_parameters(self):
        nn.init.normal_(self.token_embedding.weight, std=0.02)
        nn.init.normal_(self.position_embedding.weight, std=0.02)

    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        b, t = input_ids.shape
        # positions
        pos = torch.arange(t, device=input_ids.device).unsqueeze(0)
        pos = pos.expand(b, -1)

        # embeddings
        tok = self.token_embedding(input_ids)
        pos = self.position_embedding(pos)
        x = tok + pos
        x = self.emb_dropout(x)

        # blocks
        for layer in self.layers:
            x = layer(x, attention_mask)

        # head
        x = self.final_norm(x)
        logits = self.lm_head(x)
        return logits

    def get_architecture_summary(self) -> str:
        return f"""
DeltaNet Architecture Summary (Evolved):
- Model Type: Gated Renormalized Linear Attention Transformer
- Hidden Size: {self.hidden_size}
- Number of Layers: {len(self.layers)}
- Number of Heads: {self.layers[0].attention.num_heads}
- Max Sequence Length: {self.max_seq_len}
- Key Innovations: Content-adaptive retention, S/Z renormalization, RoPE, Pre-LN RMSNorm, SwiGLU
- Parameters: {sum(p.numel() for p in self.parameters()):,}
"""


# Factory function for creating the model
def create_model(vocab_size: int = 50257, **kwargs) -> DeltaNetModel:
    default_config = {
        'hidden_size': 512,
        'num_layers': 6,
        'num_heads': 8,
        'max_seq_len': 2048,
        'dropout': 0.1
    }
    default_config.update(kwargs)
    return DeltaNetModel(vocab_size=vocab_size, **default_config)


if __name__ == "__main__":
    # Basic smoke test
    model = create_model(vocab_size=1000, hidden_size=256, num_layers=4, num_heads=8)
    batch_size, seq_len = 2, 100
    input_ids = torch.randint(0, 1000, (batch_size, seq_len))
    with torch.no_grad():
        out = model(input_ids)
        print('Input:', input_ids.shape)
        print('Output:', out.shape)
        print(model.get_architecture_summary())
