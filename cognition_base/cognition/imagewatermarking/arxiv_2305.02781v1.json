[
    {
        "DESIGN_INSIGHT": "### DESIGN_INSIGHT_HIGH: [Temporal-to-Channel Merge (ItoV) – Treat Videos as “Fat Images” for 2D CNN Watermarking]\nItoV replaces spatiotemporal reasoning (3D/(2+1)D convolutions) and per-frame watermarking with a reshape operator that merges the temporal dimension into the channel dimension, enabling direct reuse of robust image watermarking encoders/decoders for videos. Instead of independently watermarking frames or learning temporal dynamics, ItoV processes the entire clip jointly with purely spatial convolutions.\n\nKey mechanism: given a video clip \\(V_c \\in \\mathbb{R}^{L \\times H \\times W \\times 3}\\), ItoV reshapes it as\n\\[\n\\tilde V_c \\in \\mathbb{R}^{H \\times W \\times (3L)} \\quad \\text{via} \\quad \\tilde V_c = \\text{MergeTimeToChannel}(V_c),\n\\]\nand feeds \\(\\tilde V_c\\) to any 2D image watermarking encoder \\(E_{\\theta_E}\\) along with a message embedding \\(\\tilde M\\). The watermarked output \\(\\tilde V_w\\) is then reshaped back to \\(V_w \\in \\mathbb{R}^{L \\times H \\times W \\times 3}\\). Training uses an encoder distortion term \\(L_E = \\text{MSE}(V_c, V_w)\\) and a decoder message loss \\(L_D = \\text{MSE}(M, M')\\) with an attack simulation layer in between, yielding\n\\[\nL_{\\text{total}} = \\lambda_E L_E + \\lambda_D L_D + \\lambda_F L_F.\n\\]\n\nFundamental difference from prior video watermarking: (i) Two-stream or 3D/(2+1)D CNNs explicitly learn temporal features; ItoV eliminates temporal convolution entirely, based on the insight that watermark embedding depends on spatial pixel distributions rather than motion. (ii) Compared to per-frame image watermarking, ItoV jointly optimizes across frames within the 2D network’s channel space, improving robustness to video-specific attacks (e.g., H.264, frame manipulations). Complexity-wise, for a single layer with kernel \\((k_t, k, k)\\), channels \\((C_{\\text{in}}, C_{\\text{out}})\\), and clip length \\(L\\),\n- 3D conv FLOPs: \\(\\approx LHW \\cdot (k_t k^2 C_{\\text{in}}) C_{\\text{out}}\\),\n- ItoV 2D conv FLOPs on merged input: \\(\\approx HW \\cdot (k^2 (L C_{\\text{in}})) C_{\\text{out}}\\),\nso the ratio is \\(\\text{FLOPs}_{3D}/\\text{FLOPs}_{\\text{ItoV-2D}} \\approx k_t\\), compounding across layers and avoiding the memory overhead of maintaining a temporal feature dimension. Empirically (Fig. 3–4, Table 2), purely spatial designs outperform or match spatiotemporal counterparts under broad distortions, validating the architectural premise.\n\n\n### DESIGN_INSIGHT_MEDIUM: [Depthwise-Separable Convolutions for Video Watermarking – Parameter-Efficient Spatial Dominant Design]\nItoV refactors regular convolutions into depthwise-separable blocks to slash parameters and FLOPs while preserving robustness, aligning with the finding that spatial convolutions dominate performance in video watermarking. The encoder/decoder blocks swap\n- Standard 2D conv: params \\(= k^2 C_{\\text{in}} C_{\\text{out}}\\),\n- Depthwise-separable 2D: params \\(= k^2 C_{\\text{in}} + C_{\\text{in}} C_{\\text{out}}\\),\nand analogously for 3D with temporal kernel \\(k_t\\):\n- Standard 3D: params \\(= k_t k^2 C_{\\text{in}} C_{\\text{out}}\\),\n- Depthwise-separable 3D: params \\(= k_t k^2 C_{\\text{in}} + C_{\\text{in}} C_{\\text{out}}\\).\n\nMechanism: depthwise kernels capture per-channel spatial structure, while subsequent pointwise \\(1 \\times 1\\) (or \\(1 \\times 1 \\times 1\\)) convolutions mix channels, which is sufficient for watermark signal synthesis when temporal modeling is unnecessary. This preserves channel interactions that encode the watermark while dramatically reducing compute.\n\nDifference from prior applications of depthwise convs (e.g., MobileNet, video classification): here the goal is robust, invisible watermark embedding/extraction rather than action recognition, and the paper shows that heavier temporal kernels are unnecessary and even prone to overfitting under video distortions. Quantitatively (Table 2): replacing regular 2D with depthwise 2D reduces parameters and FLOPs from 20.8M/15.57G to 5.99M/4.55G (~3.5× fewer params and ~3.4× fewer FLOPs) with comparable or better bit accuracy across H.264, blur, crop, and color attacks; depthwise 3D yields even larger savings over regular 3D (54.2M/234.63G → 6.08M/28.82G).\n\n\n### DESIGN_INSIGHT_LOW: [Per-Frame Consistency Loss L_F – Regularizing Temporal Uniformity of Perturbation Magnitude]\nTo mitigate frame-to-frame fluctuation (perceptual flicker) in watermark strength, ItoV augments the encoder distortion with a per-frame consistency term\n\\[\nL_F = \\sum_{i=1}^{L} \\lVert F^{(i)}(V_c) - F^{(i)}(V_w) \\rVert_1,\n\\]\nwhere \\(F^{(i)}(\\cdot)\\) indexes the \\(i\\)-th frame. This supplements the global reconstruction term \\(L_E=\\text{MSE}(V_c,V_w)\\) by explicitly constraining the perturbation envelope per frame, thereby enforcing uniform watermark intensity across the clip.\n\nUnlike prior works that rely solely on global pixel-wise MSE or adversarial discriminators for invisibility, this loss directly targets temporal uniformity of distortion. Empirically (Table 3), adding \\(L_F\\) reduces the standard deviation of per-frame PSNR from 1.344 dB to 0.159 dB and raises the worst-frame PSNR (38.43 → 39.63 dB), improving perceived invisibility without sacrificing robustness.",
        "EXPERIMENTAL_TRIGGER_PATTERNS": "**Task_Performance_Signatures:**\n- Video watermarking robustness and invisibility (primary tasks in this paper)\n  - Expect improved robustness vs. prior video watermarking baselines under common video distortions at similar or higher invisibility.\n    - Kinetics-600 (clip size 8×128×128, message length m=96):\n      - H.264 (CRF=22): ItoV-MBRS ≈ 99.74% bit accuracy (+6.8% absolute over DVMark 92.94%); ItoV-CIN ≈ 98.44% (+5.5% over DVMark).\n      - Frame Average (N=3): ≈ 99.99% (parity across methods).\n      - Frame Drop (p=0.5): ≈ 99.49–99.76%.\n      - Frame Swap (p=0.5): ≈ 99.54–99.94%.\n      - Gaussian Blur (σ=2.0): ≈ 99.28–99.99% (ItoV-MBRS at ceiling 99.99%).\n      - Gaussian Noise (std=0.04): 99.99%.\n      - Random Crop (p=0.4): ≈ 98.81–99.22%.\n      - Random Hue (p=1.0): ≈ 99.89–99.99%.\n    - Invisibility (PSNR): Kinetics-600 PSNR ≈ 37.97 dB (ItoV-MBRS) and ≈ 37.13 dB (ItoV-CIN), both ≥ baselines (Hidden/DVMark ≈ 37.0 dB). On Inter4K, ItoV-MBRS retains ≈ 37.97 dB with similarly high bit accuracy (e.g., H.264 ≈ 99.67%).\n  - Generalization to high-resolution videos:\n    - Inter4K: near-identical robustness/invisibility to Kinetics-600 without retraining (e.g., ItoV-MBRS H.264 ≈ 99.67%, PSNR ≈ 37.97 dB), indicating strong cross-domain generalization.\n  - H.264 robustness across compression strengths:\n    - Bit accuracy remains ≈ 100% for CRF ≤ 20; degrades gradually for 20 < CRF ≤ 25; sharp decline beyond CRF > 25. ItoV-MBRS consistently above ItoV-CIN at the same CRF.\n  - Real-world screen recording:\n    - Software screen recording (OBS default, streaming at 1000 kbps): ItoV-MBRS ≈ 97.86% accuracy; ItoV-CIN ≈ 96.92%.\n  - Effect of convolutional block choice (within ItoV-MBRS):\n    - Depthwise 2D vs Standard 2D: similar or slightly better robustness with small PSNR tradeoff and large efficiency gains.\n      - H.264: 99.22% (Depthwise 2D) vs 98.61% (2D).\n      - Frame Swap: 99.68% (Depthwise 2D) vs 99.98% (2D) [−0.30%].\n      - Random Crop: 99.99% (Depthwise 2D) vs 99.60% (2D) [+0.39%].\n      - PSNR: 38.3 dB (Depthwise 2D) vs 39.9 dB (2D) [−1.6 dB].\n    - Temporal convolution can hurt under some spatial distortions:\n      - (2+1)D suffers on Random Crop: ≈ 95.12% vs 2D ≈ 99.60% and Depthwise 2D ≈ 99.99% (suggesting overfitting or misplaced temporal emphasis).\n    - 3D conv shows strong H.264 (≈ 99.91%) but lower PSNR (≈ 37.1 dB) and significantly higher compute.\n  - Frame loss (LF) to equalize per-frame watermark strength:\n    - Reduces per-frame PSNR variance: std drops from ≈ 1.344 dB to ≈ 0.159 dB (≈ 8.5× reduction).\n    - Improves worst-frame PSNR from ≈ 38.43 dB to ≈ 39.63 dB; mean PSNR decreases slightly (≈ −0.21 dB), with no measurable drop in robustness.\n- Efficiency signatures (training/inference compute)\n  - Depthwise 2D block achieves large compute savings with comparable robustness:\n    - Params: ≈ 5.99M vs 20.8M (2D) and 54.2M (3D) → ~3.5× and ~9× fewer.\n    - FLOPs: ≈ 4.55B vs 15.57B (2D) and 234.63B (3D) → ~3.4× and ~51.6× lower.\n    - Overall robustness within ≤ ~0.3% absolute on most distortions; PSNR within ~1.6 dB of 2D.\n- Contextual conditions where improvements are most pronounced\n  - When adapting strong image watermarking encoders/decoders (e.g., MBRS, CIN) to videos by merging temporal into channel dimension.\n  - Under non-differentiable distortions (e.g., H.264) when trained with forward attack simulation layers.\n  - Clip-level processing (feeding entire L-frame clip as one sample) rather than per-frame embedding.\n  - Message length around m=96 and clip size ≈ 8×128×128; similar trends expected for nearby settings.\n- Scale and data domain dependencies\n  - Temporal convolutions (3D, (2+1)D) can over-parameterize and overfit on small clip lengths (L≈8) or when distortions are primarily spatial/codec-based; 2D/depthwise 2D maintain robustness and invisibility with lower compute.\n  - Generalization holds moving from Kinetics-600 (lower-res) to Inter4K (4K) without re-training.\n- Training vs inference differences\n  - Two-phase training (noise-free pretraining, then distortion-robust training with LF>0) yields higher invisibility consistency (per-frame PSNR) with maintained decoding accuracy.\n- Degradation bounds\n  - Under very strong compression (H.264 CRF>25), expect rapid accuracy decline.\n  - With frame loss enabled, average PSNR may drop by ~0.2 dB, but minimum-frame PSNR improves notably.\n- Non-video NLP/CV benchmarks (not the target of this method)\n  - Language modeling: lambada_openai, wikitext, ptb → Expect unchanged performance (not applicable); within ±0.5% of baseline if any incidental change.\n  - Reading comprehension: squad_completion, squad_v2, narrativeqa → Expect unchanged.\n  - Commonsense reasoning: hellaswag, piqa, social_iqa, commonsenseqa → Expect unchanged.\n  - Factual QA: arc_easy, arc_challenge, boolq, openbookqa → Expect unchanged.\n  - Context resolution: winogrande, winograd → Expect unchanged.\n  - Other tasks: swde, fda → Expect unchanged.\n\n**Architectural_Symptoms:**\n- Training characteristics\n  - Smoother encoder loss convergence after introducing frame loss; decoder loss remains stable when adding LF (robustness maintained).\n  - Reduced per-batch/per-epoch variance in invisibility metrics (PSNR) across frames once LF is enabled (std ≈ 0.159 dB vs 1.344 dB).\n  - Networks with temporal convolutions exhibit signs of overfitting:\n    - (2+1)D shows content-agnostic watermark patterns (residuals independent of background) and degraded Random Crop performance.\n    - 3D conv concentrates watermark energy in early frames (visible in residual maps), indicating temporal imbalance.\n  - Forward ASL for non-differentiable attacks yields stable backpropagation (no gradient through pseudo-distortion), fewer divergence events, and absence of NaNs.\n- Runtime/memory behaviors\n  - With temporal-to-channel merge (2D/depthwise 2D):\n    - Compute scales linearly with number of frames L via channel dimension, avoiding temporal kernel overhead; markedly lower FLOPs than 3D/(2+1)D at the same L.\n    - Depthwise 2D achieves:\n      - ~3.4× higher throughput vs 2D (FLOPs 4.55B vs 15.57B) and ~50× vs 3D (4.55B vs 234.63B), enabling larger batch sizes on the same GPU memory budget.\n      - ~3.5–9× parameter reduction vs 2D/3D, reducing checkpoint size and improving loading times.\n    - Higher GPU utilization due to reduced memory bandwidth pressure per sample (smaller activations/params), particularly with depthwise kernels.\n  - 3D/(2+1)D conv stacks show:\n    - Increased latency and lower throughput consistent with FLOPs multiples (e.g., 3D FLOPs ≈ 234.63B vs depthwise 2D ≈ 4.55B).\n    - More frequent OOM risks at batch size ≥16 on commodity GPUs relative to depthwise 2D/2D designs (given >9× params and >25–50× FLOPs).\n- Profiling signatures and visual diagnostics\n  - Residual visualizations across frames:\n    - With LF: evenly distributed, low-amplitude residuals over all frames.\n    - 3D: stronger residuals in first 1–2 frames; less uniform distribution across time.\n    - (2+1)D: residuals exhibit repeated texture-like patterns that are weakly coupled to scene content.\n  - H.264 robustness curve:\n    - Plateau near 100% bit accuracy for CRF ≤ 20; breakpoint around CRF ≈ 25 where accuracy begins a steep descent (ItoV-MBRS curve consistently above ItoV-CIN).\n  - Training regimen signatures:\n    - Phase 1 (noise-free): rapid initial convergence of encoder/decoder losses.\n    - Phase 2 (with distortions + forward ASL): stable robustness metrics across a stochastic mixed distortion set per batch; consistent accuracy ≥ 99% on most distortions after convergence.\n  - Comparison to baselines:\n    - At equal PSNR (≈ 37 dB), ItoV variants deliver ≥ 5.5–6.8% absolute gains on H.264 over DVMark and much higher over Hidden; profiling shows throughput gains when using 2D/depthwise 2D vs 3D baselines.\n- Potential neutral/negative indicators\n  - Minor PSNR decrease (~0.2 dB) when enabling LF despite improved minimum-frame PSNR.\n  - Accuracy degradation under extreme compression (CRF > 25).\n  - Temporal convolution blocks may underperform on spatially destructive perturbations (e.g., Random Crop), signaling possible over-parameterization/overfitting.",
        "BACKGROUND": "Title: ItoV: Efficiently Adapting Deep Learning-based Image Watermarking to Video Watermarking\n\nHistorical Technical Context:\nDeep learning-based watermarking evolved from classical image steganography and transform-domain watermarking. Early methods embedded bits via pixel-domain LSB modification and later via DFT/DCT/DWT coefficient perturbations, but struggled to jointly optimize invisibility and robustness. The deep learning era began with encoder–noise–decoder pipelines (e.g., HiDDeN), where CNN encoders learn content-adaptive embeddings and decoders reconstruct messages after differentiable attack layers. Progress focused on handling non-differentiable distortions such as JPEG via simulation, two-stage frameworks, and training curricula that mix real and simulated attacks (e.g., MBRS) or invertible frameworks (CIN). These advances yielded strong robustness on images, motivating exploration in videos.\n\nFor videos, prevalent architectural paradigms have been 3D CNNs, two-stream 2D CNNs, and factorizations like (2+1)D, all designed to explicitly capture temporal dynamics central to tasks like action recognition. However, 3D CNNs are compute- and memory-intensive, often unstable to train and prone to overfitting. Video watermarking with deep learning has been less explored: U-Net-based video steganography emphasized capacity and imperceptibility but ignored distortions; RivaGAN targeted a few differentiable attacks but not H.264; DVMark introduced a learned H.264 simulator and multiscale design to improve robustness. The dominant assumption carried over from video understanding tasks is that temporal modeling is necessary. This paper challenges that assumption for the specific goal of robust, imperceptible video watermarking.\n\nTechnical Limitations:\n- Computational bottlenecks in spatiotemporal CNNs: Standard 3D convolutions scale as O(L·k_t·k_h·k_w·C_in·C_out·H·W) per layer, incurring very high FLOPs. Empirically (Table 2), 3D vs 2D FLOPs are 234.63G vs 15.57G, and vs depthwise-2D 4.55G, demonstrating 15–50× overheads that hinder training and deployment.\n- Training instability and over-parameterization: 3D/(2+1)D stacks tend to overfit video watermarking, degrading robustness under spatial transformations (e.g., random crop accuracy drops to 95.12% for (2+1)D vs 99.60% for 2D), consistent with unstable convergence reported for 3D CNNs.\n- Non-differentiable distortions block gradients: Real-world codecs (H.264), cropping, frame drops/swaps, and screen recording are non-differentiable, limiting robustness gains when trained with purely differentiable noise layers or imperfect simulators; approximation gaps reduce generalization to real pipelines.\n- Frame-wise embedding inconsistency (temporal flicker): Independently watermarking frames or training without inter-frame constraints leads to uneven per-frame distortion. Measured PSNR standard deviation across frames is 1.344 dB without constraints, causing visible frame-wise quality dips.\n- Inefficiency of per-frame processing: Treating each frame as an image underutilizes the joint spatiotemporal sample and fails to capture video-specific attack patterns (e.g., frame averaging), limiting robustness while duplicating compute for each frame.\n- Hardware efficiency constraints: High-resolution content (e.g., 4K) stresses memory and compute. Heavy 3D models (54.2M params, 234.63 GFLOPs) are impractical for large-scale training/inference, motivating depthwise and 2D-centric designs with 5.99M params and 4.55 GFLOPs.\n\nPaper Concepts:\n- ItoV Temporal-to-Channel Reshaping: A reparameterization that merges temporal and channel dimensions so a video clip is processed as a single image by 2D CNNs. For input video V ∈ ℝ^{L×H×W×3}, ItoV forms\n  \\[ \\tilde{V} \\in \\mathbb{R}^{H \\times W \\times (3L)}, \\quad \\tilde{V}(:,:,3(\\ell-1)+c) = V(\\ell,:,:,c), \\]\n  where ℓ ∈ {1,…,L}, c ∈ {1,2,3}. Intuition: watermarking is content-adaptive embedding over pixel distributions; temporal order is not essential if all frames’ content is jointly visible to the encoder as channels.\n- Encoder–Decoder Watermarking with Total Loss: The encoder E embeds a binary message M ∈ {0,1}^m into V_c to produce V_w = E(θ_E, V_c, M), and the decoder D predicts M′ = D(θ_D, V_a) from attacked video V_a. The objective combines invisibility, message accuracy, and frame-consistency:\n  \\[ L_E = \\operatorname{MSE}(V_c, V_w), \\quad L_D = \\operatorname{MSE}(M, M′), \\]\n  \\[ L_{\\text{total}} = \\lambda_E L_E + \\lambda_D L_D + \\lambda_F L_F. \\]\n  Role: balances imperceptibility (PSNR) and robustness (bit accuracy) while enforcing uniform per-frame embedding strength.\n- Frame Loss for Per-Frame Consistency: To reduce frame-wise quality variance, define frames via F^{(i)}(·), i=1…L, and\n  \\[ L_F = \\sum_{i=1}^L \\| F^{(i)}(V_c) - F^{(i)}(V_w) \\|_1. \\]\n  Intuition: penalizes uneven watermark energy across frames. Empirically reduces PSNR std from 1.344 dB to 0.159 dB and raises the worst-frame PSNR from 38.43 dB to 39.63 dB.\n- Forward Attack Simulation Layer (Forward ASL): For non-differentiable attacks A(·), compute pseudo-distortion Δ = V_a − V_w with V_a = A(V_w), then form a pseudo-attacked input Ṽ_a = V_w + stopgrad(Δ) so gradients bypass A during backprop. Formally,\n  \\[ \\Delta = A(V_w) - V_w,\\quad Ṽ_a = V_w + \\operatorname{sg}(\\Delta). \\]\n  Intuition: exposes the decoder to realistic perturbations while preserving stable gradient flow to the encoder.\n- Depthwise Separable Convolutions for Video Watermarking: Replace standard k×k convolutions (params k^2 C_in C_out) with depthwise + pointwise (k^2 C_in + C_in C_out) in 2D and 3D. In ItoV-MBRS, depthwise-2D achieves comparable robustness (e.g., H.264 99.22%) with 5.99M params and 4.55 GFLOPs vs 20.8M/15.57G (2D) and 54.2M/234.63G (3D). Intuition: watermarking relies more on spatial adaptation than heavy spatiotemporal mixing; channel interactions are preserved by pointwise layers.\n- Spatiotemporal Convolutional Variants: 3D convolutions use k_t×k_h×k_w kernels; (2+1)D factorizes into spatial k_h×k_w and temporal k_t×1×1 steps. While effective for action recognition, this work shows temporal kernels are not primary for watermarking; excessive temporal modeling can overfit and hurt robustness (e.g., random crop accuracy 95.12% for (2+1)D).\n\nExperimental Context:\nThe evaluation emphasizes robustness and invisibility under realistic video distortions and efficiency across convolutional designs. Tasks are discriminative (message recovery) with strict imperceptibility constraints. Metrics include bit accuracy\n\\[ \\text{Accuracy}(\\%) = \\Big(1 - \\frac{1}{m}\\| M - M′ \\|_1 \\Big) \\times 100, \\]\nand PSNR between cover and watermarked videos. Benchmarks use Kinetics-600 clips (8×128×128) for training and validation and Inter4K for high-resolution generalization. Distortion suite includes H.264 (CRF=22 baseline and CRF sweeps), frame average (N=3), frame drop/swap (p=0.5), Gaussian blur (σ=2.0), Gaussian noise (std=0.04), random crop (p=0.4), and random hue (p=1.0). A real screen-recording test (1000 kbps streaming + OBS default capture) assesses compound non-differentiable pipelines.\n\nThe goals are to demonstrate: (1) temporal information is not essential for watermarking when the temporal dimension is merged into channels; (2) ItoV enables direct adaptation of state-of-the-art image methods (MBRS, CIN) to videos with superior robustness/invisibility compared to video-specific baselines (Hidden, DVMark); (3) depthwise designs dramatically reduce compute without harming robustness. Representative results: on Kinetics-600, ItoV-MBRS attains PSNR 37.97 dB and ≥99% bit accuracy across all listed distortions, including 99.74% under H.264, surpassing DVMark (92.94%). Both ItoV-MBRS and ItoV-CIN generalize strongly to Inter4K with similar accuracies. H.264 CRF sweeps show near-zero error for CRF < 20 and graceful degradation beyond CRF ≈ 25; screen recording yields 97.86% (ItoV-MBRS) and 96.92% (ItoV-CIN). An ablation across convolutional blocks quantifies robustness/compute trade-offs and validates the primacy of spatial and depthwise operations for this task.",
        "ALGORITHMIC_INNOVATION": "**Core_Algorithm:**\n- Replace temporal modeling in video watermarking with a reshape operation that merges the temporal dimension into the channel dimension: a clip Vc ∈ R^{L×3×H×W} is reshaped to X ∈ R^{(3L)×H×W} and processed by a 2D image-watermarking network (encoder/decoder) unchanged.\n- Insert a forward attack simulation layer (ASL) to handle non-differentiable video distortions (e.g., H.264): compute a pseudo-distortion Δ = stopgrad(A(Vw) − Vw) and use Ṽa = Vw + Δ during backprop so gradients bypass the non-differentiable operator.\n- Add a frame-wise consistency loss LF to constrain per-frame watermark strength, alongside standard invisibility (LE) and decoding (LD) losses; train first without distortions, then fine-tune with a batch-wise random mixture of video attacks.\n- Optionally replace regular convolutions with depthwise (2D or 3D) blocks throughout the encoder/decoder to reduce parameters and FLOPs while preserving performance; temporal convolutions (3D, (2+1)D) are not required and can hurt generalization.\n\n**Key_Mechanism:**\n- The key insight is that, for watermark embedding/extraction, networks primarily exploit spatial pixel statistics rather than temporal dynamics; treating the L frames as channels lets 2D spatial convolutions learn robust, content-adaptive embeddings over the entire clip with minimal architectural changes.\n- Forward ASL provides robustness to non-differentiable distortions by injecting a fixed pseudo-perturbation in the forward pass while allowing exact gradient flow, avoiding instability and gradient mismatch through non-differentiable operators.\n- The frame loss encourages uniform per-frame embedding energy, improving perceived invisibility by preventing occasional low-PSNR frames; depthwise separable convolutions remove over-parameterization, reducing compute with negligible accuracy loss.\n\n**Mathematical_Formulation:**\n- Video-to-image reshape and encoder/decoder:\n  - Input clip: Vc ∈ R^{L×3×H×W}, message M ∈ {0,1}^m.\n  - Reshape: X = R(Vc) ∈ R^{C×H×W}, C = 3L.\n  - Encoder (cover/message processors + generator): R_w = G([F_c(X), F_m(M)]) and Xw = X + R_w; Vw = R^{-1}(Xw).\n  - Decoder: M′ = D(Va).\n- Losses:\n  \\[\n  L_E = \\mathrm{MSE}(V_c, V_w) = \\frac{1}{3LH W}\\,\\|V_c - V_w\\|_2^2,\n  \\]\n  \\[\n  L_D = \\mathrm{MSE}(M, M') = \\frac{1}{m}\\,\\|M - M'\\|_2^2,\n  \\]\n  \\[\n  L_F = \\sum_{i=1}^{L} \\|F^{(i)}(V_c) - F^{(i)}(V_w)\\|_1,\n  \\]\n  \\[\n  L_{\\text{total}} = \\lambda_E L_E + \\lambda_D L_D + \\lambda_F L_F.\n  \\]\n  Here F^{(i)}(·) extracts the i-th frame; λE, λD, λF are weights (e.g., pretrain: 1, 0.1, 0; robust finetune: 1, 0.01, 0.05).\n- Forward ASL (non-differentiable attack A, e.g., H.264):\n  \\[\n  \\Delta = \\operatorname{stopgrad}\\!\\big(A(V_w) - V_w\\big),\\quad \\tilde V_a = V_w + \\Delta,\n  \\]\n  and use M′ = D(\\tilde V_a) in training. For differentiable distortions T, Va = T(Vw).\n  Gradients satisfy ∂\\tilde V_a/∂V_w = I, so ∇_{V_w} L flows unchanged to the encoder.\n- Convolutional complexity (per layer, feature map H×W, kernel k):\n  - 2D conv (ItoV after reshape, first layer only impacted by L): \n    \\[\n    \\text{FLOPs} \\sim O\\!\\big(HW \\, (3L)\\, C_{\\text{out}}\\, k^2\\big).\n    \\]\n    Subsequent layers scale as O(HW Cin Cout k^2) independent of L.\n  - 3D conv (kernel kt×k×k over L): \n    \\[\n    \\text{FLOPs} \\sim O\\!\\big(LHW \\, C_{\\text{in}}\\, C_{\\text{out}}\\, k_t k^2\\big)\n    \\]\n    at every layer.\n  - Depthwise separable 2D conv:\n    \\[\n    O\\!\\big(HW \\, C_{\\text{in}}\\, k^2\\big) + O\\!\\big(HW \\, C_{\\text{in}} C_{\\text{out}}\\big),\n    \\]\n    vs regular 2D: O(HW Cin Cout k^2).\n- Example quantitative properties from experiments (ItoV-MBRS variants, input 8×128×128):\n  - Regular 2D: 20.8M params, 15.57 GFLOPs; Depthwise 2D: 5.99M, 4.55 GFLOPs; 3D: 54.2M, 234.63 GFLOPs; Depthwise 3D: 6.08M, 28.82 GFLOPs.\n\n**Computational_Properties:**\n- Time Complexity:\n  - Training (ItoV with 2D conv): per iteration dominated by 2D CNN cost; only the first conv scales with L as O(HW·3L·Cout·k^2). Subsequent layers are identical to the image model. Total: O(T2D + TASL), where TASL is negligible for differentiable attacks and a single encode/decode pass for H.264 in forward-ASL.\n  - Inference: same as training without ASL; strictly 2D CNN cost with first-layer O(L) factor.\n  - Compared to 3D CNNs: ItoV avoids O(L) overhead at every layer; 3D is O(L)–O(Lkt) times costier per layer.\n- Space Complexity:\n  - Activations: O(B·(3L)·H·W) for the first layer, else identical to 2D image network. Memory grows linearly in L only for the input and first feature map.\n  - Parameters: identical to the base image model except for the first layer (weights increase by factor L: from 3→3L input channels). With depthwise separable convs, parameter count is reduced roughly by k^2-fold compared to regular convs in many layers.\n- Parallelization:\n  - Entire pipeline uses standard 2D convolutions and is fully compatible with highly optimized GPU kernels; no temporal data dependence remains after reshape, enabling maximal data and model parallelism.\n  - Forward ASL is embarrassingly parallel across samples; random per-batch attack selection does not introduce inter-sample dependencies.\n- Hardware Compatibility:\n  - Efficient on commodity GPUs with NCHW tensors; the reshape is a view operation (no copy) if frames are stored contiguously. Bandwidth and cache behavior match image CNNs.\n  - CPU inference benefits from 2D conv vectorization; avoids less mature 3D kernels.\n- Training vs. Inference:\n  - Training uses curriculum: (i) pretrain without attacks (λE=1, λD=0.1, λF=0), (ii) robust finetune with random attacks per batch (λE=1, λD=0.01, λF=0.05). Inference omits ASL and uses only encoder/decoder.\n- Parameter Count:\n  - Only the first conv layer increases parameters by factor L due to input channels 3→3L; depthwise separable replacements across the network can reduce total params substantially (e.g., 20.8M→5.99M in ItoV-MBRS).\n- Numerical Stability:\n  - MSE objectives (LE, LD) provide smooth gradients; forward ASL ensures stable gradients by identity Jacobian through the pseudo-attack. Potential gradient–attack mismatch is mitigated by mixing simulated differentiable attacks and forward-ASL approximations, improving robustness generalization.\n  - LF adds an L1 per-frame penalty that empirically reduces per-frame PSNR variance; choose λF conservatively to avoid over-constraining invisible embedding.\n- Scaling Behavior:\n  - With larger L, compute/memory increase only at the network input and first conv, enabling longer clips with modest overhead relative to 3D CNNs.\n  - Higher resolutions scale as O(HW) per 2D conv; depthwise convs provide near-linear parameter/FLOP reductions; robustness scales favorably with model capacity while invisibility is maintained via LE and LF.\n  - Temporal convolutions ((2+1)D, 3D) can overfit to temporal artifacts, degrading robustness to spatially dominant attacks (e.g., random crop); depthwise variants mitigate over-parameterization while preserving accuracy.\n\nImplementation-critical details and pitfalls:\n- Ensure reshape packs frames into channels consistently in both encoder and decoder; maintain the same ordering and L during training/inference.\n- For forward ASL, stop gradients through Δ strictly (e.g., .detach in PyTorch) to preserve identity gradient path.\n- Message processor must broadcast or upsample M to match latent spatial size before concatenation; use 1×1 convs to control channel growth.\n- When swapping to depthwise separable convs, preserve pointwise (1×1) mixing to keep cross-channel interactions necessary for robust embedding/extraction.",
        "IMPLEMENTATION_GUIDANCE": "Integration_Strategy:\n- Core idea (ItoV reshape):\n  - Treat a video clip V of shape [B, L, 3, H, W] as an “image” with shape [B, 3L, H, W] by merging temporal and channel dimensions before passing into an image watermarking encoder/decoder.\n  - Exact change points:\n    - Input adapter (pre-encoder): V_reshaped = V.permute(0, 2, 1, 3, 4).reshape(B, 3*L, H, W) in PyTorch; tf.transpose + tf.reshape in TensorFlow/JAX.\n    - Output adapter (post-decoder): reshape inverse for any per-frame metrics or visualization: Vw = Vw_reshaped.view(B, 3, L, H, W).permute(0, 2, 1, 3, 4).\n- Integrate into existing image watermarking models:\n  - MBRS-style autoencoder:\n    - Replace the image input channel count from 3 to 3L across first encoder layer and the last reconstruction layer.\n    - Keep Squeeze-and-Excitation blocks unchanged; they will operate over 3L channels.\n    - Message processor: instead of spatial tiling to [B, C_msg, H, W] with C_msg fixed, keep the same, concatenate along channel dimension with cover-video latent as in the original design.\n  - CIN (invertible network) style:\n    - Set input channels to 3L; all invertible coupling blocks must be configured for C=3L.\n    - Ensure split/concat operations use channel dimension compatible with 3L. If CIN previously assumed C divisible by 4, choose L so that 3L is divisible by required factors (e.g., L ∈ {4, 8} makes 3L ∈ {12, 24}).\n- Frame-loss insertion:\n  - Compute the frame-wise penalty on Vc and Vw after post-decoder reshape back to [B, L, 3, H, W].\n  - Provide two options:\n    - Paper-form L1 per frame: LF = mean_i ||Vc[i] − Vw[i]||_1\n    - Variance-regularized form (often better aligned to “consistency”): Let s_i = MSE(Vc[i], Vw[i]); LF_var = Var({s_i}); use LF_total = α*LF + β*LF_var (α≈0.0–0.2, β≈1.0) if you observe per-frame imbalance.\n- Attack Simulation Layer (ASL) integration:\n  - Place ASL between encoder and decoder; it receives Vw (watermarked) and outputs Va (attacked).\n  - Differentiable attacks (Gaussian noise/blur, random hue, spatial crop): implement as standard tensor ops within the graph.\n  - Non-differentiable attacks (H.264):\n    - Implement forward-ASL: Va_real = h264_encode_decode(Vw) performed out-of-graph; pseudo_dist = (Va_real − Vw).detach(); Va = Vw + pseudo_dist (ensures gradients bypass non-diff attack).\n    - Use ffmpeg (libx264) CLI or PyAV. Cache compressed bitstreams on disk/ramdisk when iterating over the same sample to reduce overhead.\n- Training pipeline integration:\n  - Two-stage training:\n    - Stage 1 (no-attack pretraining): Identity-only ASL; focus on invisibility and decoding stability.\n    - Stage 2 (robustness training): sample one attack per batch from predefined set; use forward-ASL for H.264.\n  - Loss: L_total = λE*LE + λD*LD + λF*LF\n  - Keep PSNR monitoring and per-attack accuracy evaluation hooks.\n- Code-level hooks (PyTorch):\n  - Implement an ItoVAdapter(nn.Module) with forward(x: [B, L, 3, H, W]) -> [B, 3L, H, W] and inverse for visualization/metrics.\n  - Wrap existing ImageEncoder/ImageDecoder classes; only change first/last Conv2d in/out_channels to 3L.\n  - For depthwise variants, replace Conv2d with nn.Conv2d(groups=in_channels) and follow with pointwise 1x1 conv; or use torchvision ops/MobileNetV2 blocks.\n  - ASL: implement AttackSampler that returns a callable transform; for H.264, spawn a subprocess to ffmpeg with CRF argument; ensure torch.no_grad around external call; pin memory and non-blocking transfers.\n- Framework compatibility:\n  - TensorFlow/JAX: same reshape strategy; for non-diff H.264, wrap in tf.py_function / JAX host callback with gradient stop and add pseudo-distortion.\n  - Mixed precision: safe except for small-magnitude residual computations; keep loss and watermark residual computation in float32 to avoid underflow.\n- Migration path:\n  - Start from a working image model checkpoint; expand first conv weights by tiling/averaging across the new channel groups or re-init first/last layers and fine-tune.\n  - Validate with Identity ASL first to match image-message accuracy on per-frame average; then enable attacks progressively.\n- External dependencies:\n  - ffmpeg (libx264) for H.264; OBS or screen-capture stack for real-world test.\n  - PyAV optional for in-memory encode/decode.\n  - Ensure deterministic seeds for attack sampling where required (torch, numpy, random).\n\nParameter_Settings:\n- Architecture:\n  - Clip length L: 8 frames for 128x128 clips; for higher resolutions (≥720p), L=4–8 to control memory.\n  - Channels: 3L (e.g., 24 for L=8).\n  - Convolutional blocks:\n    - Recommended default: Depthwise 2D blocks for best cost-performance.\n    - Alternatives: 2D standard conv for maximum PSNR; avoid 3D/(2+1)D unless you have strong reasons.\n  - Parameter/FLOP references (from paper, 128x128 input):\n    - 2D: 20.8M params, 15.57 GFLOPs\n    - 3D: 54.2M, 234.63 GFLOPs\n    - (2+1)D: 26.2M, 115.47 GFLOPs\n    - Depthwise 2D: 5.99M, 4.55 GFLOPs\n    - Depthwise 3D: 6.08M, 28.82 GFLOPs\n- Loss weights (stage-dependent):\n  - Stage 1 (no attacks): λE=1.0, λD=0.1, λF=0.0\n  - Stage 2 (attacks on): λE=1.0, λD=0.01, λF=0.05 (start) or λF ∈ [0.03, 0.1] depending on per-frame PSNR std; if using variance LF, set α=0.0–0.1, β=1.0.\n- Optimizer and LR:\n  - Adam with lr ∈ [1e-5, 5e-5] (paper uses 1e-5), betas=(0.9, 0.999), weight_decay=0–1e-5.\n  - Warmup 1–3k steps if starting from scratch; cosine decay optional.\n- Message length m:\n  - m=96 bits as in experiments; stable range m ∈ [64, 128] on 8×128×128 clips without significant PSNR drop. Larger m increases LD difficulty and can reduce PSNR.\n- Attacks and parameters (training sampling ranges):\n  - Identity (no attack): p ∈ [0.1, 0.2]\n  - H.264: CRF ∈ [18, 26]; sample uniformly or bias to [20, 24] for best robustness-accuracy balance.\n  - Frame Average: N ∈ {3, 5}\n  - Frame Drop: p ∈ [0.3, 0.7]\n  - Frame Swap: p ∈ [0.3, 0.7]\n  - Gaussian Blur: σ ∈ [0.5, 2.0], kernel ∈ {3, 5, 7}\n  - Gaussian Noise: std ∈ [0.01, 0.05] (assuming input in [0,1])\n  - Random Crop: keep ratio ∈ [0.6, 1.0], resize back to H×W\n  - Random Hue/Saturation: hue δ ∈ [−0.1, 0.1], sat scale ∈ [0.8, 1.2]\n- Batch and scheduling:\n  - Batch size: 16 on A100 40GB for 8×128×128 with depthwise 2D; reduce to 8 on 24GB GPUs.\n  - Training length: 100–200 epochs suffices for robust model on Kinetics-600 with on-the-fly crops; ablations used 2000 epochs, but diminishing returns after 300–500 epochs with strong attacks.\n- Initialization:\n  - He/Kaiming normal for Conv layers (fan_out mode); for first Conv with 3L channels, scale std by 1/√L if initializing from an image model (to keep activation variance similar).\n  - For SE blocks/invertible blocks, keep defaults; initialize decoder last layer bias to 0.\n- Precision and stability:\n  - Use AMP (fp16/bf16) for compute; keep LD bit logits in float32; clamp outputs to [0,1] after tanh/sigmoid with ε=1e-6 to avoid PSNR inf.\n- Critical vs robust parameters:\n  - Critical: λD (too large degrades PSNR), H.264 CRF range, m, first/last Conv channel configs.\n  - Robust: choice among differentiable color/spatial attacks, Adam betas, warmup schedule.\n- Hardware-dependent settings:\n  - A100/H100: enable AMP, set num_workers=8–16, pin_memory=True; consider channels_last memory format to speed Conv2d.\n  - 24–32GB GPUs: prefer depthwise 2D; clip L=4–6 for 256×256; gradient accumulation steps=2–4 if needed.\n  - CPU budget for H.264: parallelize ffmpeg with a process pool sized to CPU cores; throttle to avoid dataloader starvation.\n\nApplication_Conditions:\n- When to use ItoV:\n  - You want to adapt an existing image watermarking model to videos with minimal engineering.\n  - Video clip lengths L ≤ 16 and spatial sizes up to 256×256 during training; deployment can scale to higher res via sliding-window or tiling.\n  - Robustness needed against H.264, frame temporal perturbations, and screen recording.\n- Tasks that benefit most:\n  - Ownership/copyright watermarking requiring robustness and invisibility.\n  - Scenarios where temporal semantics are not required for embedding (watermarking fits this).\n- Tasks where it may be neutral/less helpful:\n  - If your objective explicitly encodes/decodes temporal dynamics (e.g., time-synchronized watermark payload per-frame), 3D/(2+1)D may be preferable.\n- Hardware requirements:\n  - Minimum: single 16GB GPU for L=8, 128×128, depthwise 2D with batch size 8–12.\n  - Recommended: ≥24GB GPU or A100 40GB for batch=16 and faster H.264 simulation overlap.\n  - CPU with ≥8 cores and fast storage for ffmpeg-based ASL; SSD or RAM-disk advised.\n- Scale considerations:\n  - For high-resolution training (≥720p), either:\n    - Randomly crop 128–256 windows, or\n    - Use patch-based training with overlap and aggregate at inference.\n- Choosing among convolution blocks:\n  - Prefer depthwise 2D when memory/latency constrained and you want near-SOTA robustness with 4–5× lower FLOPs vs 2D.\n  - Standard 2D if you prioritize slightly higher PSNR and can afford ~3× FLOPs vs depthwise 2D.\n  - Avoid 3D/(2+1)D unless you have strong compute and observe a specific gain; the paper shows inferior crop robustness and higher overfitting risk.\n- Alternatives comparison:\n  - Compared to dedicated video watermarking with 3D nets (e.g., DVMark), ItoV with strong image backbones + forward-ASL for H.264 yields higher H.264 robustness with far less engineering and cost.\n  - If you cannot run ffmpeg in the loop, consider differentiable approximations to compression; expect lower robustness than forward-ASL.\n\nExpected_Outcomes:\n- Performance (from reported results; Kinetics-600, m=96, 8×128×128):\n  - PSNR: ~37–40 dB (ItoV-MBRS ~38–40 dB; ItoV-CIN ~37 dB)\n  - Bit accuracy under attacks:\n    - H.264 CRF=22: 98.4–99.7% (ItoV-MBRS ~99.7%, ItoV-CIN ~98.4%)\n    - Frame Avg/Drop/Swap: ~99.5–99.99%\n    - Gaussian Blur σ=2.0: ~99.9–100%\n    - Gaussian Noise std=0.04: ~99.99%\n    - Random Crop p=0.4: ~98.8–99.7%\n    - Random Hue p=1.0: ~99.7–100%\n- Convolution block trade-offs (relative to 2D standard):\n  - Depthwise 2D: ~4× reduction in FLOPs and ~3.5× fewer params with comparable or slightly better robustness on several attacks; PSNR may drop ~1.6 dB vs 2D but remains ~38+ dB.\n  - 3D/(2+1)D: large compute increase (7–15× FLOPs) with no robustness benefit; may hurt crop robustness.\n- Speed/compute expectations:\n  - Training speedup: depthwise 2D vs 3D yields ~5–10× faster step times on A100, mainly from FLOP reduction and better memory bandwidth.\n  - Memory reduction: depthwise 2D vs 2D standard saves ~2–3× activation memory for the same batch.\n- H.264 robustness vs CRF (from figure):\n  - Near-100% accuracy up to CRF≈20 (ItoV-MBRS), robust 90%+ up to CRF≈24; steep drop beyond CRF>25.\n- Screen recording:\n  - Expected bit accuracy: ~97–98% (stream at ~1000 kbps; OBS default).\n- Timeline:\n  - Identity-only pretrain: convergence in 5–10 epochs (decoding near-perfect).\n  - Robustness training: 50–150 epochs to stabilize across all attacks; H.264 robustness improves steadily over first 30–50 epochs.\n- Failure modes and mitigations:\n  - Overfitting with 3D/(2+1)D: strong residual patterns on early frames; fix by switching to 2D/depthwise, increase attack diversity, add stronger weight decay (1e-5–3e-5).\n  - Per-frame PSNR imbalance: visible artifacts in specific frames; increase λF to 0.07–0.1 or enable LF_var with β=1.0–2.0.\n  - H.264 brittleness above CRF 26: expand training CRF range, include bitrate-based compression, or increase λD slightly (0.01→0.02) while monitoring PSNR.\n  - Decoder collapse (LD not decreasing) when m too large: reduce m to ≤128 or increase decoder capacity modestly (add one block) while keeping encoder unchanged.\n  - Dataloader/ASL bottleneck due to ffmpeg: prefetch and cache; use process pool with bounded queue; fall back to differentiable proxy on some steps (e.g., 50% real H.264, 50% proxy).\n- Debugging indicators:\n  - Healthy training: PSNR ~35–40 dB by epoch 5–10 (stage 1); per-attack accuracies >95% on noise/blur/crop after 20–40 epochs (stage 2).\n  - Check residuals: evenly distributed low-amplitude residuals across frames; no single frame with strong residuals when LF > 0.\n  - If Random Crop accuracy lags (<97% while others are >99%): you are likely overfitting to temporal cues; ensure 2D/depthwise 2D convs and randomize crop ratio more aggressively.\n- Hardware-specific outcomes:\n  - A100/H100: expect linear scaling with batch size; depthwise 2D runs at high tensor core utilization under channels_last; 2–3× throughput vs 3D blocks.\n  - Consumer GPUs (<=12GB): use L=4, 128×128, depthwise 2D, batch=4–8; accuracy drop typically <1% if training for same wall-clock by increasing epochs.\n\nValidation procedures:\n- Unit tests:\n  - Shape round-trip: [B,L,3,H,W] → [B,3L,H,W] → [B,L,3,H,W] identity.\n  - Forward-ASL gradient check: dL/dVw equals dL/d(Vw + pseudo_dist) with pseudo_dist detached; ensure non-zero gradients into encoder/decoder.\n- Metrics:\n  - Report PSNR per-frame mean and std; aim for std ≤ 0.2 dB with LF enabled (paper shows ~0.159 dB).\n  - Evaluate bit accuracy per-attack on a held-out set of 1000 clips; match ballpark metrics listed above.\n- Reproducibility:\n  - Fix seeds; log sampled attack parameters; save ffmpeg CRF values and clips for failed cases to analyze edge distortions."
    }
]