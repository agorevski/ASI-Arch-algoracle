[
    {
        "DESIGN_INSIGHT": "### DESIGN_INSIGHT_HIGH: [Pivotal Tuning Watermarking (PTW) – Post‑hoc, Data‑Free, No‑Box Verifiable Watermark Embedding for Pre‑Trained Generators]\nPTW replaces “train-from-scratch” watermarking pipelines by a post-hoc fine-tuning procedure on a frozen copy of a pre-trained generator (the Pivot), eliminating the need for training data and preserving the original latent→image mapping. Instead of modifying the training set or loss during full GAN training, PTW clones and freezes the original generator parameters θ0, and optimizes a second, trainable copy ˆθ to embed a message while keeping images perceptually close to the Pivot outputs.\n\nThe key mechanism is a two-term objective that jointly enforces perceptual similarity and message recoverability for any latent z: with x0 = G(z;θ0) and x = G(z;ˆθ),\n\\[ L_{\\text{PTW}} \\;=\\; L_{\\text{LPIPS}}(x_0, x) \\;+\\; \\lambda_R \\cdot \\text{VERIFY}(x, \\tau, m), \\]\nwhere VERIFY is a differentiable decoding loss (binary cross-entropy between the decoder output and message m under key τ). Optimization proceeds over latents sampled from Z, updating only ˆθ via Adam while θ0 remains fixed, thereby preserving pre-trained synthesis behavior and incurring negligible degradation in utility.\n\nFundamentally different from prior “artificial fingerprinting” that (i) retrains GANs on watermarked datasets or (ii) injects loss terms into full training, PTW achieves no-box verification (extraction from a single image without access to the model) and scales to high-resolution generators (StyleGAN2/3/XL). Empirically, PTW delivers a three-orders-of-magnitude speedup (GPU months → <1 hour) by avoiding discriminator updates and dataset access, while enabling larger message capacity at comparable or lower FID impact. Conceptually, PTW turns watermarking into a PEFT-like adaptation of ˆθ constrained by a perceptual prior, rather than a property learned end-to-end during adversarial training.\n\n\n### DESIGN_INSIGHT_MEDIUM: [Differentiable Key Generation with Parameter- and Latent-Mappers – Architecture-Agnostic Message Modulation Without Re‑Training]\nThis work replaces architecture-specific message inputs and re-training with a learned, differentiable key generation procedure that discovers low-perceptual-cost embedding directions given a fixed generator. Instead of altering the generator topology, KEYGEN learns (i) a parameter-perturbation mapper θ_P that predicts a sparse additive delta on a subset of generator weights and (ii) a latent-perturbation mapper θ_Z that modulates the input code, together with (iii) a decoder θ_D that extracts n-bit messages from images.\n\nThe mechanism jointly optimizes mappers and decoder with a perceptual regularizer against the unmodified generator:\n\\[\n\\begin{aligned}\n&\\Delta\\theta \\leftarrow \\mathrm{MAPPER}_P(\\theta_P, \\theta_0, m, z), \\quad\n\\Delta z \\leftarrow \\mathrm{MAPPER}_Z(\\theta_Z, \\theta_0, m, z), \\\\\n&x \\leftarrow G(\\theta_0 + \\Delta\\theta,\\, z + \\Delta z), \\quad\nx_0 \\leftarrow G(\\theta_0,\\, z), \\\\\n&\\min_{\\theta_P,\\theta_Z,\\theta_D} \\; L_{\\text{LPIPS}}(x_0, x) \\;+\\; \\lambda_R \\cdot \\mathrm{BCE}\\big(\\mathrm{Dec}_{\\theta_D}(x),\\, m\\big).\n\\end{aligned}\n\\]\nBy constraining perturbations (≤1% parameters in practice) and optimizing against LPIPS, the key τ ≡ θ_D learns to decode messages embedded where the generator can afford imperceptible changes (e.g., semantically meaningful regions), yielding a robust, differentiable VERIFY term for PTW. Crucially, this message modulation is architecture-agnostic: it requires no generator re-training, no data, and no architectural hooks for message inputs.\n\nCompared to prior GAN fingerprinting that either stamps training data or learns weight mappers during full GAN training, this approach disentangles key generation from model training and uses PEFT-style perturbations to preserve utility. It enables longer codes at fixed FID and supports efficient no-box verification because the decoder is trained explicitly to be optimizable and differentiable with respect to the generator’s outputs.\n\n\n### DESIGN_INSIGHT_MEDIUM: [Game-Theoretic Security for No‑Box Watermarking and Reverse Pivotal Tuning (RPT) – Formal Evaluation plus a White‑Box Removal Algorithm]\nThe paper replaces ad‑hoc robustness/detectability assessment with explicit games that isolate watermark effects from generator fingerprints and quantify attacker–defender capabilities under no-box verification. Detectability is posed as a binary hypothesis test on images (watermarked vs non-watermarked) with success \\( \\mathrm{Succ}_{\\text{DETECTION}} = \\mathbb{E}[a] \\), while robustness evaluates an adaptive adversary synthesizing K images under black- or white-box access, with success\n\\[\n\\mathrm{Succ}_{\\text{EVASION}} = \\mathbb{E}\\big[ e - \\mathrm{FID}(X_0, D) \\big],\n\\]\nwhere e is the evasion rate given verification threshold κ via p-values \\( p \\le \\kappa \\) on extracted bits, and utility is measured by FID to real data D. Capacity is formalized as\n\\[\nC_{\\theta} \\;=\\; n \\cdot \\mathbb{E}_{z \\sim Z}\\big[ \\mathrm{BER}(\\mathrm{EXTRACT}(G(z;\\theta), \\tau), m) - 0.5 \\big],\n\\]\ndecoupling message length from visual distortion.\n\nBeyond evaluation, the work introduces Reverse Pivotal Tuning (RPT), a principled white-box removal attack that inverts a small set of clean images and then fine-tunes the watermarked generator to match them perceptually—thus erasing the watermark signal while preserving utility. Concretely, with inverted codes \\( z^*(x) = \\arg\\min_z L_{\\text{LPIPS}}(G(\\hat{\\theta}, z), x) \\), RPT updates \\(\\hat{\\theta}\\) by\n\\[\n\\min_{\\hat{\\theta}} \\; \\mathbb{E}_{x \\in D_R} \\big[ L_{\\text{LPIPS}}\\big( G(\\hat{\\theta}, z^*(x)),\\, x \\big) \\big],\n\\]\neschewing any VERIFY term and thereby pulling the model toward non-watermarked manifolds. Unlike prior image-space augmentations (blur/crop/JPEG) or overwriting with a second watermark (which often harms FID), RPT exploits the same pivotal tuning geometry that enables PTW, but in reverse, to reliably scrub the watermark under white-box access with minimal FID change—exposing a fundamental limitation of no-box, post-hoc generator watermarking when parameters are available to an adversary.",
        "EXPERIMENTAL_TRIGGER_PATTERNS": "Task_Performance_Signatures:\n- Image generation benchmarks and attribution metrics\n  - FFHQ-256 (StyleGAN2): Expect embedding 40 bits with negligible utility loss (FID increase ≈ +0.3 over baseline FID≈5.4); embedding 100 bits remains visually acceptable but begins to degrade utility; outperforms modified Yu1/Yu2 baselines (achieves 100 bits at similar FID cost as Yu2 at ≈20 bits).\n  - FFHQ-1024 (StyleGAN2/3/XL): Expect stable performance up to ≈40 bits with “little loss” (≤+0.5–1.0 FID); embedding 100 bits produces noticeable degradation (e.g., StyleGAN2: ≈+3 FID).\n  - AFHQv2-512 (StyleGAN2/3): Expect ≤+1.0 FID at 40 bits; embedding 100 bits increases FID by >4 points; degradation is more pronounced due to higher output diversity.\n  - Capacity extraction: For n=40-bit messages, verification at p≤0.05 should yield ≥26 correct bits on clean samples; average remaining capacity after black-box perturbations should remain >30 bits unless FID deteriorates severely.\n  - Runtime (watermark embedding): PTW yields ≈3 orders of magnitude speedup vs. training-from-scratch watermarking. Representative comparison (8×A100):\n    - From scratch (GAN training): 158–1456 GPU-hours depending on model/resolution.\n    - PTW: ≈0.5 GPU-hours per model for embedding; one-time KEYGEN (decoder) training ≈2 GPU-hours.\n  - Throughput/latency: Inference throughput of the watermarked generator remains on par with the original generator (no extra runtime modules at inference).\n- Detectability/undetectability patterns\n  - Black-box detectability (ResNet-18 detector on generated samples):\n    - With watermark capacity ≤40 bits and labeled data ≤100 per class (watermarked vs. non-watermarked), expect near-chance performance (≈0.5–0.6 accuracy).\n    - With ≥400 labeled samples per class, expect high detectability (≥0.9 accuracy), increasing with capacity (20 < 40 < 60 bits).\n    - Truncation ψ: Higher diversity (larger ψ) lowers detection accuracy; tighter truncation (smaller ψ) increases detectability.\n- Robustness patterns (no-box verification with p≤0.05; K=50k samples)\n  - Black-box attacks (crop, blur, noise, JPEG, quantization, super-resolution):\n    - Expect watermarks to remain verifiable unless FID is substantially degraded. Super-resolution is typically on the Pareto front but still fails to remove the watermark without heavy quality loss (e.g., StyleGAN2: capacity drops to ≈33 bits while FID worsens to ≈11.5).\n    - Latent sampling methods (truncation, interpolation, style-mixing) do not create safe subspaces; extraction success rate remains unchanged on average.\n  - White-box attacks:\n    - Overwriting (attacker embeds a new watermark): Removes the original watermark (capacity ≈4–5 effective bits, i.e., below decision threshold) but degrades utility (e.g., FID +≈3 on StyleGAN2; +≈6 on StyleGAN-XL).\n    - Reverse Pivotal Tuning (RPT): With only ≈200 non-watermarked real images (≈0.3% of training set), expect watermark removal at negligible utility loss (e.g., StyleGAN-XL FID from ≈2.67 to ≈3.52; StyleGAN2 from ≈5.4 to ≈5.47). With ≈80 images on StyleGAN2, removal with ≤+0.3 FID is achievable. Expect BER to exceed the 40-bit decision bound (i.e., fewer than 26 bits correct on average).\n- Comparative expectations vs. prior GAN watermarking\n  - Capacity/utility: PTW-based watermark achieves higher capacity at a given FID than modified Yu1/Yu2 on large pre-trained models; Yu1 tends to encode in background leading to visible degradation; Yu2 from-scratch consistency objectives unnecessary during PTW and underperform at scale.\n  - Efficiency: Embedding from pre-trained checkpoints is feasible on models up to StyleGAN-XL; expect ≤1 hour end-to-end per model vs. >1 GPU-month from scratch for comparable resolutions.\n- Contextual conditions and scale dependencies\n  - Improvements are most pronounced on pre-trained, high-quality generators at moderate capacities (≤40 bits).\n  - Larger image resolutions (1024²) and higher-diversity domains (AFHQv2) are more sensitive to capacities >40 bits (FID degrades faster).\n  - Training vs. inference: All performance benefits (speed, no data need) manifest during embedding; inference quality/throughput remains similar to baseline unless white-box removal is applied by an adversary.\n- Irrelevant NLP/QA benchmarks (for completeness of the template)\n  - Language modeling (lambada_openai, wikitext, ptb), reading comprehension (squad_completion, squad_v2, narrativeqa), commonsense (hellaswag, piqa, social_iqa, commonsenseqa), factual QA (arc_easy, arc_challenge, boolq, openbookqa), context resolution (winogrande, winograd), other (swde, fda): Not applicable to this image-generation watermarking method; expect no meaningful change.\n\nArchitectural_Symptoms:\n- Training characteristics during PTW embedding\n  - Loss behavior: LLPIPS(x0,x) decreases quickly and stabilizes; VERIFY/BCE loss (decoder) decreases steadily until target capacity is reliably recoverable; overall smoother convergence than from-scratch training due to absence of discriminator dynamics.\n  - Stability: No mode-collapse symptoms; robust to moderate hyperparameter changes (e.g., LR≈1e-4 for generator; decoder LR≈1e-3) without divergence; no NaN occurrences reported.\n  - Pivot similarity: Per-iteration visual diffs remain small/localized; perturbations concentrate on semantically informative regions; global content/pose preserved across iterations.\n  - Capacity traces: Bit error rate (BER) curves fall below the decision threshold early for ≤40 bits and plateau; extraction p-values drop below 0.05 on clean samples.\n- Runtime/memory behaviors\n  - Embedding loop efficiency: Short wall-clock time (≈0.5 GPU-hours per model) with only generator optimization; no discriminator or real data I/O; high and stable GPU utilization due to simple forward-backward passes over random latents.\n  - Memory profile: Similar to fine-tuning a single generator plus a frozen pivot copy; lower peak memory than adversarial training (G+D) from scratch; empirically fits batch sizes ≈128 (256²), 64 (512²), 32 (1024²) without OOM on modern GPUs.\n  - Inference: Constant memory and identical throughput vs. original generator (watermark is “baked” into weights; no per-inference mappers/aux nets required).\n- Profiling signatures indicating correct operation\n  - During KEYGEN: Mapper gradients focus on a small subset of parameters/latents (≤1% of weights perturbed) while decoder accuracy on synthetic samples climbs predictably; LPIPS–decoder loss trade-off stabilizes.\n  - During EMBED: Gradients w.r.t. generator weights remain small-magnitude; cosine similarity of activations between pivot and tuned model remains high; FID measured on periodic samples drifts minimally for ≤40 bits.\n  - Robustness profiling:\n    - Black-box perturbations: Capacity remains above decision threshold unless FID is severely impacted (e.g., SR attack shows capacity drop with concurrent FID >10).\n    - White-box RPT: LPIPS to clean real images drops quickly while verification p-values rise above 0.05; capacity collapses below threshold with minimal FID change when R≥80–200 clean images are used.\n  - Latent-space invariance checks: Capacity/extraction statistics remain stable across truncation, linear interpolation, and style-mixing; no anomalous latent regions with systematically failed extraction.\n- Negative/neutral indicators\n  - High-capacity targets (>40–60 bits) on high-res or high-diversity datasets: Observe rapidly increasing FID (+3 or more), indicating over-encoding; heatmaps become more diffuse beyond key semantic regions.\n  - Overwriting detection: When an attacker overwrites, monitoring FID drift (+3 to +6) with simultaneous collapse of verification is a signature of adversarial removal rather than natural drift.",
        "BACKGROUND": "Title: PTW: Pivotal Tuning Watermarking for Pre-Trained Image Generators\n\nHistorical Technical Context:\n- Early deep generative modeling progressed from autoencoders and VAEs to GANs, with GANs (Goodfellow et al., 2014) becoming the dominant high-fidelity image synthesis approach for faces and natural images. Style-based GANs (StyleGAN, StyleGAN2, StyleGAN3) introduced an intermediate latent space W, style modulation, and architectural refinements that dramatically improved FID and controllability. Large-scale variants (StyleGAN-XL) scaled resolution and data diversity, achieving state-of-the-art FID on benchmarks like FFHQ and ImageNet. In parallel, diffusion/transformer-based generators emerged, but many high-quality public checkpoints and evaluation practices remained anchored in the StyleGAN family.\n- Deepfake detection and attribution matured along two axes: passive detection without access to the generator (artifact/semantic inconsistency classifiers, spectrum-based detectors) and methods with access to the generator (fingerprinting and watermarking). Fingerprinting typically trains discriminators/classifiers on generated samples to attribute source models. Watermarking for GANs (e.g., Yu et al., 2019; 2021) embedded identifiable codes either by training on marked data or by modifying training objectives—both required training from scratch. Meanwhile, pivotal tuning (Roich et al., 2022) demonstrated that cloning and freezing a “pivot” generator and tuning a copy could preserve latent-to-image fidelity while enabling precise edits/inversions via perceptual losses (LPIPS), suggesting a path to post-hoc modifications without full retraining.\n- Prior watermarking works largely evaluated robustness against simple black-box image-level perturbations (crop/blur/noise/JPEG/quantize) and reported favorable capacity/utility trade-offs on relatively small generators/datasets. However, formal robustness/detectability notions tailored to no-box verification (verifying from a single image without access to the generator) were under-specified, and scalability to modern, high-resolution generators with strong FID remained an open challenge due to training cost and instability when retraining from scratch.\n\nTechnical Limitations:\n- Computational bottleneck of training-from-scratch watermarking: Prior GAN watermarking approaches require full end-to-end retraining with marked data or modified objectives, incurring O(Ttrain) compute comparable to baseline GAN training. Empirically, training StyleGAN-XL: 256^2 (≈552 GPU-hours), 512^2 (≈1285 GPU-hours), 1024^2 (≈1456 GPU-hours) on 8×A100; StyleGAN2: 256^2 (≈158h), 512^2 (≈384h), 1024^2 (≈929h); StyleGAN3: 256^2 (≈482h), 512^2 (≈662h), 1024^2 (≈1161h). This is incompatible with rapid, repeated watermark provisioning.\n- Data dependence and deployment constraints: Prior methods require access to the original training data and the discriminator, which is often infeasible due to privacy/IP restrictions. This blocks post-hoc watermarking of released checkpoints and hinders MLaaS providers that cannot ship data.\n- Limited capacity/utility trade-off on large models: Existing approaches demonstrated capacity primarily on smaller GANs. Scaling to 1024^2 generators with low FID is sensitive to perturbations; prior methods embed shorter codes or degrade FID notably, limiting practicality for high-quality deployments.\n- Lack of formal no-box verification definitions: Previous detectability evaluations often conflate watermarks with model fingerprints (e.g., different random seeds), obscuring whether a detector responds to the watermark or incidental generator-specific artifacts. Clear, game-based definitions for detectability and robustness tailored to no-box verification were missing.\n- Robustness assumptions excluding adaptive white-box attackers: Prior robustness focused on black-box image perturbations. Adaptive white-box adversaries—capable of tuning model parameters—were not thoroughly studied, leaving a gap in security guarantees when model parameters are exposed.\n- Training instability and inefficiency of prior embedding procedures: Methods that modify GAN training objectives to embed watermarks also add regularizers to mitigate mode collapse/consistency loss. Such additions increase optimization complexity and can hurt scalability to larger, higher-resolution GANs.\n\nPaper Concepts:\n- Pivotal Tuning (PT): A post-hoc fine-tuning method that preserves the latent-to-image mapping of a pre-trained “pivot” generator θ0 while optimizing a trainable copy θ1 for a target objective. The loss combines a perceptual similarity and a regularizer:\n  LPT = LLPIPS(x0, x) + λR R(x), with x0 = G(z; θ0), x = G(z; θ1).\n  Intuition: keep G(z; θ1) perceptually close to G(z; θ0) for all z, while allowing small edits guided by R(·).\n- Pivotal Tuning Watermarking (PTW): A post-hoc watermark embedding procedure for pre-trained generators. Given θ0, a secret key τ, and message m, PTW optimizes θ̂ (initialized from θ0) using:\n  \\[\n  \\min_{\\hat{\\theta}} \\mathbb{E}_{z \\sim Z}\\left[ LLPIPS\\big(G(z;\\theta_0), G(z;\\hat{\\theta})\\big) + \\lambda_R \\cdot \\text{VERIFY}\\big(G(z;\\hat{\\theta}), \\tau, m\\big) \\right],\n  \\]\n  where VERIFY computes a differentiable binary cross-entropy between the decoder’s extracted bits and m. PTW preserves visual fidelity while embedding multi-bit codes into outputs with no access to training data.\n- Optimization-based Key Generation (KEYGEN) with Mappers: The secret key τ comprises a differentiable decoder θD trained to extract messages and (during key learning) auxiliary mappers for efficient modulation. Parameter mapper θP predicts additive perturbations to a subset of generator weights; latent mapper θZ predicts perturbations to z:\n  \\[\n  \\tilde{\\theta} = \\theta_0 + \\Delta\\theta(\\theta_P, m), \\quad \\tilde{z} = z + \\Delta z(\\theta_Z, m),\n  \\]\n  and trains θD by optimizing\n  \\[\n  \\min_{\\theta_P,\\theta_Z,\\theta_D} \\mathbb{E}_{m,z}\\left[ LLPIPS\\big(G(z;\\theta_0), G(\\tilde{z};\\tilde{\\theta})\\big) + \\lambda_R \\cdot \\text{VERIFY}\\big(G(\\tilde{z};\\tilde{\\theta}), \\theta_D, m\\big) \\right].\n  \\]\n  Intuition: learn where and how to encode bits with minimal perceptual damage; return θD as the verification key.\n- Capacity and Verification Thresholding: For message length n and bit error rate BER, capacity is\n  \\[\n  C_\\theta = n \\cdot \\mathbb{E}_{z \\sim Z}\\Big[ \\text{BER}\\big(\\text{EXTRACT}(G(z;\\theta), \\tau), m\\big) - 0.5 \\Big].\n  \\]\n  Verification uses a binomial tail p-value Pr(X ≥ k | H0), with null H0 that matches arise by chance. A typical threshold κ = 0.05 requires at least k correct bits (e.g., for n = 40, k ≥ 26) to accept presence of the watermark.\n- Robustness Game (No-box Verification): Let EMBED produce θ1 from θ0, τ, m. An adversary receives either watermarked samples after evasion or clean, non-watermarked samples (decided by random coin flips), and the verifier runs VERIFY on each. Define evasion rate e as the fraction of trials where verification fails when it should succeed (or vice versa), and sample quality via FID(X0, D). Success is\n  \\[\n  \\text{Succ}_{\\text{EVASION}} = \\mathbb{E}[e - \\text{FID}(X_0, D)].\n  \\]\n  Intuition: a robust watermark should keep e low unless the attacker destroys image quality (higher FID).\n- Reverse Pivotal Tuning (RPT) Attack: An adaptive white-box removal strategy using a small set of real, non-watermarked images D. First invert x ∈ D to latent codes by\n  \\[\n  z^\\star(x) = \\arg\\min_{z} LLPIPS\\big(G(z;\\hat{\\theta}), x\\big),\n  \\]\n  then fine-tune θ̂ to minimize ∑ LLPIPS(G(z⋆(x); θ̂), x), effectively pulling the generator toward clean images and away from the embedded code. RPT removes the watermark with access to limited real data and minimal FID impact.\n\nExperimental Context:\n- The evaluation focuses on generative watermarking under no-box verification with an emphasis on efficiency, capacity/utility, detectability, and robustness. Benchmarks use state-of-the-art StyleGAN-based generators: StyleGAN2, StyleGAN3, and StyleGAN-XL, across FFHQ-256, FFHQ-1024, and AFHQv2-512. Image quality is quantified via Fréchet Inception Distance (FID) against real datasets, typically on K = 50,000 generated images (AFHQv2 uses 16,000 real images due to size). Capacity is the expected number of correctly extractable bits above chance; verification uses a binomial p-value threshold κ = 0.05.\n- The goals are to demonstrate: (i) scalability and efficiency of watermark embedding into pre-trained generators (PTW) without training data (≈0.5 GPU-hours per model after a one-time ≈2 GPU-hours decoder key training), representing a three-orders-of-magnitude speedup over training-from-scratch watermarking (hundreds to thousands of GPU-hours); (ii) improved capacity/utility trade-off, e.g., embedding 40 bits on FFHQ-256 with ≈+0.3 FID increase and supporting up to 100 bits with acceptable quality, and scaling to 1024^2 generators; (iii) undetectability under constrained attacker datasets and analysis of truncation effects; and (iv) robustness under black-box attacks (crop/blur/noise/JPEG/quantize and a new super-resolution pipeline) versus adaptive white-box adversaries (Overwriting and RPT). Results show robustness in black-box settings with Pareto-optimal super-resolution attacks still failing to remove the watermark without severe FID degradation, but fundamental vulnerability in white-box settings: RPT removes any watermark using as few as 200 clean images (≈0.3% of FFHQ) with negligible FID change (e.g., StyleGAN-XL ≈2.67 → ≈3.52). The overarching philosophy balances accuracy (reliable extraction/verification) against efficiency (compute/data) and scalability (large models, high resolution), while stressing rigorous, game-based robustness/detectability definitions specific to no-box verification.",
        "ALGORITHMIC_INNOVATION": "Core_Algorithm:\n- Replace “watermark-from-scratch” training with a post-hoc Pivotal Tuning watermarking (PTW) of a pre-trained generator. Freeze the original generator as a Pivot G(·; θ0), clone it to ˆθ, and optimize ˆθ so that, for any latent z, G(z; ˆθ) stays perceptually close to G(z; θ0) while a learned decoder D(·; τ) can extract a target n-bit message m.\n- KeyGen learns a differentiable verifier key τ by jointly training a message decoder and lightweight message mappers that produce tiny, message-conditioned perturbations to the generator’s parameters and/or input latents; only the decoder parameters τ are retained as the watermarking key.\n- Embed performs PTW: at each step, sample z, compute x0 = G(z; θ0) and x = G(z; ˆθ), and minimize LPIPS(x0, x) + λR · H(D(x; τ), m) with respect to ˆθ. No real training data or discriminator is used; only synthetic latent samples drive optimization.\n- Verify extracts bits from a single image x via D(x; τ), computes a binomial p-value against random guessing, and accepts if p ≤ κ. Scope of changes: single post-hoc fine-tuning stage on the generator parameters (all or a chosen subset); no architectural changes to G; decoder is a separate verifier-only network.\n\nKey_Mechanism:\n- The core insight is to preserve the latent-to-image mapping via Pivotal Tuning while “painting” a weak, decoder-aligned signal across the generator’s entire latent space. The LPIPS constraint anchors each tuned output to its pivot counterpart, maintaining fidelity, while the differentiable decoder provides gradients that steer subtle, semantically stable perturbations carrying bits of m.\n- KeyGen leverages optimization on the fixed generator to discover high-capacity, low-perceptibility channels (pixels/features) where small, structured perturbations can be embedded with minimal quality impact, enabling higher bit capacity at a favorable utility trade-off without retraining the generator or requiring data.\n\nMathematical_Formulation:\n- Notation. Let G: Z → X be the pre-trained generator with parameters θ0; ˆθ are the tunable parameters. Let D: X → [0,1]n be the message decoder with parameters τ that outputs bit probabilities. Let m ∈ {0,1}n be the target message; z ∼ Z the latent. Let ℓp(·,·) be LPIPS, and H(·,·) the elementwise binary cross-entropy aggregated over n bits.\n\n1) KeyGen (learning the watermarking key τ):\n- Message-conditioned mappers (parameter-efficient):\n  - Parameter mapper gP(m, z; φP) predicts ΔθP; latent mapper gZ(m, z; φZ) predicts Δz.\n  - Apply a projection ΠS that restricts the perturbation to a small parameter subset S (≤ 1% conv weights).\n- Watermarked synthesis for KeyGen:\n  - x = G(z + Δz; θ0 + ΠS(ΔθP)),  x0 = G(z; θ0).\n- Optimize φP, φZ, τ with:\n  \\[\n  \\min_{\\phi_P,\\phi_Z,\\tau}\\; \\mathbb{E}_{m \\sim \\{0,1\\}^n,\\; z \\sim Z}\\Big[ \\ell_p\\big(G(z;\\theta_0),\\; G(z+\\Delta z;\\theta_0+\\Pi_S(\\Delta\\theta_P))\\big) + \\lambda_R \\cdot H\\big(D(G(z+\\Delta z;\\theta_0+\\Pi_S(\\Delta\\theta_P)); \\tau),\\; m\\big) \\Big].\n  \\]\n- Return τ as the watermark key (mappers are discarded).\n\n2) PTW embedding (post-hoc tuning the generator):\n- For cloned initialization ˆθ ← θ0, per iteration sample z ∼ Z, compute:\n  - x0 = G(z; θ0),  x = G(z; ˆθ).\n- Optimize:\n  \\[\n  \\min_{\\hat{\\theta}} \\; \\mathbb{E}_{z \\sim Z}\\Big[ \\ell_p\\big(G(z;\\theta_0),\\; G(z;\\hat{\\theta})\\big) + \\lambda_R \\cdot H\\big(D(G(z;\\hat{\\theta}); \\tau),\\; m\\big) \\Big].\n  \\]\n- Update rule (Adam):\n  \\[\n  \\hat{\\theta} \\leftarrow \\hat{\\theta} - \\alpha \\cdot \\text{AdamGrad}\\big(\\nabla_{\\hat{\\theta}} \\; \\ell_p(x_0, x) + \\lambda_R \\cdot H(D(x;\\tau), m)\\big).\n  \\]\n\n3) Verification and decision:\n- Extract y = D(x; τ) ∈ [0,1]^n, threshold to bits \\(\\hat{m} \\in \\{0,1\\}^n\\), count matches k = n − BER(ĥm, m) · n.\n- Compute p-value under H0 (random guessing):\n  \\[\n  p = \\sum_{i=k}^{n} \\binom{n}{i} 2^{-n}.\n  \\]\n- Accept watermark if \\(p \\le \\kappa\\).\n\n4) Capacity (expected correct bits beyond chance):\n\\[\nC_{\\hat{\\theta}} = n \\cdot \\mathbb{E}_{z \\sim Z}\\Big[\\mathrm{BER}\\big(D(G(z;\\hat{\\theta});\\tau), m\\big) - 0.5\\Big].\n\\]\n\n5) Robustness/detectability games (summary metrics):\n- Evasion success:\n\\[\n\\text{Succ}_{\\text{EVASION}} = \\mathbb{E}\\big[e - \\mathrm{FID}(X_0, D)\\big],\n\\]\nwhere e is evasion rate over K challenges (Algorithm 1).\n- Detectability success:\n\\[\n\\text{Succ}_{\\text{DETECTION}} = \\mathbb{E}[a],\n\\]\nwith a the classifier accuracy distinguishing watermarked vs. clean (Algorithm 2).\n\n- Complexity. Let NG be the per-sample forward/backward cost of G (includes LPIPS feature extractor), ND the per-sample forward/backward cost of D, batch size B, iterations N:\n  - PTW time: \\(O\\big(N \\cdot B \\cdot (N_G + N_D)\\big)\\).\n  - KeyGen time: \\(O\\big(N_{\\text{key}} \\cdot B \\cdot (N_G + N_D)\\big)\\) (one-time cost).\n  - Inference time: unchanged for G; verification adds O(ND) per image.\n\nComputational_Properties:\n- Time Complexity:\n  - PTW training: O(N B (NG + ND)); two generator forwards (pivot + tuned) but only the tuned path backpropagates; LPIPS adds fixed-cost feature passes.\n  - KeyGen: O(Nkey B (NG + ND)) with additional lightweight mappers (negligible vs. NG).\n  - Inference (generation): O(NG,fw) identical to the original generator; verification adds O(ND,fw) if performed.\n- Space Complexity:\n  - Parameters: store θ0 (pivot) and ˆθ during training: O(|θ|); deploy only ˆθ (same size as θ0). Decoder τ is small (e.g., ResNet-18) and held by verifier only.\n  - Activations: O(B · (A_G + A_D + A_LPIPS)); can use activation checkpointing and mixed precision to reduce memory.\n- Parallelization:\n  - Fully data-parallel over z; pivot and tuned forwards are embarrassingly parallel. Compatible with multi-GPU and distributed data-parallel; LPIPS/decoder convolutions map efficiently to GPUs.\n  - No inter-layer dependencies beyond standard backprop; easy pipeline with overlapping compute (e.g., compute x0 in parallel stream).\n- Hardware Compatibility:\n  - GPU-friendly convolutional workloads (G, LPIPS, D). Batch size scales with VRAM; bandwidth dominated by feature maps. Works on commodity GPUs; accelerates further on tensor cores with AMP.\n  - CPU feasible for verification (D forward) but training strongly benefits from GPU.\n- Training vs. Inference:\n  - Training (PTW) is short fine-tuning (<1 hour reported) vs. months for from-scratch; requires no dataset or discriminator.\n  - Inference cost identical to original generator; verification is optional and external (one D forward per image).\n- Parameter Count:\n  - Generator: unchanged at deployment. No extra layers or heads introduced. Optional tuning can be restricted to subset S to reduce optimization footprint.\n  - Verifier: additional τ parameters stored privately; not part of deployed generator.\n- Numerical Stability:\n  - Balance λR to avoid overfitting the message at the expense of fidelity; too large λR increases artifacts, too small harms capacity.\n  - Use LPIPS (perceptual) as stabilizer to preserve identity across latents; Adam with small learning rate α and weight decay can mitigate drift.\n  - Decoder training benefits from input downscaling (e.g., 224×224) to stabilize gradients; calibrate bit-thresholds for BER.\n- Scaling Behavior:\n  - Runtime scales linearly with resolution and N; memory scales with activation size of G and LPIPS backbone.\n  - Capacity saturates as n increases; beyond a threshold, FID degrades quickly (observed on high-res models). Larger models (e.g., StyleGAN-XL) maintain better utility at moderate capacities (≈40 bits).\n  - Restricting updates to subset S (PEFT-style) improves speed and preserves utility but may reduce maximal capacity.\n- Practical Implementation Notes / Pitfalls:\n  - Sampling: train PTW across diverse z (and truncation ψ) to generalize the watermark over the latent space.\n  - Bit decision threshold and p-value: choose κ (e.g., 0.05) and n such that required k ensures low false accepts; for n=40, k≥26 suffices.\n  - LPIPS backbone choice affects both stability and speed; cache pivot features x0→ϕ(x0) when feasible to reduce compute.\n  - For very high-res generators, downsample to decoder’s resolution for D while keeping full-res for LPIPS; mismatched scales may limit capacity if n is too large.",
        "IMPLEMENTATION_GUIDANCE": "Integration_Strategy:\n- Target architectures and repositories\n  - StyleGAN2/3/XL (NVLabs reference repos). Keep the synthesis network unchanged; implement watermarking as a post-hoc pivotal-tuning pass plus a lightweight decoder.\n  - Other GAN backbones that map z → x and can be evaluated with LPIPS are compatible with minimal changes (replace the call to G(z) in the loop).\n- Modules to add or modify (PyTorch)\n  - Add a WatermarkDecoder class (e.g., ResNet18 head with sigmoid outputs for n bits) operating on 224×224 RGB.\n  - Add a KeyGenTrainer module that optimizes: parameter mapper θP, latent mapper θZ, and decoder θD (Algorithm 4). These mappers are only used to produce the key (decoder); they are not needed at inference/deployment.\n  - Add a PTWEmbedder class implementing Algorithm 3:\n    - clone and freeze the pivot generator θ0 (deep copy with requires_grad=False).\n    - keep a trainable copy ˆθ (requires_grad=True).\n    - optimize LLPIPS(G(z, θ0), G(z, ˆθ)) + λR · Verify(G(z, ˆθ), τ, m).\n  - Add a Verify function that runs decoder(x↓224) to return logits; compute bitwise BCE against target message m; for verification produce a p-value using the binomial CDF.\n  - Add a WatermarkVerifier utility implementing Algorithm 2 DETECTABILITY and Algorithm 1 ROBUSTNESS with batching.\n- Code-level placement in NVLabs codebases\n  - stylegan2-ada-pytorch: implement PTW in training/training_loop.py as a separate routine run after loading a pretrained .pkl; or add a new tool ptw_embed.py next to projector.py. Use the existing G_ema as θ0 (pivot).\n  - stylegan3: add ptw_embed.py mirroring gen_images.py but with an optimizer on the generator copy; place LPIPS in training/networks_cond.py or a utilities module.\n  - stylegan-xl: integrate similarly; use the existing inference backbone; ensure latent sampling matches the architecture (z∼N(0, I)).\n- Framework compatibility\n  - PyTorch: 1.13–2.1, CUDA 11.6+; LPIPS package (richzhang/PerceptualSimilarity).\n  - TensorFlow/JAX: port the small PTW loop and decoder; keep the pivot frozen and use a differentiable LPIPS implementation; identical losses suffice.\n- Migration from training-from-scratch watermarking (Yu1/Yu2)\n  - Yu1 (data-stamping): replace real data stamping with synthetic data stamping: stamp G(z) on-the-fly, train only decoder; embed using PTW instead of retraining the GAN.\n  - Yu2 (weight-mapper): ignore training-stability/consistency losses; train the authors’ mapper with the generator frozen on synthetic data; at embed time, sum mapper output into selected conv weights, or better, use PTW with Verify term for stability.\n- Dependencies and kernels\n  - LPIPS (AlexNet or VGG backbone); torchvision transforms; no custom CUDA kernels required beyond those already in StyleGAN repos.\n  - Optional: torch.compile for PTW loop; AMP (fp16/bf16) safe for generator forward/backward and LPIPS; keep decoder in fp32 for stable BCE.\n- Training pipeline integration\n  - Offline step (one-time per generator instance/seed): run KEYGEN for τ=θD (~2 GPU-hours).\n  - Embedding step (per watermark message): run PTW for 0.3–1.0 GPU-hours depending on resolution.\n  - Verification: packaged as a stateless function consuming a single image; exposes p-value and bit estimates.\n- Storage and deployment\n  - Persist: pivot checkpoint (θ0), watermarked checkpoint (ˆθ), decoder weights (τ), message m, and verification threshold policy (n, κ).\n  - At inference, only use ˆθ. Verification service uses τ,m; generator weights are not needed (no-box verification).\n\nParameter_Settings:\n- Decoder and PTW core hyperparameters (robust defaults)\n  - Message length (capacity): n = 20–60 bits for high quality; 100 bits possible with noticeable quality loss on some datasets.\n  - Decoder backbone: ResNet18 with final linear to n and sigmoid; input 224×224; normalize to ImageNet stats.\n  - KEYGEN (Algorithm 4):\n    - Optimizer: Adam(lr_decoder=1e-3, betas=(0.9,0.999), weight_decay=0); lr_mapper=1e-4.\n    - Loss weights: λLPIPS=1.0, λM (decoder BCE weight)=0.1.\n    - Batch size: 128 (FFHQ-256), 64 (AFHQv2-512), 32 (FFHQ-1024).\n    - Steps: 10k–30k iterations typically converge; stop when train BER ≤ 0.05 at ψ=1 sampling.\n  - Parameter mapper θP (only in KEYGEN):\n    - Scope: ≤1% of total generator params; choose conv kernels in early/mid synthesis blocks.\n    - Form: LoRA-style low-rank adapters on selected convs with rank r=4–8; scale αLoRA∈[0.5,1.0].\n    - Init: Gaussian(0, 1e-3); clamp per-layer L2 norm ≤ cP with cP ∈ [1e-3, 5e-3].\n  - Latent mapper θZ (only in KEYGEN):\n    - 2–3 layer MLP, widths 256–512, GELU, output matches z dimension; residual add to z.\n    - Init: Xavier normal; L2 norm clamp ≤ cZ with cZ ∈ [0.02, 0.1].\n  - PTW embedding (Algorithm 3):\n    - Optimizer: Adam(lr_gen=1e-4, betas=(0.9,0.999), weight_decay=0).\n    - Loss weights: LLPIPS + λR·Verify; λR ∈ [0.05, 0.5] for 20–60 bits; λR ∈ [0.5, 1.2] for 60–100 bits.\n    - Iterations: 3k–8k (FFHQ-256), 5k–12k (AFHQv2-512), 8k–16k (FFHQ-1024).\n    - Batch size: 128/64/32 as above; use AMP for memory.\n- Verification settings\n  - Thresholding: Compute p = ∑i=k..n C(n,i) 0.5^n; accept if p ≤ κ with κ=0.05.\n  - For n=40, require k≥26 correct bits to accept.\n  - Preprocess: center-crop to square, resize to 224, no augmentations.\n- Sampling and truncation\n  - Default z∼N(0, I), truncation ψ ∈ [0.7, 1.1]. Detection becomes easier at very low ψ; prefer ψ≈1 in eval and deployment.\n- Scale-dependent guidance\n  - FFHQ-256: target 40–80 bits; λR≈0.1–0.3; expect small FID change.\n  - AFHQv2-512: target 20–60 bits; λR≈0.1–0.25; diversity is higher, so increase N by ~25%.\n  - FFHQ-1024: target 20–40 bits; λR≈0.1–0.2; decoder downscales to 224, so ROI capacity saturates sooner.\n- Hardware-dependent settings\n  - A100 40/80GB: use batch sizes above; AMP bf16/fp16; set cudnn.benchmark=True; torch.backends.cuda.matmul.allow_tf32=True.\n  - RTX 3090/4090 24GB: halve batch sizes; gradient accumulation x2; keep decoder in fp32; set torch.set_float32_matmul_precision('high').\n  - V100 16GB: batch sizes 64/32/16; increase iterations by ~20% to match convergence.\n\nApplication_Conditions:\n- When to use PTW\n  - You have a high-quality pretrained GAN and no access to its original training data or discriminator.\n  - Need no-box verification: verify provenance from a single image without model access.\n  - Service constraints discourage request logging/monitoring; watermarking provides post-hoc attribution with minimal quality impact.\n- Beneficial scenarios\n  - StyleGAN2/3/XL on FFHQ-256/1024 and AFHQv2-512; generators with FID ≤ 6 benefit most.\n  - Multi-tenant deployment where each tenant receives a unique m; per-tenant embedding completes in <1 GPU-hour at 1024².\n- Hardware requirements\n  - Minimum: single 24GB GPU (e.g., RTX 3090) for 1024² PTW with batch=16–32; 12GB suffices for 256² with batch≥64.\n  - LPIPS and decoder inference are light; verification can run on CPU if needed (<5 ms/image with ResNet18).\n- Scale considerations\n  - Capacity vs. perceived quality: keep ≤40 bits for 1024² faces to maintain FID within +0.5; 60–100 bits introduce visible artifacts on some samples.\n  - If you need >64 bits reliably, consider sharding across multiple independent decoders or multi-image verification rather than increasing λR.\n- Task compatibility\n  - Strong for photorealistic synthesis with spatially consistent structure; weaker for highly diverse domains with extreme pose/texture variance unless you increase N and reduce capacity.\n  - Neutral to downstream editing operations (latent walks, style mixing); watermark persists throughout latent space.\n- Alternatives and trade-offs\n  - Compared to data-stamping (Yu1): PTW avoids retraining and is more stable on large models; Yu1 can work if retraining is acceptable and data is available.\n  - Compared to weight-mapper training (Yu2): PTW yields better capacity/utility on large models; Yu2 can be integrated but degrades FID faster at equal capacity.\n- Resource-constrained cases\n  - Use AMP + gradient accumulation; reduce batch sizes; run KEYGEN once and reuse τ per model/seed; embed messages in short runs (e.g., 3k steps) for ≤40 bits.\n\nExpected_Outcomes:\n- Performance and runtime\n  - Embedding time: ~0.5 GPU-hours on FFHQ-256 (A100); ~0.6–1.0 GPU-hours on FFHQ-1024 for ≤40 bits.\n  - Speedup vs. retraining: ≈10^3× (e.g., StyleGAN-XL FFHQ-256 ~552 GPU-hours from scratch vs. <1 GPU-hour PTW).\n- Quality and capacity trade-offs (FID measured on 50k samples unless dataset smaller)\n  - FFHQ-256, StyleGAN2 baseline FID ≈5–6:\n    - 40 bits: ΔFID ≈ +0.3 (typical), acceptance rate near 100% at κ=0.05.\n    - 100 bits: ΔFID ≈ +1–2; artifacts begin to be perceptible on some samples.\n  - FFHQ-1024, StyleGAN2/3/XL baseline FID ≈2–3:\n    - 20–40 bits: ΔFID ≈ +0.2–0.6.\n    - 100 bits: ΔFID ≈ +2–3 (avoid unless necessary).\n  - AFHQv2-512:\n    - 40 bits: ΔFID ≈ +0.5–0.9.\n    - 100 bits: ΔFID > +4 on average; not recommended for quality-sensitive use.\n- Detectability expectations (adversary-size dependent)\n  - With ≤100 labeled images (balanced watermarked/non-watermarked) and ψ≈1:\n    - n=20–40 bits: unseen-image detection accuracy commonly ≤0.65.\n  - With ≥400 labeled images: detection accuracy can reach ≥0.9 even at 40–60 bits.\n  - Truncation ψ: lower ψ increases detectability; keep ψ≈1 in deployment for lower detectability.\n- Robustness in common black-box postprocessing\n  - Cropping (ρ≥0.9), JPEG (q≥80), Gaussian noise (σ≤0.05), quantization (q≥0.5) and moderate blur reduce capacity by 0–10 bits but typically keep verification above threshold at κ=0.05 for n≈40.\n  - Super-resolution attack (downscale ρ=0.125–0.5 then SR×4): lies on Pareto front but does not fully remove watermark without severe FID degradation (FID often >10).\n- White-box risks (for evaluation and policy planning)\n  - Overwriting with a different decoder can remove the watermark but often increases FID by ≈+3 to +6.\n  - Reverse Pivotal Tuning using a small set of clean real images (e.g., 200) can remove the watermark with minimal FID change (<+1 on StyleGAN-XL FFHQ-1024). Plan governance accordingly if distributing weights.\n- Debugging and validation\n  - During PTW:\n    - LLPIPS between G(z, θ0) and G(z, ˆθ) should decrease to ≤0.2–0.3 (AlexNet LPIPS) while Verify loss plateaus near target BER ≤ 0.1.\n    - Bit extraction on random z should stabilize with ≥k correct bits for your n (e.g., ≥26/40).\n  - After embedding:\n    - Run DETECTABILITY with R1=R2=100; expect classifier accuracy near chance for n≤40 at ψ≈1.\n    - Run ROBUSTNESS with K=50k; compute Cθ and FID(D, X). Watermark should hold against benign transforms.\n- Failure modes and remedies\n  - Excessive quality drop: lower λR by 2×, increase N by 1.5×; consider reducing n by 10–20 bits.\n  - Poor extraction consistency across z: increase KEYGEN steps by 10–20%; expand mapper coverage slightly (<1%→~1.5% params) during KEYGEN only; ensure balanced bit sampling.\n  - Decoder overfits pivot artifacts (detectable): add light augmentations in KEYGEN (±2% scale/shift), and random ψ∈[0.9,1.1] during PTW.\n  - Capacity collapse at high resolution: ensure input to decoder is consistently downscaled to 224; consider multi-scale decoder (e.g., pyramid pooling) if pushing beyond 40 bits at 1024².\n- Hardware-specific outcomes\n  - A100 80GB: largest batches, shortest runtimes; expect ~0.4 GPU-hours for FFHQ-256 40-bit embedding.\n  - RTX 3090: 1.0–1.5× longer PTW than A100; KEYGEN ~2–3 GPU-hours.\n- Troubleshooting checklist\n  - Pivot frozen: confirm requires_grad=False for θ0.\n  - Only ˆθ updated by optimizer; LPIPS network and decoder are eval-mode during PTW.\n  - Deterministic BER evaluation uses sigmoid>0.5 threshold; ensure no mixed-precision underflow in decoder.\n  - p-value computation uses correct n and bit counts; validate with known positives/negatives.\n- Validation procedures\n  - Capacity/utility sweep: λR in [0.05, 1.0], n∈{20,40,60,100}; plot Cθ vs FID with 3 seeds; confirm monotonic trends similar to reported figures.\n  - Detectability vs dataset size: R1=R2 in {32, 64, 128, 256, 512, 1024}; ψ∈{0.7, 1.0, 1.2}; confirm accuracy rises with R and declines with larger ψ.\n  - Black-box robustness suite: crop, JPEG, noise, quantize, blur, SR; report remaining capacity and FID; Pareto front should match qualitative trends above.\n\nQuality Requirements:\n- Provide reproducible seeds and log:\n  - Save θ0, ˆθ, τ, m, λR, n, optimizer states, and random seeds.\n- Common issues and fixes\n  - Instability with AMP: keep LPIPS and decoder in fp32; cast only generator forward/backward to amp.\n  - StyleGAN-XL memory spikes: disable gradient checkpointing for final blocks; reduce batch by 2×, increase steps accordingly.\n- Risks and limitations\n  - White-box access to ˆθ allows adaptive removal with limited clean data; do not distribute ˆθ broadly if strong provenance guarantees are required.\n  - Watermark detectability increases with large labeled sets; periodically rotate m and/or τ per deployment epoch if adversarial dataset growth is anticipated."
    }
]