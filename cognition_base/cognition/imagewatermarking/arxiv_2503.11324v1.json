[
    {
        "DESIGN_INSIGHT": "### DESIGN_INSIGHT_HIGH: [Adaptive Scale Interaction over Multi-Scale VQVAE Residuals – Learned Top-k Cross-Scale Watermark Injection]\nSafe-VAR replaces fixed or post-hoc watermark placement with an adaptive, learned injection strategy operating directly in the VAR model’s multi-scale residual token space. Instead of embedding a watermark at a predetermined scale (as in diffusion noise-space or decoder-side methods), Safe-VAR encodes both the cover image and watermark via a shared multi-scale VQVAE codebook and selects the optimal cross-scale pairings for fusion.\n\nThe key mechanism is a per-scale selector that computes affinities between watermark residual maps \\(M_i\\) and cover residual maps \\(C_j\\). For each pair, the concatenation \\(A_{i,j}=[M_i \\| C_j]\\) is processed by an MLP and Softmax to produce selection weights:\n\\[\nw_{i,j} = \\text{Softmax}(\\text{MLP}(A_{i,j})), \\quad i,j \\in \\{1,\\ldots,K\\}.\n\\]\nFor each \\(i\\), Safe-VAR selects the top-\\(k\\) indices \\(\\{a_i^1,\\ldots,a_i^k\\}\\) and forms the set \\(\\{A_{i,a_i^1},\\ldots,A_{i,a_i^k}\\}\\) used downstream. This adaptivity aligns watermark complexity and image content with the most compatible resolution levels, minimizing distortion and maximizing recoverability.\n\nFundamentally different from diffusion-native watermarking (initial-noise tagging or decoder modulation) and post-hoc schemes, this approach exploits VAR’s next-scale prediction structure and the cumulative residual reconstruction \n\\[\nF_k=\\sum_{m=1}^{k}\\varphi(R_m,(h,w)),\n\\]\nembedding in the discrete residual token maps where VAR operates. The result is a content- and watermark-aware injection that respects the sequential, multi-resolution generative pathway of AR models, reducing quality degradation without sacrificing robustness.\n\n### DESIGN_INSIGHT_HIGH: [Cross-Scale Fusion with Routed Mixture-of-Heads + Mixture-of-Experts – Sparse, Conditional Composition of Watermark–Image Features]\nSafe-VAR replaces standard multi-head attention and dense FFNs with a cross-scale fusion module that couples Mixture-of-Heads (MoH) attention and Mixture-of-Experts (MoE) feedforward layers, both governed by routers. The MoH uses shared heads for common cross-scale structure and routed heads activated conditionally per input. Given selected residual maps \\(A_{i,a_i^s}\\), MoH computes:\n\\[\nB_{i,a_i^s}=\\sum_{m=1}^{h} g_m\\, H_m(W_o^m) = \\text{MoH}(A_{i,a_i^s}, A_{i,a_i^s}),\n\\]\nwhere \\(g_m\\) is the head routing score and \\(H_m\\) denotes per-head attention with projections \\(W_o^m\\).\n\nThese MoH outputs pass through a sparse MoE FFN. A router assigns each token to a subset of \\(N\\) experts, producing:\n\\[\nB^{*}_{i,a_i^s}=\\sum_{n=1}^{N} g_n\\, \\text{MoE}_n(B_{i,a_i^s}) + B_{i,a_i^s},\n\\]\nwith gating \\(g_n\\) for expert selection. Finally, scale-wise fusion applies the adaptive selector’s weights:\n\\[\nR_i=\\sum_{s=1}^{k} w_{i,a_i^s}\\, B^{*}_{i,a_i^s}.\n\\]\nThis design departs from prior VAR/diffusion pipelines by introducing conditional sparsity at both attention and FFN stages, targeted explicitly at multi-resolution watermark–image interactions. It improves fusion fidelity and stability under varying watermark complexities while conserving compute via routed activation (effective cost depends on active heads/experts rather than the full set), and is further regularized by a balanced load objective:\n\\[\nL_{\\text{MoE}} = N \\cdot \\sum_{n=1}^{N} \\text{load}_n \\cdot P_n,\n\\]\nwhich prevents expert collapse and stabilizes training.\n\n### DESIGN_INSIGHT_MEDIUM: [Two-Stage Fusion Attention Enhancement (Spatial→Channel) – Aggregation of Cross-Scale Residuals into Final Feature \\(F\\)]\nSafe-VAR augments the standard VAR decoding path by inserting a two-stage attention refinement over the fused residual maps \\(\\{R_1,\\ldots,R_K\\}\\). First, spatial attention enhances intra-map spatial dependencies, denoising and aligning watermark structures with image geometry. Second, channel attention reweights feature channels to emphasize watermark-bearing components while suppressing artifacts.\n\nOperationally, each \\(R_i\\) is processed as:\n\\[\n\\tilde{R}_i = \\text{CA}(\\text{SA}(R_i)),\n\\]\nand aggregated into a final fused feature representation \\(F\\) (e.g., sum or learned weighted sum over \\(i\\)), which is then decoded to the watermarked image. Unlike single-stage attention or purely convolutional refinements, this sequential spatial→channel design targets the distinct roles of spatial coherence and feature selectivity in watermark embedding within VAR’s residual space. It synergizes with the adaptive selector and MoH+MoE fusion to preserve perceptual quality while maintaining high watermark extraction accuracy:\n\\[\nL_{\\text{rec}}=\\lambda_{\\text{im}}\\|I-\\hat{I}\\|_2^2 + \\lambda_{\\text{wm}}\\|W-\\hat{W}\\|_2^2,\\quad \nL_{\\text{total}}=\\lambda_{\\text{rec}}L_{\\text{rec}}+\\lambda_{\\text{perc}}L_{\\text{perc}}+\\lambda_{\\text{adv}}L_{\\text{adv}}+\\lambda_{\\text{MoE}}L_{\\text{MoE}}.\n\\]\nThis module is a significant refinement rather than a wholesale change of computation, but it is pivotal in converting fused residuals into high-fidelity, watermark-robust features suitable for VAR decoding.",
        "EXPERIMENTAL_TRIGGER_PATTERNS": "Task_Performance_Signatures:\n- Image watermarking quality on text-to-image AR generation:\n  - Expect improved performance on LAION-Aesthetics, LSUN-Church, ImageNet when using Safe-VAR compared to generative baselines (Stable Signature, Safe-SD), especially at higher resolutions.\n  - Cover/Watermarked image pair (LAION-Aesthetics):\n    - At 1024×1024: PSNR ≈ 32.5, SSIM ≈ 0.918, LPIPS ≈ 0.104, RMSE ≈ 6.66; improvements vs Safe-SD (512) of +15–20% in PSNR/SSIM while maintaining lower LPIPS.\n    - At 512×512: PSNR ≈ 29.2, SSIM ≈ 0.861, LPIPS ≈ 0.151; expect 15.6% PSNR and 16.8% SSIM improvement over Safe-SD at the same resolution (as reported).\n    - At 256×256: PSNR ≈ 24.6, SSIM ≈ 0.779, LPIPS ≈ 0.151; competitive with generative methods, but post-hoc methods may show higher PSNR/SSIM at low resolution; expect within 1–5% of post-hoc metrics while preserving generative security.\n  - Watermark/Recovered watermark image pair (LAION-Aesthetics):\n    - At 1024×1024: PSNR ≈ 39.7, SSIM ≈ 0.980, LPIPS ≈ 0.058, RMSE ≈ 2.74; robust recovery markedly above generative baselines.\n    - At 512×512: PSNR ≈ 33.95, SSIM ≈ 0.943, LPIPS ≈ 0.086; +18–20% improvements in PSNR/SSIM over Safe-SD.\n    - At 256×256: PSNR ≈ 28.22, SSIM ≈ 0.901, LPIPS ≈ 0.102; similar to or slightly below Safe-SD at equal resolution; expect within ±2% of baseline.\n- Robustness under perturbations (LAION-Aesthetics):\n  - Expect improved performance on crop_30%, rotate_±40°, blur_k=9, brightness_±0.4, Gaussian noise σ=0.3, erase_30%, jpeg_quality=50, combined attacks.\n  - Quantitative expectations vs Safe-SD:\n    - FID reduced from ~5.8–6.7 to ~3.06–4.60 (≈35–50% reduction depending on attack).\n    - PSNR of extracted watermarks ≈ 32.4–33.6 across most attacks (vs ~28.1–28.3 for baseline).\n    - LPIPS improved to ≈ 0.084–0.094 (vs ~0.133–0.147 baseline).\n  - Most pronounced gains under composite attacks and noise-based distortions; expect PSNR ≥ 32.6 under noise σ=0.3.\n- Generalization:\n  - Expect improved/stable performance on zero-shot out-of-domain QR codes: preserved fine-grained details compared to Safe-SD; visual differences remain subtle, with recovered codes legible under standard perturbations.\n- Ablation-driven triggers:\n  - ASIM presence yields ≈ +2.9% PSNR and +2.4% SSIM vs w/o ASIM; CSFM yields ≈ +3.8% PSNR and +3.5% SSIM vs w/o CSFM; FAEM yields ≈ +0.4–0.7 PSNR and +0.2–0.4 SSIM vs w/o FAEM.\n  - Adaptive scale selection (Top-4) outperforms fixed-scale fusion; K=4 optimal yields PSNR/SSIM maxima; K>8 shows mild degradation due to redundancy.\n- Efficiency pattern descriptions:\n  - Performance remains competitive on watermark fidelity while achieving higher sampling efficiency typical of AR next-scale prediction (order-of-magnitude fewer denoising steps than diffusion); expect noticeably faster inference throughput vs diffusion baselines at similar resolutions.\n- Contextual conditions:\n  - Improvements most pronounced at high resolutions (512–1024), on complex watermarks (logos, QR), and under multi-attack scenarios.\n  - Training vs inference: recovery PSNR/SSIM gains appear early in fine-tuning (after discriminator activation ≈10k steps) and stabilize with MoE load balancing.\n- Language modeling, reading comprehension, commonsense reasoning, factual QA, context resolution, other tasks (non-target for this work):\n  - lambada_openai, wikitext, ptb: Expect unchanged performance (orthogonal to image watermarking); within ±1% of baseline if measured.\n  - squad_completion, squad_v2, narrativeqa: Expect unchanged performance; any variance within typical run-to-run noise.\n  - hellaswag, piqa, social_iqa, commonsenseqa: Expect unchanged performance; no capability change expected.\n  - arc_easy, arc_challenge, boolq, openbookqa: Expect unchanged performance; within ±1% of baseline.\n  - winogrande, winograd: Expect unchanged performance.\n  - swde (structured extraction), fda (data augmentation): Not directly applicable; no measurable impact expected.\n\nArchitectural_Symptoms:\n- Training characteristics:\n  - Loss curve behaviors: Lrec decreases smoothly after discriminator activation (~10k steps); Lperc and Ladv stabilize without oscillations typical of GAN-trained decoders; combined Ltotal shows faster initial convergence and reduced variance compared to variants without ASIM/CSFM/FAEM.\n  - Stability indicators: No NaN occurrences; less likely to diverge when varying λperc, λadv within standard ranges (e.g., 0.05–0.2); balanced MoE load (LMoE term stays small with λMoE ≈ 0.02) and absence of expert collapse (routing probabilities not peaking at a single expert).\n  - Scale selection entropy: ASIM softmax weights exhibit peaked but non-degenerate distributions (Top-4 scales dominate with stable weights across batches), indicating robust adaptive selection; fixed-scale runs show higher variance and poorer metrics.\n  - Ablation signatures: Removing ASIM/CSFM produces noisier attention weight distributions and slower convergence; FAEM removal reduces spatial/channel attention activations and yields lower watermark PSNR.\n- Runtime/memory behaviors:\n  - Memory scaling: Linear growth with number of selected scales (K=4 recommended); training at 256×256 with batch size 60 proceeds without OOM on standard 24–48 GB GPUs; fine-tuning at 512/1024 with batch size 5 stable without OOM (consistent with reported setup).\n  - Throughput patterns: Next-scale AR prediction exhibits higher token throughput than diffusion’s iterative denoising; inference latency scales approximately with number of scales rather than hundreds of diffusion steps; consistent speed improvements at larger resolutions relative to diffusion baselines.\n  - Hardware utilization: Higher sustained GPU utilization during MoH/MoE layers due to parallel routed heads/experts; efficient memory bandwidth usage from upsample-and-sum residual reconstruction; no OOM at larger batch sizes in Stage-1 (bs=60 at 256).\n- Profiling signatures:\n  - Attention head usage: MoH routing selects a mix of shared and routed heads (hs=3, hr=3 optimal); head utilization histograms show balanced participation, with shared heads always active and routed heads varying per sample.\n  - Expert gating: With N=1–2 experts selected, gating probabilities remain distributed (no saturation), and per-expert token fraction (load) avoids spikes; increasing N≥3 yields diminishing returns and minor performance drops, detectable via small increases in LMoE or reduced PSNR.\n  - Decoder–discriminator interplay: PatchGAN discriminator loss stabilizes without mode collapse; gradients remain bounded; feature maps from FAEM show increased spatial and channel attention activations correlating with higher SSIM and lower LPIPS.\n  - Robustness profiling: Under perturbations, FID drops by ≈35–50% vs baseline; PSNR remains ≥32.4 on most attacks; LPIPS ≤0.094; combined attack runs maintain metrics close to single-attack averages, indicating strong resilience.\n  - Baseline comparison: Safe-VAR exhibits lower pixel-wise difference maps (×10 visualization) than post-hoc baselines and preserves fine-grained watermark structure in recovery images; QR code preservation visible in qualitative grids, confirming cross-domain generalization.\n\nPotential neutral/negative effects:\n- At low resolution (256×256), post-hoc methods may show higher PSNR/SSIM on cover/watermarked pairs; Safe-VAR remains within 1–5% and trades slight metric deficit for embedded security and robustness.\n- Selecting too many scales (K≥8 or K=14) introduces redundancy/interference, leading to mild PSNR/SSIM degradation and slower training; profiling reveals flatter ASIM weight distributions and increased cross-scale fusion overhead.\n- Excess experts (N≥3) increase compute without gains; may slightly worsen LMoE and watermark fidelity; recommended N=1–2.",
        "BACKGROUND": "Title: Safe-VAR: Safe Visual Autoregressive Model for Text-to-Image Generative Watermarking\n\nHistorical Technical Context:\n- Early image generation progressed from GANs, which offered fast sampling but suffered from instability and limited controllability, to diffusion models that produced high-fidelity images at the cost of hundreds to thousands of denoising steps. In parallel, the Transformer architecture displaced RNNs/LSTMs in language, bringing self-attention’s global receptive field and next-token prediction as a unifying paradigm. The rise of large language models (LLMs) established autoregression (AR) as a scalable training and inference strategy.\n- Bridging language and vision, text-to-image generation explored autoregressive tokenization of images via vector quantization (e.g., VQ-VAE/VQGAN) so images could be modeled as sequences akin to text. Recent visual autoregressive models (e.g., VAR’s next-scale prediction, HART’s hybrid tokens, Fluid’s continuous tokens) improved efficiency and sampling quality by predicting multi-scale residual maps sequentially. Meanwhile, watermarking for AI-generated images matured along two branches: post-hoc methods that embed after generation (low security, potential quality loss) and generation-phase diffusion-native methods (robust but slow, diffusion-specific).\n- Prior generative watermarking has largely been diffusion-native, including strategies embedding watermarks in initial noise (Tree-Ring), modifying latent decoders (Stable Signature, FSW), or constraining imperceptible structure-related pixels (Safe-SD). However, these methods presume diffusion’s parallel denoising dynamics and cannot directly transfer to AR’s sequential, token-based or scale-based generation. Moreover, binary string watermarking is vulnerable to DDIM inversion, overlay, and backdoor attacks, motivating robust graphical watermarking integrated with AR generation.\n\nTechnical Limitations:\n- Architectural mismatch: Diffusion-native watermarking exploits the noise schedule and parallel score updates; AR generators proceed sequentially over tokens/scales. Directly porting diffusion strategies to AR leads to poor stability and weak fidelity due to incompatible generation dynamics.\n- Scale selection sensitivity: In next-scale AR, embedding at different scales causes markedly different distortions; the optimal injection scale depends on watermark complexity and image content (as visualized by pixel-wise difference maps). Fixed-scale policies yield inconsistent quality and fidelity.\n- Computational and memory costs: Diffusion incurs O(T) iterative steps (T≈50–1000), while AR reduces steps but still faces O(L^2) attention complexity and memory for sequence length L. Naive all-to-all multi-scale concatenation/fusion over K scales is O(K^2) in pairing, which is wasteful and harms scalability.\n- Robustness gaps of prior watermark types: Binary string watermarks are susceptible to DDIM inversion, overlays, and backdoors. Ensuring robustness of graphical watermarks under real-world perturbations (crop, rotate, blur, noise, JPEG, erase, and combinations) remains challenging.\n- Optimization instability in sparse routing: MoE-based fusion can suffer from expert collapse or load imbalance, degrading training stability and generalization without explicit balancing terms.\n- Quality–traceability trade-off: Watermark embedding can increase perceptual error or reduce SSIM/LPIPS. Prior methods often degrade cover-image quality to achieve recoverability, lacking adaptive mechanisms to minimize visible/invisible artifacts while maintaining extraction accuracy.\n\nPaper Concepts:\n- Multi-scale VQ-VAE residual maps: A VQ-based encoder produces K discrete residual maps {R1,…,RK} at decreasing resolutions that progressively reconstruct feature map F via upsampling and summation. Formally,\n  F_k = ∑_{i=1}^k ϕ(R_i, (h, w)),\n  where ϕ(·) bilinearly upsamples R_i to (h,w). The decoder reconstructs the image from F_K. Intuitively, early scales capture coarse structure; later scales refine details.\n- Next-scale autoregressive likelihood: The VAR Transformer predicts the next residual R_k conditioned on previous residuals and text embedding Ψ(t):\n  p(R_1,…,R_K) = ∏_{k=1}^K p(R_k | R_{<k}, Ψ(t)).\n  This “next-scale” factorization aligns with hierarchical image structure, improving efficiency and quality compared to flat token sequences.\n- Adaptive Scale Interaction Module (ASIM): Learns to select, for each watermark residual M_i, the top-k most compatible cover-image residual scales C_{a_i1},…,C_{a_ik} via concatenation Ai,j=[M_i; C_j], pooling, MLP, and softmax weight\n  w_{i,j} = Softmax(MLP(A_{i,j})).\n  The top-k indices {a_i1,…,a_ik} minimize distortion while maximizing recoverability, reducing pairing complexity from O(K^2) to O(Kk) and adapting to watermark/image complexity.\n- Cross-Scale Fusion Module (CSFM) with MoH and MoE: Selected residuals A_{i,a_ij} are fused by (1) Mixture-of-Heads attention (shared + routed heads) producing\n  B_{i,a_ij} = MoH(A_{i,a_ij}, A_{i,a_ij}) = ∑_{m=1}^h g_m H_m(W_o^m),\n  where g_m are head routing scores; then (2) Mixture-of-Experts FFN layers with sparse gating\n  B^*_{i,a_ij} = ∑_{n=1}^N g_n MoE_n(B_{i,a_ij}) + B_{i,a_ij}.\n  Finally, weighted aggregation yields the fused residual per scale\n  R_i = ∑_{s=1}^k w_{i,a_is}·B^*_{i,a_is}.\n  Intuitively, MoH captures cross-scale interactions and content–watermark relations; MoE routes to specialized experts to stabilize and enrich fusion.\n- Fusion Attention Enhancement Module (FAEM): Applies two-stage attention over {R_1,…,R_K}. Spatial attention refines spatial dependencies; channel attention reweights channels to emphasize watermark-consistent features, producing a final fused representation F aggregated over scales and passed to the decoder. This boosts imperceptibility and extraction robustness.\n- Training objectives and MoE balancing: A U-Net extractor T(·) predicts ˆW=T(ˆI) from the watermarked image ˆI. Reconstruction loss\n  L_rec = λ_im ||I−ˆI||_2^2 + λ_wm ||W−ˆW||_2^2,\n  promotes image fidelity and watermark recovery. MoE load-balancing loss mitigates expert collapse:\n  L_MoE = N·∑_{i=1}^N load_i·P_i,\n  where load_i is token fraction routed to expert i and P_i is its routing probability. The full objective\n  L_total = λ_rec L_rec + λ_perc L_perc + λ_adv L_adv + λ_MoE L_MoE,\n  combines LPIPS perceptual loss (λ_perc=0.1), PatchGAN adversarial loss (λ_adv=0.1), and balancing (λ_MoE=0.02); typically λ_im=λ_wm=1.\n\nExperimental Context:\n- The evaluation targets generative watermarking within AR text-to-image models, balancing image quality, watermark invisibility, and robustness. Training uses LAION-Aesthetics (≈200k images), with LSUN-Church and ImageNet as test sets for generalization. Experiments pair 5,000 cover images with 5,000 graphical logo watermarks (Logo-2K). Zero-shot out-of-domain tests use QR Codes to probe robustness beyond training distribution.\n- Metrics emphasize both cover–watermarked fidelity and watermark–recovered fidelity: PSNR, MAE, RMSE, SSIM, and LPIPS; FID is reported under perturbation attacks. Safe-VAR is compared against post-hoc (HiDDeN, Baluja et al.) and diffusion-native generative methods (Stable Signature, Safe-SD), across resolutions 256, 512, and 1024. Results show state-of-the-art performance: at 1024, cover–watermarked PSNR≈32.5 and SSIM≈0.918 on LAION-Aesthetics, and watermark–recovered PSNR≈39.7 and SSIM≈0.98. Under perturbations (crop 30%, rotate ±40°, blur k=9, brightness ±0.4, noise σ=0.3, erase 30%, JPEG q=50, and combined), Safe-VAR attains lower FID (e.g., ≈3.09 under combined vs ≈5.82 for Safe-SD) and higher PSNR (≈33.6 under combined). The evaluation philosophy stresses scalability (AR sampling efficiency), invisibility (high PSNR/SSIM, low LPIPS), and robustness, with zero-shot generalization to out-of-domain graphical patterns (QR codes).",
        "ALGORITHMIC_INNOVATION": "Core_Algorithm:\n- Replace the “single fixed-scale injection” paradigm in VAR-based image generation with an adaptive, cross-scale watermark embedding pipeline that operates on multi-scale VQVAE residual maps. Specifically, Safe-VAR inserts three modules between the encoder and decoder: (1) Adaptive Scale Interaction Module (ASIM) to score and select top-k watermark–image scale pairs, (2) Cross-Scale Fusion Module (CSFM) to fuse selected pairs via routed mixture-of-heads attention and sparse mixture-of-experts feed-forward routing, and (3) Fusion Attention Enhancement Module (FAEM) to refine fused residuals with spatial and channel attention.\n- Steps: encode cover image and watermark into K-scale residual maps using a shared multi-scale VQVAE; compute pairwise selection weights w_{i,j} between watermark scale i and image scale j; pick top-k image scales per watermark scale; apply MoH attention on each selected pair, then MoE FFN routing; aggregate with learned weights to produce fused residuals R_i; refine via FAEM and decode to the watermarked image.\n- Fundamental change: instead of injecting a watermark at a hand-chosen scale or via post-hoc pixel space operations, Safe-VAR adaptively embeds in the residual-token domain aligned with VAR’s next-scale prediction, and fuses watermark–image content by conditional routing across both attention heads and experts.\n- Scope: modules are applied once per generation pass at the multi-scale residual level (not per Transformer layer), with routing decisions per selected pair and per attention head/expert.\n\nKey_Mechanism:\n- The core insight is that watermark perceptibility and robustness depend on the interaction between watermark spatial frequency (complexity) and the image’s scale-dependent structure; therefore, selecting and fusing the “right” scales minimizes distortion while maximizing recoverability. ASIM learns these scale correspondences from pooled residual statistics.\n- MoH captures complementary interactions among scales by sparsely routing only the most relevant attention heads, while MoE supplies content-specialized non-linear transformations with balanced load to avoid expert collapse. FAEM strengthens salient spatial and channel cues of the fused residuals to improve both invisibility and extractor robustness.\n- Operating in the shared discrete residual space of a multi-scale VQVAE aligns watermark tokens with the VAR pipeline, preserving sampling efficiency while providing stable, differentiable control over watermark embedding.\n\nMathematical_Formulation:\n- Multi-scale VQVAE residual reconstruction (shared codebook, bilinear upsampling ϕ):\n  F_k = ∑_{i=1}^{k} ϕ(R_i, (h, w)), with final decoded image Ĩ = Dec(F_K).\n  Here R_i ∈ R^{h_i×w_i×d}, F_k ∈ R^{h×w×d}.\n- VAR next-scale likelihood (unchanged generator, watermarking sits in residual space):\n  p(R_1, …, R_K) = ∏_{k=1}^{K} p(R_k | R_{<k}, Ψ(t)).\n- ASIM scale selection. Encode cover I → C={C_1..C_K}, watermark W → M={M_1..M_K}. For each (i,j), concatenate and pool A_{i,j}=[M_i; C_j] → a_{i,j} ∈ R^{d_p}. Compute weight:\n  w_{i,j} = Softmax_j( MLP(a_{i,j}) ), i,j ∈ {1..K}.\n  For each i, take top-k indices {a_i1,…,a_ik} by w_{i,j}.\n- CSFM, MoH attention on each selected pair A_{i,a_i s}. Let X = A_{i,a_i s} reshaped to n tokens with d channels. With h heads, router produces gating g ∈ R^{h} (sparse for routed heads; some heads always shared). Per head m:\n  Att_m(X) = softmax( (Q_m K_m^T)/√d_h ) V_m,  with Q_m=XW^Q_m, K_m=XW^K_m, V_m=XW^V_m.\n  MoH output: B_{i,a_i s} = ∑_{m=1}^{h} g_m Att_m(X) W^O_m.\n- CSFM, MoE FFN routing (top-r, typically r=1) over N experts with gating π ∈ R^{N}:\n  B^*_{i,a_i s} = ∑_{n∈Top-r} π_n (σ(B_{i,a_i s} W^{(1)}_n + b^{(1)}_n) W^{(2)}_n + b^{(2)}_n) + B_{i,a_i s}.\n- Weighted aggregation to fused residual for watermark scale i:\n  R_i = ∑_{s=1}^{k} w_{i,a_i s} · B^*_{i,a_i s}.\n- FAEM refinement. Spatial attention S and channel attention C (CBAM-style):\n  S = σ(Conv([AvgPool(R_i), MaxPool(R_i)])),  R'_i = S ⊙ R_i;\n  c = GAP(R'_i),  α = σ(MLP(c)),  R''_i = α ⊙ R'_i;\n  F = Aggregate({R''_i}_{i=1}^{K}) (e.g., sum/concat+1×1 conv), Ĩ = Dec(F).\n- Training objectives. U-Net extractor T(·): Ŵ = T(Ĩ). Reconstruction and balanced MoE losses with perceptual and adversarial terms:\n  L_rec = λ_im ||I−Ĩ||_2^2 + λ_wm ||W−Ŵ||_2^2,\n  L_total = λ_rec L_rec + λ_perc LPIPS(Ĩ,I) + λ_adv L_adv + λ_MoE L_balance,\n  where L_balance is a load-balancing term over expert routing probabilities to prevent hot experts.\n\nComputational_Properties:\n- Time Complexity:\n  - ASIM: weight computation over all (i,j) pairs after pooling is O(K^2 d_p) per image; top-k selection adds O(K·K log K) in worst case, but K is small (e.g., ≤14), so constant in practice.\n  - CSFM MoH: for each i and s in top-k, attention over n tokens with hr routed heads (hr ≤ h): O(K·k·hr·n^2·d_h) for training/inference; routing overhead O(K·k·h).\n  - CSFM MoE: top-r experts (r=1 or 2) per token with hidden width f: O(K·k·n·(d·f + f·d)) ≈ O(K·k·n·d·f).\n  - FAEM: lightweight conv and pooling per scale, O(K·n·d) to O(K·n·d^2) depending on implementation.\n  - Overall dominant term: O(K·k·hr·n^2·d_h + K·k·n·d·f). Compared to standard MHA on all scales without routing O(K·k·h·n^2·d_h), MoH routing reduces the constant by using fewer active heads; MoE uses top-r rather than dense FFN across N experts.\n- Space Complexity:\n  - Activations: O(K·k·n·d) for selected pairs; KV caches per active head O(K·k·hr·n·d_h).\n  - Parameters: MoH adds gating router (~O(h·d_g) small) on top of standard MHA weights; MoE adds N experts with 2-layer FFNs each ~O(N·(d·f + f·d)). FAEM adds small conv/MLP heads O(d^2).\n  - Total memory scales linearly in K, k, and n; MoE parameter memory scales with N but compute scales with r (top-r).\n- Parallelization:\n  - Inter-scale and inter-pair (i,s) computations are embarrassingly parallel on GPU; attention heads are parallelized as usual; MoE supports expert parallelism with all-to-all routing across devices.\n  - ASIM (pairwise scoring) runs as batched MLPs over pooled vectors; top-k selection uses parallel segmented selection kernels.\n- Hardware Compatibility:\n  - GPU-friendly due to tensorized attention and batched expert routing; MoE benefits from NVLink/InfiniBand for all-to-all in multi-GPU. Memory bandwidth dominated by attention KV reads and MoE FFN matmuls; FAEM uses lightweight convs compatible with tensor cores.\n  - CPU inference feasible for small K,k but attention n^2 term favors GPUs.\n- Training vs. Inference:\n  - Discriminator (for L_adv) and load-balancing loss used only during training; inference omits them. The extractor T is used for evaluation (and can be omitted at deployment if not needed).\n  - Routing decisions are identical in training and inference; no sampling loops beyond standard VAR decoding.\n- Parameter Count:\n  - MoH adds gating router (negligible) and reuses MHA weights; CSFM can share attention parameters across pairs to limit growth. MoE adds N experts: ~2N·d·f parameters; with top-1 routing, compute does not scale with N.\n  - ASIM MLP over pooled vectors adds O(d_p^2) parameters; FAEM adds O(d^2) small heads.\n- Numerical Stability:\n  - Softmax-based w_{i,j} and head/expert gates are bounded; convex aggregation of B^* with w_{i,a_i s} prevents exploding norms. Residual connections in MoE (B^* + B) stabilize gradients.\n  - Balanced load loss prevents expert collapse (hot-spotting) and mitigates gradient starvation. Working in quantized residual space with bilinear upsampling avoids pixel-space high-frequency amplification.\n- Scaling Behavior:\n  - With more scales K or larger k, ASIM’s selection remains cheap while CSFM cost grows linearly in K·k. Token length n grows with image resolution, making O(n^2) attention the main bottleneck; MoH routing and restricting hr mitigates this. MoE scales in parameter count with N but in compute with r (constant), enabling larger capacity without proportional compute growth.\n  - For higher resolutions (e.g., 1024^2), keeping hr and r small and limiting k (empirically k≈4) maintains practical latency while preserving watermark fidelity and invisibility.",
        "IMPLEMENTATION_GUIDANCE": "Integration_Strategy:\n- Where to integrate in a VAR pipeline (e.g., VAR, HART, LlamaGen-like next-scale models)\n  - Keep the pretrained multi-scale VQVAE (encoder/codebook/decoder) and the AR Transformer untouched initially.\n  - Insert Safe-VAR between the AR Transformer and the VQVAE decoder at each scale:\n    1) At sampling time, after the AR Transformer predicts residual tokens Rk (or continuous residuals for continuous-token models), decode tokens to residual feature maps at resolution hk × wk.\n    2) Run Adaptive Scale Interaction Module (ASIM) to select top-k cross-scale pairs between current watermark residual map Mi and available image residual maps {C1..Ck} (predicted so far).\n    3) Fuse with Cross-Scale Fusion Module (CSFM: MoH + MoE) to produce fused residual R̃k.\n    4) Replace Rk with R̃k before feeding to the cumulative reconstruction Fk = sum_{i≤k} ϕ(Ri, (h, w)); decode to image when k = K.\n  - For training with paired cover images:\n    - Encode both cover image I and watermark W via the shared VQVAE encoder to obtain C = {C1..CK}, M = {M1..MK}.\n    - Apply ASIM+CSFM+FAEM to produce fused {R1..RK} and decode to Î (watermarked image).\n    - Train the extractor T(·) and the ASIM/CSFM/FAEM jointly with the specified losses.\n\n- Exact code-level changes (PyTorch; analogous in TF/JAX)\n  - Expose residual maps in your VQVAE:\n    - Add VQVAE.encode_multi_scales(x) -> {R1..RK}, and VQVAE.decode_from_residuals({R1..RK}) -> x̂.\n    - Ensure codebook Z is shared for cover and watermark encodings.\n  - Implement ASIM (nn.Module)\n    - Inputs: Mi (Bh×Wh×D), {Cj}j=1..K.\n    - Build concatenation Ai,j = concat[Mi, Cj] along channel; apply global average pooling (GAP) + global max pooling (GMP), concat (2D), MLP (e.g., Linear(2D→D/2), GELU, Linear(D/2→1)), Softmax over j for each fixed i to get weights wi,j.\n    - Top-k selection: for each i, pick indices {a_i1..a_ik} of largest wi,j (torch.topk).\n  - Implement MoH (custom Multi-Head Attention with head routing)\n    - Start from torch.nn.MultiheadAttention or SDPA (PyTorch 2.1+): project Q,K,V per head; implement gating vector g ∈ R^h per sample via a small router (Linear(D→h), softmax or sigmoid); mask-in shared heads (first hs heads set gate=1) and apply gates on head outputs before the final output projection: out = sum_m g_m·Head_m(Q,K,V).\n    - Provide hooks to always-keep hs shared heads and route hr heads dynamically (learned).\n  - Implement MoE FFN (Switch-style)\n    - Replace standard FFN with a gated expert pool: experts = ModuleList([FFN_i: Linear(D→4D), GELU, Linear(4D→D)]).\n    - Router: r = Softmax(Linear(D→N)/τ), pick top-1 or top-2 experts per token; dispatch tokens; combine outputs with gate r; add residual.\n    - Add load-balancing loss (see Parameter_Settings) to your loss dict.\n  - Implement FAEM (CBAM-like)\n    - Spatial Attention: Conv2d(2→1, k=7, padding=3) over [avgpool(x), maxpool(x)] along channel; multiply with input.\n    - Channel Attention: MLP on GAP and GMP; two-layer MLP with reduction ratio r=16; sigmoid; multiply with input.\n    - Aggregate across scales: sum or learned weighted sum over {R1..RK} → F.\n  - Watermark Extractor T(·) (U-Net with attention)\n    - Standard U-Net backbone with 4 down/upsampling stages, base channels 64; add a self-attention block at the bottleneck (e.g., 1–2 heads, dim 256); output 3-channel watermark reconstruction.\n  - Training loop integration\n    - Add losses: image reconstruction, watermark reconstruction, perceptual (LPIPS), adversarial (PatchGAN), MoE load balance.\n    - For AR integration training: either\n      1) Teacher forcing with ground-truth residuals C for stability (recommended first), or\n      2) Scheduled sampling from AR predictions to match generation-time distribution.\n\n- Migration path\n  - From post-hoc or diffusion-native watermarking: keep your data, replace post-hoc embedder with Safe-VAR at the residual level; remove diffusion-specific injections.\n  - From discrete-token-only AR: use your tokenizer to convert Rk tokens → codebook embeddings → residual maps; inject via ASIM/CSFM; then re-quantize if needed.\n  - From continuous-token AR (Fluid): treat multi-resolution feature pyramids as residual maps; skip quantization and operate directly on continuous features.\n\n- Framework compatibility and optional dependencies\n  - PyTorch 2.1+ recommended; torch.compile for speed; use SDPA or FlashAttention-2 (optional) for MoH speedups.\n  - Libraries: kornia (data attacks), lpips (perceptual loss), piq (SSIM/PSNR), xFormers/FA2 (optional), timm (PatchGAN/disc utils).\n  - No custom CUDA kernels required; FlashAttention is optional but beneficial at 1024².\n\n- Training pipeline fit\n  - Use DistributedDataParallel (DDP) + mixed precision (AMP) + gradient checkpointing on ASIM/CSFM for 1024².\n  - Introduce discriminator after 10k steps as in the paper; alternate 1:1 G/D steps.\n\nParameter_Settings:\n- Architecture and fusion\n  - Number of scales K: 8–14 supported; empirically best with selecting k=4 scales per watermark scale (Top-4 from ASIM). If K=14, set k=4; avoid k>8 (degrades slightly).\n  - MoH heads h: 8 total; shared hs=3, routed hr=3 active heads (best in ablation). Keep 2 heads idle to simplify gating or use all 8.\n  - MoE experts N: 1–2 recommended; N=1 suffices and was best overall; N≥3 risks diminishing returns and load imbalance.\n  - FAEM channel reduction r: 16 (range 8–32); spatial attention kernel 7×7 (range 5–9).\n  - Router temperatures: τ_head ∈ [0.5, 1.5], default 1.0; τ_expert ∈ [0.3, 1.0], default 0.7.\n\n- Loss weights and objectives\n  - Lrec = λim·||I−Î||² + λwm·||W−Ŵ||² with λim=1.0, λwm=1.0 (adjust λwm to 1.5–2.0 if extractor underfits).\n  - Lperc (LPIPS): λperc=0.1 (0.05–0.2).\n  - Ladv (PatchGAN): λadv=0.1 (0.05–0.2); PatchGAN: 70×70 patch, n_layers=3–4, base_channels=64.\n  - LMoE (load balance): λMoE=0.02 (0.01–0.05).\n  - Total: Ltotal = λrec·Lrec + λperc·Lperc + λadv·Ladv + λMoE·LMoE with λrec=1.0.\n  - Optimizer: AdamW with betas (0.9, 0.999), weight decay 0.01; LR 1e-4 (stage-1), 5e-6 (stage-2/hi-res).\n\n- Training schedule (two-stage from the paper; batch sizes assume mixed precision)\n  - Stage-1 (256²): 40 epochs, batch 48–64, LR 1e-4, introduce discriminator at 10k steps; freeze image encoder and codebook; train ASIM, CSFM, FAEM, extractor, decoder, and (optionally) AR adapter.\n  - Stage-2 (512² then 1024²): 1 epoch each; batch 4–8 (512²), 1–4 (1024²); LR 5e-6; freeze watermark encoder; keep disc on after 10k steps.\n  - Learning rate schedule: cosine decay or step decay ×0.1 at 60% of stage progress.\n\n- Initialization\n  - New linears/conv: Kaiming uniform for ReLU/GELU; Xavier uniform for attention projections; set router last-layer bias to 0, weights std=1e-3 to start near uniform routing.\n  - ASIM MLP: last-layer weight/bias initialized near zero to yield near-uniform wi,j at start; Softmax temperature 1.0.\n  - Expert FFNs: identical init; apply dropout 0.0–0.1 inside FFNs if overfitting.\n\n- Critical vs robust parameters\n  - Critical: k (top-k scales), hs/hr ratio, λwm (controls extractor’s learning), τ_expert (affects MoE stability), λMoE (prevents expert collapse).\n  - Robust: λperc, λadv within listed ranges; FAEM r; spatial kernel; LPIPS network choice (Alex/VGG).\n\n- Hardware-dependent settings\n  - A100 80GB: 256² batch 60 on 8 GPUs feasible; 1024² batch 2–4/GPU with gradient checkpointing and SDPA/FlashAttention.\n  - 24GB GPUs (3090/4090): 256² batch 12–16/GPU; 512² batch 2–4; 1024² requires activation checkpointing, sequential scale-by-scale decoding, and offloading (DeepSpeed ZeRO-3 or PyTorch Fully Sharded).\n  - Enable torch.set_float32_matmul_precision(\"high\") for SDPA; use channels_last memory format for conv-heavy modules (VQVAE/UNet).\n\nApplication_Conditions:\n- Beneficial scenarios\n  - AR text-to-image systems using multi-scale tokenization (VAR/HART/LlamaGen-like) with next-scale prediction.\n  - Need for invisible, robust graphical watermarking (logos, QR codes) with strong fidelity under perturbations (crop 30%, rotate ±40°, blur 9, brightness ±0.4, noise σ=0.3, erase 30%, JPEG q=50).\n  - Latency-sensitive generation where diffusion-native watermarking is too slow.\n\n- Hardware requirements\n  - Minimum: single 24GB GPU for 256² training and 512² inference; multi-GPU (≥4×24GB) recommended for 512² training; 1024² fine-tuning ideally ≥4×40–80GB.\n  - For production inference, single 16–24GB GPU is sufficient if AR model already fits; Safe-VAR adds ~5–15% overhead.\n\n- Scale considerations\n  - K≥8 multi-scales maximize adaptive selection gains; if tokenizer exposes only 3–4 scales, benefits persist but are smaller; use k=min(2, K).\n  - More pronounced benefits at 512²–1024² where mid/high-frequency detail can carry robust watermarks.\n\n- Task compatibility\n  - Works best with graphical watermarks (logos, QR codes) and natural image generation.\n  - Neutral to mildly negative for extremely low-detail outputs or flat-color illustrations where any perturbation is visible; mitigate by reducing λwm and favoring low-frequency scales via temperature schedule on ASIM.\n\n- Alternatives and when to choose differently\n  - Choose Safe-VAR over post-hoc methods when original non-watermarked image must not exist or leakage is a concern.\n  - Prefer diffusion-native watermarking if your stack is exclusively diffusion and AR conversion is infeasible.\n  - For binary-string watermarking needs, StableSignature-like approaches may be simpler but are less robust to perturbations.\n\n- Resource constraints\n  - If compute-constrained, train only extractor T(·) and ASIM, freeze CSFM (MoH heads shared only, hs=h, hr=0) to reduce complexity; you lose ~0.3–0.6 PSNR on watermark recovery.\n\nExpected_Outcomes:\n- Performance improvements vs baselines (indicative from the paper; 512² unless noted)\n  - Image quality vs Safe-SD: +15–16% PSNR and SSIM improvements; PSNR ≈ 29.2–32.5 (512–1024²) vs 25.4–24.6; LPIPS lower by 20–40% (0.124–0.105 vs 0.207–0.255).\n  - Watermark fidelity: PSNR of recovered watermark 33.9–39.7; SSIM 0.94–0.98; LPIPS ≈ 0.086–0.058.\n  - Robustness under perturbations (LAION-Aesthetics):\n    - PSNR of extracted watermark under combined attacks ≈ 33.6 (vs 27.9 for Safe-SD); LPIPS ≈ 0.084 (vs 0.136); FID reduced from ~5.82 to ~3.09.\n\n- Overheads and latency\n  - Training: 1–2 days on 8×A100 80GB for stage-1 (256², 40 epochs, 200k images), plus ~6–12 hours for 512² and ~6–12 hours for 1024² fine-tune.\n  - Inference: +5–15% latency over vanilla AR at 512² due to ASIM/CSFM/FAEM; negligible memory increase (<1–2 GB).\n\n- Trade-offs\n  - Improvements: higher image PSNR/SSIM/LPIPS, significantly better watermark recovery, stronger robustness to crop/rotate/blur/noise/JPEG.\n  - Possible degradations: slight color shift or sharpening on highly textured regions if λadv too high or ASIM over-selects high-frequency scales; minor AR sampling slowdown.\n\n- Failure modes and mitigations\n  - Expert collapse (one expert dominates): monitor LMoE load stats (load_i, P_i); if Gini coefficient > 0.6, increase λMoE to 0.03–0.05, raise τ_expert to 0.9, or reduce N to 1.\n  - Head routing degeneracy (all routed heads off): check mean gating g; if avg g_routed < 0.05, lower τ_head to 0.7 and add entropy regularization 1e-4·H(g).\n  - Visible artifacts on flat regions: reduce λwm to 0.5–0.8; bias ASIM to lower-frequency scales by subtracting a penalty α·f_high from wi,j (α=0.05–0.1) based on a simple high-frequency energy estimator.\n  - Poor QR code recovery: increase k to 6, increase λwm to 1.5–2.0, and favor mid/high scales (raise τ_head to 1.2) to preserve sharp edges.\n  - Adversarial loss destabilization: if GAN losses oscillate, reduce λadv to 0.05, use R1 regularization (γ=10) on the discriminator.\n\n- Debugging indicators and validation\n  - Sanity checks:\n    - wi,j distribution: for each i, entropy H(wi,·) ∈ [1.2, 2.0] (K=14). Too low → overconfidence; too high → non-selective.\n    - Per-scale PSNR drop (cover→watermarked) should be small and concentrated on mid/high scales; if low-scale PSNR drops > 1.0 dB, ASIM is overusing low-frequency channels.\n    - Extractor: on clean images, watermark PSNR > 33 dB at 512² after stage-1; under combined attacks > 31 dB at convergence.\n  - Validation protocol:\n    - Compute PSNR/SSIM/LPIPS on 5k random pairs (I, Î) and (W, Ŵ).\n    - Robustness: apply 8 perturbations (crop 30%, rotate ±40°, blur k=9, brightness ±0.4, Gaussian σ=0.3, erase 30%, JPEG q=50, combined 50% prob) and re-evaluate extractor metrics.\n    - Visualize pixel-wise differences (×10 scaling) to confirm imperceptibility.\n\n- Hardware-specific outcomes\n  - A100/H100 with SDPA/FlashAttention: MoH attention cost near parity with vanilla MHA; overall throughput within 0.85–0.95× of baseline AR.\n  - Consumer GPUs (3090/4090): MoH introduces 5–12% slower attention; mitigate with torch.compile and head dimension multiples of 64; use channels_last.\n\nQuality Requirements:\n- Troubleshooting quick checklist\n  - Watermarks not recoverable: raise λwm, increase k to 6, use N=1, lower τ_expert to 0.5, ensure extractor capacity (base channels ≥64), and train longer at 256².\n  - Image quality drop: lower λwm, lower λadv, enforce low-frequency bias in ASIM, reduce hr (routed heads) or switch off MoE (N=1).\n  - Memory OOM at 1024²: enable gradient checkpointing in ASIM/CSFM/FAEM, reduce batch to 1–2, accumulate grads to 8–16, and use bf16 AMP.\n\n- Validation to ensure correct implementation\n  - Ablate modules and reproduce trends: removing ASIM or CSFM should drop image PSNR by ~3–4% and SSIM by ~2–3%; removing FAEM should reduce PSNR/SSIM by ~1–2%.\n  - Scale selection study: K=4 (selected) vs fixed {0,3,6,9} should show ≥1.0 dB PSNR gap in watermark fidelity; K=14 with top-4 similar to K=8–10.\n  - Cross-dataset generalization: comparable metrics on LAION-Aesthetics, LSUN-Church, ImageNet; QR codes recover with higher fine-grain detail than Safe-SD baselines.\n\n- Concrete metric targets (reasonable expectations)\n  - 512²: Image pair PSNR 28.5–29.5, SSIM 0.85–0.87, LPIPS 0.12–0.16; Watermark pair PSNR 33–35, SSIM 0.94–0.95.\n  - 1024²: Image pair PSNR ~32.4–32.6, SSIM ~0.91–0.92, LPIPS ~0.104–0.105; Watermark pair PSNR ~39.6–39.7, SSIM ~0.98, LPIPS ~0.058–0.060."
    }
]