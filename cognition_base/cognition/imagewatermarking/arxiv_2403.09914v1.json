[
    {
        "DESIGN_INSIGHT": "### DESIGN_INSIGHT_HIGH: [Causal Watermark-Conditioned Latent Diffusion – Joint LDM+BCE Training to Persist Orthogonal Bit-Secrets]\nProMark replaces correlation-based attribution pipelines (CLIP/DINO/ALADIN-style embedding retrieval) with a proactive mechanism that modifies latent diffusion model (LDM) training to causally persist concept-linked watermarks in generations. Training images are encrypted by adding imperceptible, concept-specific spatial watermarks, and the LDM is jointly optimized to denoise while preserving the corresponding bit-secret detectable from generated images.\n\nThe core mechanism couples standard LDM supervision with a binary cross-entropy (BCE) guidance that enforces the presence of a concept’s bit-secret in the synthesized image. Given the LDM loss\n\\[\nL_{\\mathrm{LDM}}=\\mathbb{E}_{E_L(X),\\epsilon\\sim\\mathcal{N}(0,1),t}\\left\\|\\epsilon-\\epsilon_{\\theta}(z_t,t)\\right\\|_2^2,\n\\]\ntraining images are encrypted as \\(X_W=T(X;W)=X+m\\cdot R(W,h,w)\\) (with strength \\(m\\)), and latent codes are formed as \\(z=E_L(X_W)\\). ProMark computes fixed spatial watermarks per concept via spatial noise conversion from RoSteALS bit-secrets: for secret \\(s_j\\in\\{0,1\\}^b\\),\n\\[\nW_j=\\frac{1}{100}\\sum_{i=1}^{100}\\left(X_i - E_S(X_i,s_j)\\right).\n\\]\nThe denoised latent \\(\\hat{z}\\) is decoded and a pretrained robust secret decoder \\(D_S\\) predicts \\(\\hat{s}\\):\n\\[\n\\hat{s}=D_S\\!\\left(D_L(\\hat{z})\\right),\\quad\nL_{\\mathrm{BCE}}(s_j,\\hat{s})=-\\frac{1}{b}\\sum_{i=1}^b\\big[p_{ji}\\log\\hat{p}_i+(1-p_{ji})\\log(1-\\hat{p}_i)\\big],\n\\]\nand the joint objective is\n\\[\nL_{\\mathrm{attr}}=L_{\\mathrm{LDM}}+\\alpha\\,L_{\\mathrm{BCE}}.\n\\]\n\nUnlike prior passive methods that infer attribution from visual similarity, ProMark directly tests for causation by checking if the concept’s watermark is present in the generated image. Attribution reduces to bitwise matching with Hamming-style scoring:\n\\[\nf(\\hat{s},s_j)=\\sum_{k=1}^{b}[\\hat{p}_k=p_{jk}],\\qquad j^\\star=\\arg\\max_j f(\\hat{s},s_j).\n\\]\nThis design causally ties training concepts to synthetic outputs (supporting up to 216 orthogonal concepts), works for both unconditional and conditional LDMs, and can be applied via brief finetuning of pretrained models—fundamentally shifting attribution from similarity search to causal signal detection in the generative pipeline.\n\n### DESIGN_INSIGHT_MEDIUM: [Spatially Partitioned Multi-Watermark Composition – Two-Concept Attribution via Split-Domain Embedding]\nProMark extends single-concept attribution by modifying the image encryption transform to embed multiple orthogonal watermarks in disjoint spatial regions of a single image, enabling simultaneous attribution to multiple training concepts. This replaces the typical “single label per image” assumption with a compositional watermarking scheme that scales additively in the number of concepts represented per image.\n\nThe key mechanism splits the input image into two halves and injects distinct watermarks \\(W_i\\) and \\(W_j\\) into each half:\n\\[\nT(X;W_i,W_j)=\\Big( X(:,0:\\tfrac{w}{2},:)+R(W_i,h,\\tfrac{w}{2}),\\; X(:,\\tfrac{w}{2}:w,:)+R(W_j,h,\\tfrac{w}{2}) \\Big),\n\\]\nwith symmetrical variants (left/right or top/bottom) achieving similar performance due to spatial robustness. The loss becomes\n\\[\nL_{\\mathrm{attr}}=L_{\\mathrm{LDM}}+\\alpha\\big(L_{\\mathrm{BCE}}(s_i,\\hat{s}_1)+L_{\\mathrm{BCE}}(s_j,\\hat{s}_2)\\big),\n\\]\nwhere \\(\\hat{s}_1\\) and \\(\\hat{s}_2\\) are secrets decoded from the corresponding halves of the generated image. This design yields multi-concept causal attribution without exploding the label space (e.g., scaling as \\(7+8\\) vs. \\(7\\times 8\\) for media/content), differs fundamentally from passive multi-label embedding matching, and empirically demonstrates consistent accuracy across spatial placements and robustness to common degradations.\n\n### DESIGN_INSIGHT_MEDIUM: [Bit-Space Attribution with Hamming Scoring – Constant-Time Decoding and O(Nb) Matching vs Embedding Search]\nProMark replaces high-dimensional embedding retrieval with a compact bit-space attribution pipeline: a single forward pass through a robust secret decoder \\(D_S\\) yields a \\(b\\)-bit prediction \\(\\hat{s}\\), followed by an \\(O(Nb)\\) per-image match over \\(N\\) concept secrets using a Hamming-style score \\(f(\\hat{s},s_j)\\). This design shifts attribution complexity from large-scale descriptor indexing/search to lightweight bitwise comparison.\n\nMechanistically, watermark detection is content-agnostic due to the fixed spatial watermarks \\(W_j\\) derived by averaging RoSteALS residuals (which removes image content), making the watermark orthogonal and independent of the visual appearance. The attribution step is\n\\[\nf(\\hat{s},s_j)=\\sum_{k=1}^{b}[\\hat{p}_k=p_{jk}],\\qquad j^\\star=\\arg\\max_j f(\\hat{s},s_j),\n\\]\nwith decoding latency measured at \\(\\sim 5.6\\) ms on an A100 GPU and no dataset-scale index traversal. In contrast, embedding-based methods require feature extraction plus nearest-neighbor search over large galleries (e.g., tens of milliseconds for descriptor inference plus tens to hundreds of milliseconds for search on 20K items). By collapsing attribution to bit-decoding and matching, ProMark improves efficiency and decisiveness (causation rather than correlation), while maintaining robustness under image degradations and preserving generation fidelity at moderate watermark strengths \\(m\\).",
        "EXPERIMENTAL_TRIGGER_PATTERNS": "Task_Performance_Signatures:\n- Causal concept attribution on vision benchmarks (primary signal of effectiveness):\n  - Unconditional diffusion, single-concept attribution (Stock, LSUN, Wiki-A, Wiki-S, ImageNet):\n    - At watermark strength m=1.0: expect near-perfect attribution on Stock/LSUN/Wiki-A/Wiki-S (≈100%) and high accuracy on ImageNet (≈91.07%), outperforming correlation baselines by large margins.\n    - At watermark strength m=0.3: expect strong accuracy while preserving image quality, e.g., Stock ≈100%, LSUN ≈95.12%, Wiki-A ≈97.45%, Wiki-S ≈98.12%, ImageNet ≈83.06; still above ALADIN/CLIP/F-CLIP/SSCD/EKILA baselines (e.g., F-CLIP ≈62.83% on ImageNet).\n  - Conditional diffusion (ImageNet-1K):\n    - Held-out evaluation: m=1.0 ≈95.60% vs. m=0.3 ≈91.24%; baselines: F-CLIP ≈62.83%, CLIP ≈60.12%.\n    - Newly sampled images: m=1.0 ≈90.13% vs. m=0.3 ≈87.30%; baselines: F-CLIP ≈50.19%, CLIP ≈41.01%.\n  - Multi-concept attribution (BAM, media+content):\n    - Dual-watermark configuration (half-splits): m=1.0 combined accuracy ≈90.12% (Media ≈95.61%, Content ≈93.31); m=0.3 combined ≈84.66 (Media ≈91.33, Content ≈89.21).\n    - Single-concept (7×8 pairs treated as single classes): m=0.3 combined ≈97.73% (not scalable but indicative of upper bound).\n  - Scale with number of concepts (Adobe Stock templates):\n    - Accuracy declines smoothly with concept count: ≈100% at 10 classes (chance 10%) to ≈82% at 216 classes (chance ≈0.15%), tracking bit-secret recovery accuracy; indicates method remains effective under exponential concept growth.\n- Efficiency and quality trade-offs:\n  - Attribution latency: watermark decoding ≈5.6 ms/sample on A100; CLIP/ALADIN embedding inference ≈28.32 ms plus dataset search ≈87.91 ms for 20K images; expect ≈20× pipeline speedup vs. correlation-based search for attribution.\n  - Image quality impact: FID increases modestly (ImageNet conditional baseline 13.28 vs. ProMark 17.63), PSNR decreases as m increases; select m≈0.3 for balance (accuracy high; PSNR substantially higher than m≥0.5).\n  - Robustness under degradations: across 14 attacks, training images attribution ≈90.21±7.63%, generated images ≈89.51±8.18% (vs. ≈95.12% no attack), indicating resilience to common distortions.\n- Training/inference conditions that amplify improvements:\n  - BCE supervision is critical: removing BCE drops attribution to ≈2% (vs. ≈100% with BCE on Stock at m=1.0), expect visibly unstable watermark recovery without BCE.\n  - Pretrained secret decoder DS preferred: replacing DS with trained generic decoder reduces attribution from ≈100% to ≈80.56%; expect higher accuracy and stability with DS.\n  - Watermark placement invariance: left/right/top/bottom split configurations yield combined ≈90.0–90.73%; expect consistent performance regardless of spatial split.\n  - Data sufficiency: with fixed 500 concepts, reducing images per concept from 700 to 10 yields only ≈2.5% accuracy drop; expect effectiveness even with limited per-concept data.\n- Leakage and false attribution safeguards:\n  - No watermark retention when none is present: detection ≈50.56% (chance) after heavy noising or random latents; attribution on held-out images without or with incorrect watermarks still encrypts the correct concept watermark at inference (≈94.32% and ≈94.01%, respectively).\n- Cross-domain/NLP benchmark expectations (technique is orthogonal to text tasks; expect neutral impact):\n  - Language modeling: lambada_openai, wikitext, ptb — expect similar accuracy to baseline (within ±1%) and unchanged perplexity; no speed or memory changes (method targets vision diffusion watermarking).\n  - Reading comprehension: squad_completion, squad_v2, narrativeqa — expect stable performance (within ±1% EM/F1); technique does not alter textual reasoning.\n  - Commonsense reasoning: hellaswag, piqa, social_iqa, commonsenseqa — expect unchanged scores (±1%); no sequence-length dependence introduced.\n  - Factual QA: arc_easy, arc_challenge, boolq, openbookqa — expect neutral impact (±1%).\n  - Context resolution: winogrande, winograd — expect neutral (±1%).\n  - Other tasks: swde (structured extraction), fda (data augmentation) — expect neutral accuracy and throughput; potential minor attribution-related logging overhead only if image data is involved.\n\nArchitectural_Symptoms:\n- Training characteristics:\n  - With BCE supervision, expect smoother convergence of bit-sequence BCE (monotonic decrease), lower variance in secret prediction accuracy across batches, and rapid early-phase gains in attribution accuracy.\n  - Without BCE, expect flat or noisy bit-BCE, near-random secret recovery (≈2% attribution on Stock), and failure to associate concepts to watermarks.\n  - Stable performance when increasing number of concepts (up to 216), with a predictable accuracy decline attributable to watermark confusion rather than training instability.\n  - Robustness to watermark placement: identical loss behavior across split strategies (left/right/top/bottom), confirming spatial invariance.\n- Runtime/memory behaviors:\n  - Training overhead is negligible compared to baseline LDM; adding fixed spatial watermark and BCE head does not materially increase step-time or GPU memory beyond the autoencoder/UNet norm.\n  - Inference attribution path exhibits constant memory and latency per image (secret decoder DS forward ≈5.6 ms), independent of dataset size (no retrieval/search); contrast with correlation baselines that grow with gallery size (embedding search adds ≈87.91 ms for 20K images).\n  - Throughput patterns: higher end-to-end attribution throughput vs. embedding search pipelines; expect higher effective GPU utilization during attribution due to absence of large-scale nearest-neighbor scans.\n- Profiling signatures:\n  - Loss curves: LLDM behaves similarly to baseline; added LBCE reduces total Lattr steadily; no NaN occurrences or divergence introduced by watermark training at m≤0.5.\n  - Quality metrics vs. strength: PSNR decreases as m increases (e.g., multi-concept PSNR slightly higher than single at equal m), indicating controllable quality-accuracy trade-off; choose m≈0.3 for balanced profiles.\n  - Attribution robustness profile: accuracy vs. degradation remains within ≈5–6% of clean for moderate noise/blur/fog; confirms decoder’s robustness consistent with RoSteALS properties.\n  - Decoder dependency: replacing pretrained DS with a generic decoder increases compute in training and reduces final attribution (≈80.56%); profiling shows higher gradient noise and slower convergence in watermark extraction head.\n  - End-to-end latency comparison: ProMark attribution per image ≈5.6 ms vs. correlation methods ≥116 ms including search on 20K (≈20× faster), with dataset-size independence as a key symptom of proactive causal attribution.\n  - Scaling symptoms: as concepts grow from 10 to 216, attribution declines smoothly to ≈82%; bit-secret Hamming similarity metric drops correspondingly, confirming confusion rather than training failure; no OOM or instability observed at larger concept counts or dual-watermark settings.",
        "BACKGROUND": "Title: ProMark: Proactive Diffusion Watermarking for Causal Attribution\n\nHistorical Technical Context:\nDiffusion models have become the dominant architecture for high-fidelity image synthesis, superseding earlier GAN- and autoregressive-based approaches. Denoising Diffusion Probabilistic Models (DDPM), deterministic variants such as DDIM, and Latent Diffusion Models (LDM) enable training in latent space using a pretrained autoencoder to reduce computational cost while preserving quality. In parallel, representation learning with CNN/Transformer encoders (e.g., CLIP, DINO) established embedding spaces for visual similarity and retrieval. These embedding models underpin passive attribution methods that correlate generated images with training data by nearest-neighbor search or learned similarity metrics.\n\nAlongside provenance and IP protection efforts, watermarking and fingerprinting have been studied for model and content authentication. Prior watermarking for generative models typically embeds a single global signal for detection or ownership verification (e.g., in LDM decoders or via token steering in LLMs). Proactive perturbation schemes have recently emerged, adding content-independent templates to inputs to aid detection, localization, or robustness. However, causal attribution of a synthetic image to specific training concepts—objects, styles, artists, motifs—has predominantly relied on correlation using similarity in an embedding space (e.g., EKILA, ALADIN, CLIP fine-tuning), which does not establish that a particular training concept caused the generation.\n\nProMark integrates proactive watermarking with diffusion training to causally attribute generated content to training concepts. It assigns distinct, orthogonal watermarks to concept partitions in the training set, trains the diffusion model to retain these signals, and decodes the watermark from generated images to determine provenance. This reframes attribution from correlation to causal inference by design: if the concept-specific watermark is present, the concept influenced generation.\n\nTechnical Limitations:\n- Lack of causality in passive methods: Embedding-based attribution retrieves nearest neighbors in a learned space, which can match visually similar but unseen or non-contributory images. This yields false attributions and cannot provide a causative link between training concepts and generation.\n- Scalability and search cost: Passive methods require embedding computation and nearest-neighbor search over M training images, with query cost O(Md) (exact) or O(M log M) with ANN structures; memory footprint is O(Md). This becomes expensive for large-scale datasets (e.g., ImageNet-scale).\n- Multi-concept attribution gap: Prior approaches typically attribute to a single concept; they lack mechanisms to disentangle and attribute multiple orthogonal factors (e.g., media and content) expressed in a single image.\n- Limited watermark capacity in prior GenAI watermarking: Most watermarking for diffusion models embeds one universal signal, insufficient for attributing many concepts or multiple concepts per image, and does not tie watermarks to training concepts.\n- Training dynamics for watermark retention: Naively training on watermarked images with only the diffusion loss does not reliably bind concept semantics to the watermark; without auxiliary supervision, attribution accuracy collapses (reported drop to ~2% without BCE).\n- Quality–attribution trade-off: Stronger watermark strength improves detection but degrades image quality (lower PSNR, slightly higher FID). There is a need to balance imperceptibility with reliable recovery.\n\nPaper Concepts:\n- Latent Diffusion Model (LDM): A diffusion model trained in latent space. An image X is encoded z = E_L(X) and denoised via ε_θ(·) on a noise schedule t = 1…T. The standard LDM objective is\n  L_LDM = E_{E_L(X), ε∼N(0,1), t} [ || ε − ε_θ(z_t, t) ||_2^2 ].\n  Intuitively, the model learns to invert a forward noising process in latent space to generate images.\n\n- Proactive Spatial Watermark: A content-independent watermark W ∈ R^{h×w} derived from a concept-specific bit-secret s ∈ {0,1}^b (with b = 160). Training images X are encrypted by\n  X^W = T(X; W) = X + m · R(W, h, w),\n  where m ∈ [0,1] controls watermark strength and R resizes W. Watermarks are constructed using RoSteALS by averaging residuals over K images encrypted with secret s:\n  W = (1/K) ∑_{i=1}^K (X_i − E_S(X_i, s)).\n  This yields spatial, content-agnostic, approximately orthogonal watermarks across concepts.\n\n- Attribution Loss (Joint Diffusion + BCE): To couple concepts and watermarks during training, an auxiliary binary cross-entropy on the decoded secret is added:\n  L_BCE(s, ŝ) = −(1/b) ∑_{i=1}^b [ p_i log(ŝ_i) + (1−p_i) log(1−ŝ_i) ],\n  with ŝ = D_S(D_L(ẑ)) and ẑ the denoised latent. The full objective is\n  L_attr = L_LDM + α L_BCE,\n  with α = 2 in experiments. This explicitly teaches the model to retain and reveal the concept-specific secret in generated images.\n\n- Causal Attribution Function: At inference, the predicted bit-sequence ŝ is matched to candidate concept secrets {s_j} via a Hamming-similarity score\n  f(ŝ, s_j) = ∑_{k=1}^b [ŝ_k = p_{jk}],\n  and attribution is j* = argmax_j f(ŝ, s_j). Because secrets are orthogonal and tied to training partitions, detecting a concept’s secret constitutes a causal claim that the concept influenced generation.\n\n- Multi-concept Watermarking: To attribute two orthogonal concepts simultaneously (e.g., media and content), two watermarks W^i, W^j are placed in disjoint spatial regions (e.g., left/right halves):\n  T(X; W^i, W^j) = { X_left + R(W^i, h, w/2), X_right + R(W^j, h, w/2) }.\n  Two secrets (ŝ_1, ŝ_2) are decoded, with loss\n  L_attr = L_LDM + α ( L_BCE(s_i, ŝ_1) + L_BCE(s_j, ŝ_2) ).\n  This composes independent signals for multi-concept attribution while preserving imperceptibility.\n\n- Orthogonal Watermark Set and Capacity: Watermarks corresponding to different secrets have near-zero inter-watermark cosine similarity, ensuring distinguishability. With b = 160 bits and robust decoding, the system supports up to 216 distinct concepts in practice while maintaining high attribution accuracy; matching cost per image is O(bN) over N concepts.\n\nExperimental Context:\nThe paper evaluates causal attribution across unconditional and conditional LDMs with emphasis on generative settings. Primary metric is attribution accuracy (%), measured on (i) held-out (real) images passed through the diffusion inversion pipeline and (ii) newly sampled synthetic images. Image quality is monitored via PSNR (vs. watermarked held-out images) and FID for generated images; robustness is assessed against 14 degradations. Efficiency is reported as decoder inference latency.\n\nBenchmarks span diverse attribution notions: Adobe Stock (100 template clusters), LSUN (10 scenes), WikiArt-Artists (23), WikiArt-Styles (28), ImageNet-1K (classes as concepts/conditions), and BAM (multi-label: 7 media × 9 content, evaluated as dual-concept attribution). ProMark is compared to passive correlation baselines (ALADIN, CLIP, fine-tuned CLIP, SSCD, EKILA). Key findings: near-perfect accuracy at m = 1.0 on most unconditional datasets and strong performance even at m = 0.3 (e.g., Stock 100%, LSUN 95.12%, Wiki-A 97.45%, Wiki-S 98.12%, ImageNet 83.06). Conditional ImageNet LDM achieves 95.60% on held-out and 90.13% on newly sampled images (m = 1.0). Multi-concept BAM attribution reaches 90.12% combined at m = 1.0 (vs. 46.23% for the best passive baseline). Scaling studies show robust attribution up to 216 concepts (82% accuracy; chance ≈ 0.15%), mild sensitivity to images per concept (≤2.5% drop from 700 to 10), and the necessity of the BCE term (dropping to ~2% without it). Image quality trade-offs are quantified (FID: 13.28 pretrained vs. 17.63 ProMark; PSNR decreases as m increases). The watermark decoder runs in 5.6 ms on an A100 GPU, while passive methods incur feature extraction (~28.3 ms) plus dataset search (~87.9 ms for 20K images), highlighting both the causal nature and efficiency of ProMark’s attribution.",
        "ALGORITHMIC_INNOVATION": "Core_Algorithm:\n- Replace standard latent diffusion model (LDM) training on raw images with training on proactively watermarked images and augment the loss with a bit-sequence recovery objective. Specifically, compute a per-concept spatial watermark W_j from a robust steganographic bit-secret s_j and encrypt each training image X by X_W = X + m·R(W_j, h, w).\n- Train the LDM denoiser ϵ_θ on latents of encrypted images while backpropagating through the fixed secret decoder D_S to minimize a binary cross-entropy between the decoded bit-sequence \\(\\hat{s}\\) and the concept’s secret \\(s_j\\).\n- At inference, decode \\(\\hat{s} = D_S(X_S)\\) from synthesized image \\(X_S\\) and attribute by matching bits to the catalog of concept secrets. For multi-concept attribution, embed two orthogonal watermarks in disjoint spatial halves and apply BCE independently per half.\n- Scope of change: data preprocessing (encryption per training image), loss augmentation (add BCE over decoded bits), and inference-time attribution (bit matching). The LDM architecture and sampling remain unchanged.\n\nKey_Mechanism:\n- The key insight is to tie each training concept to an orthogonal bit-secret and force the generative model to retain the corresponding spatial watermark through a differentiable recovery objective. Minimizing BCE makes the denoiser produce images that preserve the watermark features, establishing a causal link between generated images and the underlying training concepts.\n- Orthogonality of secrets reduces inter-concept interference, enabling robust attribution even when multiple concepts co-occur. Using a robust watermark encoder/decoder (RoSteALS) ensures watermark recoverability under common image transformations and diffusion sampling noise, yielding higher attribution accuracy than correlation-based matching.\n\nMathematical_Formulation:\n- Image encryption with per-concept watermark:\n  \\[\n  X_W = T(X; W_j) = X + m \\cdot R(W_j, h, w),\n  \\]\n  where \\(X\\in\\mathbb{R}^{h\\times w\\times 3}\\), \\(W_j\\in\\mathbb{R}^{h\\times w\\times 3}\\) is a spatial watermark for concept \\(j\\), \\(R(\\cdot)\\) resizes to \\((h,w)\\), and \\(m\\in[0,1]\\) controls strength.\n\n- Spatial watermark construction from bit-secret \\(s_j\\in\\{0,1\\}^b\\) via residual averaging using a robust secret encoder \\(E_S\\):\n  \\[\n  W_j = \\frac{1}{K} \\sum_{i=1}^{K} \\big( X_i - E_S(X_i, s_j) \\big), \\quad K=100.\n  \\]\n\n- LDM training on encrypted latents with auxiliary bit recovery:\n  \\[\n  z = E_L(X_W), \\quad \\hat{z} = \\text{denoise}(z_t;\\,\\epsilon_\\theta), \\quad \\hat{s} = D_S\\!\\big( D_L(\\hat{z}) \\big),\n  \\]\n  and losses\n  \\[\n  L_{\\text{LDM}} = \\mathbb{E}_{E_L(X),\\,\\epsilon\\sim\\mathcal{N}(0,1),\\,t}\\, \\big\\| \\epsilon - \\epsilon_\\theta(z_t, t) \\big\\|_2^2,\\quad\n  L_{\\text{BCE}}(s_j,\\hat{s}) = -\\frac{1}{b}\\sum_{i=1}^{b}\\big[p_{ji}\\log\\hat{p}_i + (1-p_{ji})\\log(1-\\hat{p}_i)\\big].\n  \\]\n  Final objective:\n  \\[\n  L_{\\text{attr}} = L_{\\text{LDM}} + \\alpha\\,L_{\\text{BCE}}, \\quad \\alpha>0.\n  \\]\n\n- Attribution by bit matching:\n  \\[\n  f(\\hat{s}, s_j) = \\sum_{k=1}^{b} [\\hat{p}_k = p_{jk}], \\quad\n  j^* = \\arg\\max_{j\\in\\{1,\\dots,N\\}} f(\\hat{s}, s_j),\n  \\]\n  where \\([\\cdot]\\) is the indicator.\n\n- Multi-concept encryption (two watermarks; left-right split shown):\n  \\[\n  T(X; W_i, W_j) = \\big( X(:,0:\\tfrac{w}{2},:)+R(W_i,h,\\tfrac{w}{2}),\\; X(:,\\tfrac{w}{2}:w,:)+R(W_j,h,\\tfrac{w}{2}) \\big),\n  \\]\n  with loss\n  \\[\n  L_{\\text{attr}} = L_{\\text{LDM}} + \\alpha\\Big( L_{\\text{BCE}}(s_i,\\hat{s}^{\\text{left}}) + L_{\\text{BCE}}(s_j,\\hat{s}^{\\text{right}}) \\Big).\n  \\]\n\n- Complexity (per image, single-concept): training time \\(O(T\\cdot F_{\\epsilon} + F_{D_S} + F_{E_L}+F_{D_L})\\); inference time \\(O(T\\cdot F_{\\epsilon} + F_{D_S})\\). Here \\(T\\) is the number of diffusion steps, \\(F_{\\epsilon}\\) cost of one denoiser pass, and \\(F_{D_S},F_{E_L},F_{D_L}\\) are decoder/encoder costs. Multi-concept adds ~\\(O(F_{D_S})\\) per extra watermark region.\n\nComputational_Properties:\n- Time Complexity:\n  - Training (per batch of size B): O(B·(T·F_ε + F_{E_L} + F_{D_L} + F_{D_S})). The BCE head adds one forward/backward through D_S and D_L; D_S is fixed but differentiable, so its activations contribute to backprop.\n  - Inference (per image): O(T·F_ε + F_{D_S}). No dataset-wide nearest-neighbor search (contrast with correlation methods).\n  - Offline watermark construction: O(N·K·F_{E_S}) for N concepts, K residuals per concept (K=100).\n- Space Complexity:\n  - Model activations: O(B·H_l·W_l·C_l) for latent diffusion features over T steps; added memory for D_S and D_L activations during BCE backprop.\n  - Watermark storage: O(N·h·w·c) floats (c=3). For N=216 and h=w=256, storage is on the order of tens of MB with FP32.\n  - Secrets: O(N·b) bits (b=160).\n- Parallelization:\n  - Fully compatible with data-parallel multi-GPU training; encryption T(·) is a per-image addition and trivially parallelizable.\n  - BCE and D_S decoding are per-sample convolutions amenable to tensor-core acceleration and can be pipelined with denoiser steps.\n- Hardware Compatibility:\n  - GPU-friendly; memory access patterns follow standard Conv/UNet diffusion training. D_S adds a small conv net pass with negligible bandwidth overhead relative to the denoiser.\n  - CPU inference feasible for attribution (D_S only), but diffusion sampling remains GPU-preferred.\n- Training vs. Inference:\n  - Training adds BCE and gradients through D_S/D_L; inference adds only one or two D_S passes (multi-concept doubles).\n  - Reported measured attribution decoder time ≈5.6 ms on A100; correlation-based baselines additionally incur feature extraction plus dataset search (e.g., >100 ms for 20k images).\n- Parameter Count:\n  - No changes to LDM parameters; D_S is pretrained and fixed (no added trainable parameters). Storage increases by D_S checkpoint and N watermarks.\n- Numerical Stability:\n  - Stability controlled by α (weighting BCE) and m (watermark strength). Too large m degrades visual fidelity; empirically m≈0.3 balances PSNR and attribution.\n  - Orthogonal secrets s_j (low pairwise cosine similarity when mapped to \\{-1,1\\}^b) mitigate bit confusion; BCE on bits (not on spatial residuals) reduces prediction variance.\n  - Backprop through fixed D_S avoids co-adaptation instability seen when training a new decoder jointly; removing BCE collapses attribution performance.\n- Scaling Behavior:\n  - With increasing N, attribution accuracy decreases due to higher inter-secret confusion; however, complexity scales linearly in N only for offline watermark construction and storage, not for inference (no search).\n  - Multi-concept scales linearly in number of embedded regions, with near-constant additional cost per region (\\(F_{D_S}\\)).\n  - Data efficiency: effective with as few as ~10 images per concept; performance degrades modestly as per-image counts drop, indicating robust scaling in limited-data regimes.\n- Implementation Notes / Pitfalls:\n  - Maintain fixed D_S and ensure it is differentiable to pass gradients back to ϵ_θ via D_L; if training a generic decoder end-to-end, expect lower performance and higher memory due to predicting full-resolution watermark maps.\n  - Choose disjoint spatial regions for multi-watermark to avoid interference; left/right or top/bottom splits show similar robustness.\n  - Ensure secrets are randomized and near-orthogonal; monitor bit-level accuracy alongside attribution accuracy to diagnose watermark recovery issues.",
        "IMPLEMENTATION_GUIDANCE": "Integration_Strategy:\n- Where to integrate in Latent Diffusion Models (LDMs):\n  - Data pipeline: insert an encryption transform that adds a precomputed spatial watermark W_j to each training RGB image X before encoding to latent. Place it right before EL(X) in the LDM input path.\n  - Denoiser training step: augment the loss with an auxiliary BCE on the secret bits decoded from DL(zhat). Keep the standard LDM noise-prediction loss unchanged.\n  - Inference path: after sampling an image, run the secret decoder DS on the generated image (or its decoded version if operating in latent space) to recover the bit sequence and attribute via Hamming similarity.\n- Code-level changes (PyTorch reference; similar mapping for TF/JAX):\n  - Precompute orthogonal bit secrets and spatial watermarks:\n    - secrets: generate N binary vectors s_j ∈ {0,1}^b (b=160) with min Hamming distance ≥ 0.4b. Persist to disk (JSON/NPY).\n    - spatial watermarks: for each s_j, compute W_j via RoSteALS secret encoder ES as:\n      - For i in 1..100, compute X_i_w = ES(X_i, s_j), residual r_i = X_i − X_i_w; then W_j = mean_i(r_i).\n      - Save W_j as float32 HxW (grayscale or 3-channel replicated).\n  - Dataset transform:\n    - def encrypt_image(X, W_j, m): return clamp(X + m * resize(W_j, H, W), 0, 1).\n    - Multi-watermark mode (two concepts): split image either horizontally or vertically; apply R(W_i, H, W/2) to left/right (or top/bottom).\n  - Training step modifications:\n    - z = EL(encrypt_image(X, W_j, m)).\n    - Standard diffusion step to get z_t and predict epsilon_hat = epsilon_theta(z_t, t, cond).\n    - z_hat = predict_denoised_latent(z_t, epsilon_hat, t) using your usual LDM schedule.\n    - X_hat = DL(z_hat) in pixel space.\n    - s_hat = DS(X_hat) -> logits or probabilities for b bits.\n    - Loss: L = L_LDM + α * BCEWithLogitsLoss(s_hat, s_j).\n    - Ensure DS parameters are frozen (p.requires_grad=False) but kept in the graph to backprop through DS to DL and epsilon_theta.\n  - Inference-time attribution:\n    - Generate image X_gen via standard sampling.\n    - s_hat = DS(X_gen).\n    - Attribute via argmax_j f(s_hat, s_j) where f is bitwise match count (or 1 − normalized Hamming distance).\n    - Optional abstention: if max match < τ*b (recommend τ ∈ [0.6, 0.75]), return “unattributed”.\n  - Multi-watermark inference:\n    - Split X_gen same way as training; run DS on each half; attribute each secret independently, then report pair.\n  - Compatibility:\n    - Works with Stable Diffusion-style LDMs (latent autoencoder EL/DL frozen). No change to U-Net architecture.\n    - Keep DL and EL frozen as in standard LDM; gradients still flow through them.\n    - RoSteALS dependency: use pretrained ES and DS; do not fine-tune them.\n  - Migration path:\n    - For a pretrained conditional LDM (e.g., ImageNet class-conditional), fine-tune with watermarked data for 3k–15k iterations with α > 0, m ∈ [0.2, 0.4]. This is sufficient to bind concepts to secrets without retraining from scratch.\n    - For unconditional LDM, train from scratch or re-train head for 15k–100k iterations depending on dataset size.\n  - Dependencies and kernels:\n    - PyTorch ≥ 1.13 (or 2.0) with AMP; torchvision for data; RoSteALS code and weights.\n    - xFormers/FlashAttention optional; not required for correctness.\n  - Training pipeline integration:\n    - Add concept-to-secret mapping in the dataset; each sample yields (X, s_j, W_j).\n    - Mixed precision (fp16/bf16) is safe for LDM; keep BCE on logits in fp32 for numerical stability (cast as needed).\n\nParameter_Settings:\n- Core hyperparameters:\n  - Secret length b: 160 (as used). For larger concept counts (≥1k), you may raise to 192–256; ensure DS supports chosen b.\n  - Watermark strength m (Eq. 2): 0.3 default. Effective range 0.2–0.5. Plateau in accuracy beyond 0.5 with visible artifacts; avoid >0.7 unless attribution priority outweighs image quality.\n  - Loss weight α: 2.0 default. Range 1.0–4.0. Increase if bit accuracy plateaus below 0.9; decrease if image quality degrades excessively.\n  - Min Hamming distance between secrets: ≥0.4b recommended (64 for b=160); aim for 0.5b when possible.\n  - Averaging count for W_j residuals: 100 images per concept. Range 50–200; 100 balances stability and preprocessing cost.\n- Optimizer and schedule:\n  - Learning rate for U-Net: 3.2e-5 (as reported) with AdamW (β1=0.9, β2=0.999, wd=1e-2). Range 1e-5–5e-5.\n  - Batch size: 32 at 256×256 (per 8×A100). Scale to 4–16 per 24GB GPU; use grad accumulation to reach effective ≥32.\n  - Iterations: 15k for small-scale; 50k–200k for large datasets.\n  - EMA: keep your standard EMA (e.g., 0.999–0.9999).\n- Initialization:\n  - Secrets: random Bernoulli(0.5) with rejection sampling to satisfy Hamming distance constraint.\n  - U-Net weights: pretrained (preferable for conditional) or Xavier/SD-init if from scratch.\n  - α warmup: optional linear warmup from 0 to α over 1k–5k steps to avoid early overfitting of watermarking signal.\n- Critical vs robust parameters:\n  - Critical: m, α, min Hamming distance, DS/ES versions compatibility with b.\n  - Robust: optimizer defaults, batch size (with accumulation), watermark placement (left/right/top/bottom are equivalent).\n- Parameter interactions:\n  - Increasing m raises BCE signal but reduces PSNR and may slightly increase FID; counterbalance by lowering α or using augmentation.\n  - More concepts N require larger b or stronger m to keep per-bit accuracy high; if raising b is not possible, increase α or training iterations.\n- Scale-dependent:\n  - Small models (<=400M params): m closer to 0.35–0.45 and α 2–3 to ensure sufficient signal.\n  - Large models (>=800M params): m 0.25–0.35 and α 1.5–2.5 usually suffice; larger capacity retains watermarks more easily.\n- Hardware-dependent:\n  - A100 40/80GB: 256–512 px training feasible; keep m=0.3, α=2.0; batch 32–64 at 256 px.\n  - 3090/4090 24GB: 256 px recommended; batch 8–16 with grad accumulation; AMP enabled; gradient checkpointing to fit.\n  - CPU preprocessing: precompute W_j offline to avoid overhead; store resized W_j for used resolutions.\n\nApplication_Conditions:\n- Beneficial scenarios:\n  - Need causal attribution of generated images to training concepts (objects, scenes, styles, ownership/templates).\n  - Datasets organized into N concept groups with ≥10 images per concept (works down to ~10; ideal 50–700).\n  - Unconditional or conditional diffusion training/fine-tuning where you control the training data and can encrypt it.\n- Hardware requirements:\n  - Minimum: 1×24GB GPU for 256×256 training; 8×A100 (as in paper) for faster runs and larger batches.\n  - Inference/attribution: DS decode per image ~5.6 ms on A100; <15 ms on 3090 typical.\n- Scale considerations:\n  - Works up to at least N=216 concepts with 24 images per concept (achieved ~82% attribution). For N≈1k concepts, prefer conditional training or increase b and iterations.\n  - Multi-concept attribution: practical for 2 concepts per image with split-halves; scalable to more splits but with diminishing returns and added complexity.\n- Task compatibility:\n  - Most effective on diffusion models with LDM architecture; both unconditional and class-conditional validated. Text-to-image LDMs are compatible but require mapping prompt concepts to concept groups for evaluation.\n  - Neutral/less effective: when you cannot modify training data (e.g., closed-source pretrained models without fine-tuning).\n- Alternative comparisons:\n  - Choose ProMark over correlation-based attribution (CLIP/ALADIN/SSCD) when legal or financial attribution requires causal evidence rather than similarity.\n  - If only copy-detection or near-duplicate matching is needed, correlation methods may suffice and avoid training changes.\n- Resource constraints:\n  - If preprocessing time is constrained, reduce residual averaging to 50; if memory constrained, store W_j in half precision and upcast at runtime.\n  - If visual quality must be pristine, keep m ≤ 0.3 and consider α warmup; expect slightly lower attribution accuracy.\n\nExpected_Outcomes:\n- Performance improvements (attribution accuracy compared to baselines):\n  - Unconditional models at 256 px, m=0.3, α=2 on Stock/LSUN/WikiArt: near state-of-the-art; typically 83–100% attribution vs 37–99% for baselines depending on dataset. At full strength (m=1.0) near-perfect accuracy on all tested datasets.\n  - Multi-concept (BAM, two watermarks): Combined accuracy ~90% at m=1.0; ~85% at m=0.3. Correlation baselines ~37–47%.\n  - Conditional ImageNet: Held-out ~91–96% (m=0.3–1.0), newly sampled ~87–90%, vs best embedding baseline ~50–63%.\n- Quality trade-offs:\n  - FID increase on ImageNet conditional from ~13.28 (no watermark) to ~17.63 (with watermark, m=0.3). PSNR vs watermark strength roughly: 58–60 dB at m=0.01, 50–52 dB at m=0.1, 44–46 dB at m=0.3, 36–38 dB at m=1.0.\n  - Visual artifacts (“bubble-like”) become noticeable for m ≥ 0.5; keep m ≤ 0.3 for imperceptible or mild artifacts.\n- Timeline expectations:\n  - Fine-tuning a pretrained conditional LDM: 3k–15k steps sufficient to bind watermarks and reach >85% attribution.\n  - Training from scratch: 15k–100k steps depending on data size and N.\n- Robustness:\n  - Against 14 common degradations, attribution ~89–90% with m=0.3 (vs 95% clean).\n  - Concept leakage: none detected; watermark detection falls to chance (~50%) when no watermark is present or latent noise is used; model embeds correct concept watermark during inversion.\n- Failure modes:\n  - Too many concepts with short secrets or weak m leads to bit confusion and attribution drop (e.g., 82% at N=216 with 24 imgs/concept).\n  - Very low per-concept data (<10 images) may slow convergence; accuracy drops by ~2–3% when going from 700 to 10 images/concept.\n  - Overly strong m or α can degrade image fidelity and training stability (monitor exploding BCE or artifacts).\n  - If DS is inadvertently trainable, loss may collapse; ensure DS is frozen.\n- Debugging indicators and validation:\n  - Track bitwise accuracy (per-bit correctness) alongside attribution accuracy; aim for per-bit ≥0.95 at convergence for easy tasks; sudden drops indicate mis-normalization into DS.\n  - Sanity checks:\n    - With random Gaussian noise input to DS, accuracy should be ~50% per bit (chance).\n    - If you encrypt evaluation images with the wrong watermark, attribution should still decode to the correct concept after a full generation pass (reported ~94% vs 95.6%).\n  - If BCE plateaus >0.4 with low bit accuracy:\n    - Verify image ranges into DS (expect [0,1] or DS’s expected normalization).\n    - Confirm W_j resizing and placement matches training/inference.\n    - Increase α by 0.5–1.0 or m by 0.05–0.1; or extend training by 5k–10k steps.\n- Hardware-specific outcomes:\n  - A100: DS decode ~5.6 ms per image; attribution overhead negligible vs generation.\n  - 3090/4090: expect 8–15 ms decode; still faster than embedding search (CLIP+index ~100 ms on 20k images).\n- Validation procedures:\n  - Held-out encryption protocol: encrypt held-out images with their concept watermark, partially diffuse to random t, denoise with trained model, decode DS, compute attribution accuracy via Eq. (9).\n  - Newly sampled images: sample per concept; decode DS; compare to intended concept (labels/conditions).\n  - Multi-watermark validation: measure per-secret accuracy and combined accuracy; test multiple spatial placements (left/right/top/bottom) to verify invariance (expect ~90% combined).\n- Risk mitigation:\n  - Choose m in [0.25, 0.35] and α in [1.5, 2.5] for best quality/accuracy balance.\n  - Ensure secrets have large Hamming separation; regenerate secrets if any pair <0.4b.\n  - Freeze DS parameters; enable gradient flow through DS into LDM."
    }
]