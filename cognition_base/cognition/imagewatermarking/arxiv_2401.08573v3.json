[
    {
        "DESIGN_INSIGHT": "### DESIGN_INSIGHT_HIGH: [Unified Performance–Quality Normalized Benchmarking (WAVES-PQ) – Quantile-Aligned Multi-Metric Scoring with TPR@0.1%FPR]\nWAVES replaces AUROC- or p-value-centric evaluations and single-metric quality checks with a unified, architecture-like benchmarking pipeline that jointly optimizes for watermark detection performance and perceptual fidelity. Instead of reporting raw ROC areas or idiosyncratic p-values, the framework standardizes performance at strict operating points and normalizes heterogeneous quality metrics into a single, comparable axis to produce Performance vs. Quality 2D curves per attack and watermark.\n\nThe key mechanism is a quantile-based normalization and category-balanced aggregation of eight heterogeneous quality metrics coupled with a stringent detection axis given by TPR at a fixed FPR. For each quality metric m with raw score u_m(x), WAVES defines its normalized value via the empirical 10th/90th percentiles over all attacked images:\n\\[ \\tilde{u}_m(x) \\;=\\; 0.1 \\;+\\; 0.8 \\cdot \\mathrm{clip}\\!\\left(\\frac{a_m \\,\\big(u_m(x)-q_{0.1}(m)\\big)}{q_{0.9}(m)-q_{0.1}(m)},\\,0,\\,1\\right), \\]\nwhere a_m∈{+1,-1} orients the metric so larger implies better quality (smaller degradation). Category-level averages are computed for similarities {PSNR, SSIM, NMI}, distribution distances {FID, CLIP-FID}, perceptual {LPIPS}, and human-centered quality {Aesthetics, Artifacts}, and the final “Normalized Quality Degradation” is the mean of the four category means. Performance is fixed to\n\\[ \\mathrm{Perf}(x) \\;=\\; \\mathrm{TPR}@\\mathrm{FPR}\\le\\alpha, \\quad \\alpha = 0.001. \\]\n\nFundamentally different from prior evaluations, WAVES makes performance comparable across watermark families with dissimilar message spaces and statistical tests by (i) fixing a stringent operating point TPR@0.1%FPR instead of AUROC and (ii) mapping multiple quality axes onto a shared scale using global quantiles derived from a large, diverse pool of attacked images. A second contribution is a principled leaderboard protocol that ranks attacks by quality at fixed performance thresholds:\n\\[ \\text{Q@}P^\\star \\;\\;\\text{with}\\;\\; P^\\star\\in\\{0.95,\\,0.7\\}, \\]\nand tie-breaks with average performance and quality over strengths, yielding apples-to-apples comparisons across datasets and watermark types.\n\n### DESIGN_INSIGHT_MEDIUM: [Rinsing Regeneration (R_diff^k) – Multi-Cycle Diffusion Purification with Prompted and Mixed Variants]\nWAVES augments single-pass regeneration attacks by introducing a multi-cycle “rinsing” operator that repeatedly applies forward noising and reverse denoising through a surrogate diffusion model, optionally conditioned on prompts and/or followed by a VAE denoiser. This replaces prior single regeneration (one noise–denoise pass) with a compositional operator that more aggressively perturbs the latent manifold supporting watermark signals while allowing explicit control of perceptual loss.\n\nLet F_{β}^{(s)} denote s steps of forward noising and D_{θ’}^{(s)} the corresponding s-step reverse process by a surrogate diffusion θ’ (lower-version model). Define single regeneration:\n\\[ R_{\\mathrm{diff}}(x;\\,s) \\;=\\; D_{θ’}^{(s)}\\!\\big(F_{β}^{(s)}(x)\\big), \\]\nits k-cycle “rinsing” variant:\n\\[ R_{\\mathrm{diff}}^{k}(x;\\,s) \\;=\\; \\underbrace{R_{\\mathrm{diff}}(\\cdot;\\,s)\\,\\circ\\,\\cdots\\,\\circ\\,R_{\\mathrm{diff}}(\\cdot;\\,s)}_{k\\ \\text{times}}(x), \\]\nand the prompted form \\(R_{\\mathrm{diff}}^{k}(x;\\,s, y)\\) that conditions D_{θ’} on prompt y. WAVES also defines “mixed regeneration” with a VAE denoiser:\n\\[ R_{\\mathrm{mix}}(x) \\;=\\; \\mathrm{Dec}_{\\psi}\\!\\big(\\mathrm{Enc}_{\\psi}( R_{\\mathrm{diff}}^{k}(x;\\,s) )\\big), \\]\nand studies KL-VAEs with controllable bottlenecks b via \\(R_{\\mathrm{KLVAE}}(x;b)=\\mathrm{Dec}_b(\\mathrm{Enc}_b(x))\\).\n\nThe fundamental difference from prior work is the architectural composition of multiple regeneration passes and modalities (diffusion + VAE) to trace quality–performance Pareto fronts; e.g., k=2 often maximizes TPR reduction at modest normalized quality degradation, whereas k=4 pushes further at higher perceptual cost. Prompted rinsing reflects realistic attacker capabilities (known prompts for API-generated content), while mixed regeneration restores appearance after aggressive rinsing, revealing nuanced trade-offs invisible to single-pass evaluations.\n\n### DESIGN_INSIGHT_MEDIUM: [Transferable Feature-Space Adversaries under Limited Knowledge – Embedding-Shift PGD and Surrogate-Detector Targeting]\nInstead of directly attacking proprietary watermark decoders, WAVES introduces encoder-centric and surrogate-detector adversaries that operate under limited or no knowledge of the victim system. The core replacement is to perturb shared feature embeddings that upstream watermark pipelines rely on (e.g., VAE encoders in latent diffusion) or to attack a trained surrogate classifier, thereby inducing detection failures without access to the true detector.\n\nEmbedding-shift attacks maximize feature drift under an L∞ budget:\n\\[ \\max_{x_{\\mathrm{adv}}}\\ \\big\\|f(x_{\\mathrm{adv}}) - f(x)\\big\\|_2^2 \\quad \\text{s.t.}\\quad \\|x_{\\mathrm{adv}}-x\\|_{\\infty} \\le \\varepsilon, \\]\nsolved via PGD. WAVES instantiates f as: (i) grey-box KL-VAE f_8 used by the victim latent diffusion, and (ii) black-box proxies (CLIP image encoder, ResNet18 pre-logits, KL-VAE f_{16}, SDXL-VAE). For surrogate-detector attacks, a classifier g_φ:X→{0,1} (or multi-user) is trained on accessible data (e.g., watermarked vs. non-watermarked, real vs. watermarked, or WM1 vs. WM2). A targeted PGD then solves\n\\[ \\min_{x_{\\mathrm{adv}}}\\ \\mathcal{L}_{\\mathrm{CE}}\\!\\big(g_{\\phi}(x_{\\mathrm{adv}}),\\,y_{\\text{target}}\\big) \\quad \\text{s.t.}\\quad \\|x_{\\mathrm{adv}}-x\\|_{\\infty} \\le \\varepsilon, \\]\nto flip detection or induce misidentification.\n\nThis differs from prior adaptive attacks that assume white-box access to the watermarking algorithm: here, transfer is induced by exploiting the architectural dependencies of watermark recovery on feature encoders (e.g., Tree-Ring’s KL-VAE + DDIM inversion path) and by training realistic surrogates from data obtainable via public APIs. WAVES also formalizes knowledge regimes (grey/black-box encoders; three surrogate-training setups) and empirically shows asymmetric feasibility: removal succeeds via broad latent disturbance, while “spoofing” (adding a precise watermark) largely fails—highlighting distinct robustness surfaces for watermark erasure vs. fabrication.",
        "EXPERIMENTAL_TRIGGER_PATTERNS": "**Task_Performance_Signatures:**\n- Watermark detection (primary KPI in WAVES):\n  - Expect improved robustness characterization via TPR@0.1% FPR vs. Normalized Quality Degradation 2D curves; curves should be monotonic with attack strength and consistent across metrics (PSNR, SSIM, LPIPS, FID, CLIP-FID, Aesthetics, Artifacts).\n  - Tree-Ring:\n    - Expect degraded performance under adversarial embedding when grey-box access to the same KL-VAE encoder exists: TPR@0.1% FPR drops toward 0 across ϵ ∈ {2/255, 4/255, 6/255, 8/255}; Normalized Quality Degradation remains low (≈0.16–0.25).\n    - Surrogate detector attacks (UnWM&WM, WM1&WM2): TPR@0.1% FPR falls below 0.7 at low quality degradation (Q@0.7P ≈ 0.10–0.14), indicating high potency with minimal visual impact; spoofing (adding watermarks) fails.\n    - Regeneration: single diffusion/VAEs reduce TPR while maintaining moderate quality (Q ≈ 0.30–0.35); rinsing 2× achieves better balance (TPR below 0.7 at Q ≈ 0.33–0.37), 4× further reduces TPR with noticeable quality drop (Q ≈ 0.47).\n    - Distortions: mostly stable; strong blur and geometric combos can reduce TPR with high quality degradation (Q ≈ 0.70–1.22).\n  - Stable Signature:\n    - Regeneration attacks are completely destructive: Avg TPR ≈ 0.0–0.22 for Regen-Diff/DiffP/KLVAE; Q ≈ 0.20–0.33; expect TPR collapse at low FPR even for single regeneration.\n    - Distortions largely stable except blur: Dist-Blur shows Avg P ≈ 0.0 at high quality loss (Q ≈ 1.20); other distortions maintain high TPR (>0.95) at moderate Q.\n    - Adversarial embedding/surrogates: generally robust; expect near-baseline TPR (>0.99) and low Q (≈0.21–0.25).\n  - StegaStamp:\n    - Overall robust: detection remains competitive (Avg TPR ≈ 0.94–1.0) under most attacks; regeneration affects TPR at higher strengths but average detection stays high; distortion combos can lower detection but at larger Q (≈0.66–0.87).\n    - Adversarial embedding/surrogates: minimal transfer, TPR ≈ 1.0; Q low (≈0.20–0.24).\n- User identification (K-way classification):\n  - Identification accuracy degrades similarly to detection; vulnerability increases with user count.\n  - With 1,000,000 users: expect attacks that already depress detection to reduce identification accuracy significantly; Tree-Ring most vulnerable under rinsing and adversarial embedding; Stable Signature collapses under regeneration; StegaStamp maintains higher accuracy under most settings.\n  - Targeted misidentification using AdvCls-WM1&WM2 rarely succeeds; success rates ≤1% even at high perturbation (ϵ=8/255); with 100 users, targeted mis-ID may reach ≈0.7% under strong attacks.\n- Cross-dataset consistency:\n  - Attack rankings are consistent across DiffusionDB, MS-COCO, and DALL·E3; CLIP-based embedding attacks show slightly stronger transfer on MS-COCO due to training distribution alignment.\n- Quality metric aggregation:\n  - Eight metrics yield similar attack potency rankings; normalized aggregation reduces metric-specific variance; expect CDF-normalized values concentrated in [0.1, 0.9].\n- Attack potency thresholds (leaderboard triggers):\n  - Q@0.95P and Q@0.7P: strong attacks achieve low Q at threshold crossings (e.g., Tree-Ring: AdvEmbG-KLVAE8 Q@0.7P ≈ 0.164; AdvCls-UnWM&WM Q@0.7P ≈ 0.102).\n  - Avg P and Avg Q: strong attacks exhibit low Avg P with small Avg Q for adversarial categories; regeneration induces moderate Avg Q with low Avg P; distortions require high Avg Q to lower Avg P.\n- Scale/condition dependencies:\n  - Improvements in robustness insight are most pronounced for long attack chains (rinsing 4×) and grey-box scenarios (public VAE encoders); identification becomes more fragile as |Q| (users) increases.\n- Training vs. inference differences:\n  - Surrogate detector training reaches near-100% validation accuracy quickly (≤10 epochs) but adversarial examples show limited transfer for Stable Signature and StegaStamp; inference-side PGD attacks on embeddings transfer strongly to Tree-Ring’s latent-space-based detector.\n- Quantitative expectations beyond watermark domain (required benchmarks list):\n  - Language modeling (lambada_openai, wikitext, ptb): expect stable/unchanged performance (technique is watermark robustness evaluation; no LM impact).\n  - Reading comprehension (squad_completion, squad_v2, narrativeqa): stable/unchanged.\n  - Commonsense reasoning (hellaswag, piqa, social_iqa, commonsenseqa): stable/unchanged.\n  - Factual QA (arc_easy, arc_challenge, boolq, openbookqa): stable/unchanged.\n  - Context resolution (winogrande, winograd): stable/unchanged.\n  - Other tasks (swde, fda): stable/unchanged; WAVES does not alter task accuracy, but provides robustness stress signals for image watermarking pipelines.\n\n**Architectural_Symptoms:**\n- Training characteristics (surrogate detectors and normalization pipeline):\n  - Surrogate detector fine-tuning (ResNet18) converges rapidly (≈10 epochs, LR≈1e-3, batch≈128) with validation accuracy ≈100% on binary tasks (WM vs. unWM, WM1 vs. WM2); loss curves show smooth decline and low variance.\n  - PGD attack behavior:\n    - Embedding attacks (200 iterations, α≈0.05×ϵ) produce consistent reductions in latent feature similarity; no NaNs; stable under ϵ up to 8/255.\n    - Surrogate-targeted attacks (50 iterations, α≈0.01×ϵ) flip labels reliably for Tree-Ring; limited transfer for Stable Signature/StegaStamp indicates reliance on non-transferrable features (transferability gap symptom).\n  - CDF-based normalization:\n    - Metric distributions show near-linear behavior between 10% and 90% quantiles; out-of-band values are rare; normalized metrics across categories average to stable aggregate scores; rankings consistent within ±0.01 tolerance.\n- Runtime/memory behaviors:\n  - Diffusion regeneration:\n    - Throughput scales approximately linearly with total timesteps; rinsing (2×/4×) multiplies compute proportionally; memory growth is linear with batch size and model; OOM unlikely at standard batch sizes used in WAVES.\n  - VAE regeneration:\n    - Encoder/decoder passes maintain constant per-image memory; bottleneck size (e.g., f8 vs. f16 vs. f32) affects latency minimally; grey-box encoder usage yields higher GPU utilization due to gradient-based attacks.\n  - Feature extraction:\n    - CLIP-FID/LPIPS/Aesthetics/Artifacts computations dominate evaluation time; GPU utilization high and steady; CPU-bound steps (JPEG, rotation) show lower GPU utilization.\n  - Embedding PGD attacks:\n    - Compute-bound; memory footprint stable across ϵ; consistent speed regardless of image content; sequence length not applicable; no OOM errors at tested batch sizes.\n- Profiling signatures that indicate the technique is working:\n  - Monotonic Performance vs. Quality curves per attack strength; clear separations between attack categories:\n    - Adversarial attacks: low Q (≈0.10–0.25) with sharp TPR drops for Tree-Ring; minimal TPR change for Stable Signature/StegaStamp.\n    - Regeneration: moderate Q (≈0.30–0.48) with significant TPR declines across Tree-Ring and Stable Signature; StegaStamp less impacted on average.\n    - Distortions: high Q needed to impact TPR; curves shift rightward substantially (Q≈0.70–1.20).\n  - Cross-dataset consistency:\n    - Attack leaderboards remain consistent across DiffusionDB/MS-COCO/DALL·E3; CLIP-based attack potency slightly higher on MS-COCO (symptom of distribution alignment).\n  - Identification scaling symptom:\n    - As the number of users increases (100 → 1,000,000), the identification accuracy curves drop faster under the same attack strengths; targeted mis-ID rates stay ≤1% under strong attacks, confirming limited precision of surrogate-based perturbations.\n- Stability indicators and negative/neutral effects:\n  - Less likely to diverge: no NaNs observed in metric computation or PGD optimization; robust to moderate hyperparameter changes (steps, step size).\n  - Neutral/negative: Prompted regeneration requires prompt access (operational dependency); surrogate detectors may overfit to non-watermark features (high validation accuracy but poor transfer), leading to neutral/negative transfer to true detectors for Stable Signature/StegaStamp.\n  - Architectural vulnerability symptoms:\n    - Public VAE usage (grey-box) correlates with near-zero TPR for Tree-Ring under AdvEmbG-KLVAE8 while keeping Q low; Stable Signature’s decoder-rooted watermark shows collapse under alternative decoders via regeneration.",
        "BACKGROUND": "Title: WAVES: Benchmarking the Robustness of Image Watermarks\n\nHistorical Technical Context:\nInvisible image watermarking has evolved from classical signal-processing schemes to deep learning and diffusion-era methods. Early approaches operated in transform domains such as DCT, DWT, and SVD, embedding spread-spectrum signals that offered partial invariance to translation, rotation, or mild compression, but were fragile under stronger manipulations and modern generative “purification” pipelines. With the rise of CNNs and encoder–decoder architectures, learned post-processing watermarks (e.g., HiDDeN, RivaGAN) trained decoders to recover messages after differentiable distortions. StegaStamp further pushed real-world robustness by injecting strong photometric, geometric, and printing/photography augmentations during end-to-end training.\n\nThe diffusion-model era (e.g., DDPMs, DDIM, latent diffusion with VAEs) introduced in-processing watermarking strategies. Model-wide tuning (e.g., pivotal tuning, artificial fingerprints), decoder-only fine-tuning in LDMs (Stable Signature), and seed/noise-space schemes that encode a key in the initial latent (Tree-Ring using DDIM inversion) became prevalent. Concurrently, a modern attack taxonomy emerged: (1) distortions (blur, crop, JPEG), (2) regeneration/purification via VAEs or diffusion noising–denoising that alters latent representations while preserving perceptual quality, and (3) adversarial attacks including transfer and surrogate-based methods. However, evaluations across works used inconsistent datasets, metrics (e.g., AUROC, bit accuracy, p-value thresholds), and attack strengths, obscuring fair, low-FPR comparisons central to safety-critical provenance use cases.\n\nTechnical Limitations:\n- Metric mismatch at low false-positive regimes: Prior evaluations often reported AUROC or p-values. AUROC does not guarantee high true positive rate (TPR) at very low FPR; p-values depend on message space, hypothesis tests, and thresholds, making cross-method comparisons non-standard. WAVES targets TPR@0.1% FPR.\n- No joint performance–quality analysis: Many studies maximize watermark removal without quantifying perceptual and distributional fidelity. This misses the operational trade-off between detector failure and utility. WAVES standardizes Performance vs. Quality 2D analyses across eight quality metrics.\n- Limited and uneven attack coverage: Earlier work emphasized single-shot distortions or single regeneration. Missing were stronger and realistic variants such as rinsing (multi-regeneration), prompt-guided regeneration, mixed regeneration (rinse + VAE denoising), and transferable adversarial embedding or surrogate-detector attacks.\n- Insufficient scale and heterogeneity: Previous benchmarks often used one dataset, small samples (≤1k), or only synthetic sources, weakening statistical power and transfer conclusions. WAVES uses three distinct datasets (DiffusionDB, MS-COCO, DALL·E 3), 5k images each, enabling tight FPR control.\n- Identification scalability untested: Identification (multi-class over users/messages) becomes harder as the number of users grows; prior works rarely report performance at large K. WAVES evaluates identification up to 1,000,000 users, revealing scaling behavior.\n- Architectural blind spots and leakage risks: Systems that reuse public VAEs (e.g., KL-VAEs) expose shared encoders exploitable by adversarial embedding. Prior evaluations rarely tested grey-box feature-space attacks aligned with the victim encoder.\n- Computational scaling of realistic attacks underexplored: Diffusion regeneration costs O(T) denoising steps; rinsing costs O(rT) for r cycles. Adversarial PGD in feature space costs O(K) gradient steps per image. Evaluations must factor these costs when comparing attack practicality at scale.\n\nPaper Concepts:\n- Invisible Watermark: A signal embedded in an image x to convey a message m∈M without perceptible artifacts. Post-processing methods apply an encoder E to produce x̂=E(x,m); in-processing methods integrate watermarking into the generative process x=EMBED(θ_G,m). Quality-preserving robustness is judged after attacks A, via recovery m′=DECODE(x′) with x′=A(x).\n- Low-FPR Operating Point (TPR@x%FPR): For a detector score s(x), and threshold τ chosen such that FPR=Pr[s(x_clean)≥τ]=x%, the main metric is TPR@x%FPR = Pr[s(x_wm)≥τ]. WAVES fixes x=0.1% to reflect provenance-critical false-positive aversion.\n- p-value Verification for Message-Based Watermarks: Given recovered m′ and ground-truth m, define a dissimilarity D(·,·) over message space M. The verification p-value is\n  p = Pω∼M[ D(ω,m′) < D(m,m′) | H0 ],\n  where H0 denotes no access to the watermark during generation. A decision rule VERIFYα returns 1 if p<α. WAVES discusses p-values but bases comparisons on ROC-derived metrics to avoid test- and threshold-dependence.\n- Normalized Quality Degradation: Let q_k(x,x′) denote quality metrics (e.g., PSNR, SSIM, FID, CLIP-FID, LPIPS, aesthetics, artifacts). WAVES normalizes each metric via 10th and 90th percentiles over all attacks to a common [0.1,0.9] scale and aggregates:\n  \\[\n  \\tilde{q}_k = \\mathrm{Norm}_{k}(q_k), \\quad Q = \\frac{1}{4}\\sum_{c=1}^{4}\\left(\\frac{1}{|C_c|}\\sum_{k\\in C_c}\\tilde{q}_k\\right),\n  \\]\n  where {C_c} are the four metric categories. Lower Q = less degradation (better).\n- Regeneration and Rinsing Attacks: A single regeneration applies a VAE or diffusion mapping R(·) that (ideally) preserves semantics while altering latent representations: x′=R(x;T), with diffusion cost O(T). Rinsing repeats noising–denoising r times, x′=R_r(x;T)=R∘…∘R(x) (r compositions), with cost O(rT). Prompted regeneration conditions on the original prompt; mixed regeneration applies VAE denoising after rinsing to restore quality.\n- Adversarial Embedding Attack (feature-space PGD): Given encoder f: X→Z, craft x_adv within an ℓ∞-ball to maximally separate embeddings:\n  \\[\n  \\max_{\\|x_{\\text{adv}}-x\\|_\\infty \\le \\varepsilon} \\ \\| f(x_{\\text{adv}}) - f(x) \\|_2,\n  \\]\n  approximated by PGD over K steps. Transfer is strongest when f matches a public victim encoder (e.g., the KL-VAE used by the watermark or generator).\n- Surrogate Detector Targeted Attack: Train a surrogate classifier \\(\\hat{f}\\) on available data (e.g., watermarked vs. unwatermarked, or two watermark messages), then solve\n  \\[\n  \\min_{\\|x_{\\text{adv}}-x\\|_\\infty \\le \\varepsilon} \\ \\mathcal{L}\\big(\\hat{f}(x_{\\text{adv}}), y_{\\text{target}}\\big),\n  \\]\n  to induce misclassification (removal/spoofing or cross-user confusion). Success depends on feature alignment between \\(\\hat{f}\\) and the true detector.\n\nExperimental Context:\nWAVES standardizes robustness evaluation across detection and identification. It tests three representative watermarks—Stable Signature (in-processing, decoder fine-tuning), Tree-Ring (noise/seed-space with DDIM inversion), and StegaStamp (post-processing, physical-world robustness)—on three diverse datasets (DiffusionDB, MS-COCO, DALL·E 3), each with 5,000 images/prompts. Attacks span 26 methods in three families: distortions (single and combinations), regeneration (single, prompted, rinsing, mixed with VAE), and adversarial (feature-space embedding against RN18/CLIP/KL-VAEs; surrogate-detector attacks in multiple data settings). For each attack, strengths are swept (e.g., diffusion steps 40–200; rinsing 2×/4×; PGD ε∈{2,4,6,8}/255), and results are aggregated into Performance vs. Quality 2D plots using TPR@0.1% FPR on the y-axis and normalized quality degradation on the x-axis.\n\nThe evaluation philosophy emphasizes low-FPR reliability and quality-preserving attack potency. WAVES reports unified curves by aggregating eight quality metrics across four categories and three datasets, then ranks attacks via thresholds such as Q@0.95P and Q@0.7P (quality at TPR=0.95 or 0.7), plus Avg P and Avg Q over strengths. Identification is evaluated as multi-class classification over large user sets (e.g., 100 to 1,000,000), reporting accuracy vs. quality with the same aggregation. Overall goals are to (1) reveal hidden failure modes under realistic, stronger attacks, (2) quantify performance–quality trade-offs comparably, and (3) provide an attack and watermark leaderboard to guide robust design and deployment.",
        "ALGORITHMIC_INNOVATION": "Core_Algorithm:\n- Standardize robustness evaluation by replacing heterogeneous metrics and thresholds with a unified Performance–Quality protocol: compute TPR at a strict 0.1% FPR (TPR@0.1%FPR) versus an aggregated Normalized Quality Degradation metric. This yields a single 2D curve per attack and watermark, aggregated across datasets.\n- For each watermark method and dataset, generate watermarked images, apply each attack A at multiple strengths s, evaluate detection (or identification) performance, compute 8 quality metrics, normalize each metric via global 10%/90% quantiles, and aggregate to one normalized quality score. Plot TPR@0.1%FPR against normalized quality for all strengths, then rank attacks by the quality achieved at fixed performance thresholds (e.g., Q@0.95P and Q@0.7P).\n- Introduce new attack algorithms within the evaluation: (1) Rinsing regeneration (multi-cycle noise–denoise via diffusion), (2) Adversarial embedding attacks (PGD to maximize feature-space deviation of chosen encoders, including grey-box VAE), and (3) Surrogate-detector attacks (train a surrogate classifier and run targeted PGD to flip labels under l∞ constraints). Also evaluate prompted and mixed regeneration variants.\n- Aggregate over datasets to produce unified attack curves and watermarks’ radar plots; compute average performance and quality across strengths to benchmark both watermarks and attacks.\n\nKey_Mechanism:\n- Measuring TPR at very low FPR captures operational risk where false alarms are costly; pairing it with normalized quality reveals the Pareto tradeoff attackers face between watermark removal and visual utility. Quantile normalization equalizes disparate metric scales and distributions, enabling fair cross-metric aggregation.\n- Multi-cycle diffusion (“rinsing”) repeatedly perturbs the latent manifold, compounding deviations from the watermark carrier while partly preserving image semantics; feature-space PGD transfers because several detectors rely (explicitly or implicitly) on VAE/encoder latents, making latent perturbations degrade recovery even with minimal pixel changes.\n- Surrogate classifiers, when trained on watermarked/non-watermarked or two watermark messages, learn a projection into the victim’s latent space; targeted PGD on this surrogate disrupts watermark-bearing regions sufficiently to reduce detectability, even if message replacement is imprecise.\n\nMathematical_Formulation:\n- Watermark protocol:\n  - EMBED: \\( x = \\mathrm{EMBED}(\\theta_G, m) \\), DECODE: \\( m' = \\mathrm{DECODE}(x) \\).\n  - Verification by hypothesis test: \\( p = \\Pr_{\\omega \\sim \\mathcal{M}}\\left[D(\\omega,m') < D(m,m') \\mid H_0\\right] \\), and \\( \\mathrm{VERIFY}_\\alpha(m',m) = \\mathbb{1}[p < \\alpha] \\).\n- Detection/identification metrics:\n  - For a score function \\( s(x) \\), define TPR and FPR at threshold \\(\\tau\\): \\( \\mathrm{TPR}(\\tau) = \\Pr[s(x_{\\text{wm}}) \\ge \\tau], \\ \\mathrm{FPR}(\\tau) = \\Pr[s(x_{\\text{unwm}}) \\ge \\tau] \\).\n  - Operational performance is \\( \\mathrm{TPR}@\\mathrm{FPR}=\\varepsilon \\) by selecting \\( \\tau^\\star \\) s.t. \\( \\mathrm{FPR}(\\tau^\\star)=\\varepsilon \\); report \\( \\mathrm{TPR}(\\tau^\\star) \\) with \\(\\varepsilon=0.001\\).\n- Quality normalization and aggregation:\n  - Let raw quality metric \\(q_j(x)\\) have global quantiles \\(q_j^{0.1}\\) and \\(q_j^{0.9}\\) computed over all attacked watermarked images. Normalize to \\([0.1,0.9]\\) with monotone mapping:\n    \\[\n    \\hat{q}_j(x) = 0.1 + 0.8 \\cdot \\frac{\\operatorname{clip}(q_j(x), q_j^{0.1}, q_j^{0.9}) - q_j^{0.1}}{q_j^{0.9}-q_j^{0.1}},\n    \\]\n    with increasing \\(\\hat{q}_j\\) meaning higher degradation (metrics that are “higher-is-better” are inverted before normalization).\n  - Aggregate category means and global mean:\n    \\[\n    Q(x) = \\frac{1}{4}\\sum_{c=1}^{4} \\left( \\frac{1}{|J_c|} \\sum_{j \\in J_c} \\hat{q}_j(x) \\right),\n    \\]\n    where \\(J_c\\) are metric indices for the 4 categories (similarity, distribution, perceptual, aesthetics/artifacts).\n- Attack curves and ranking:\n  - For attack \\(A\\) at strength \\(s\\), compute performance \\(P_{A}(s)=\\mathrm{TPR}@0.1\\%\\mathrm{FPR}\\) and quality \\(Q_{A}(s)\\). Define thresholds \\(P^\\star \\in \\{0.95,0.7\\}\\) and compute quality at threshold:\n    \\[\n    Q@P^\\star(A) = \\min_{s} \\{ Q_A(s) \\ : \\ P_A(s) \\le P^\\star \\},\n    \\]\n    with “inf” if \\(P_A(s) > P^\\star\\ \\forall s\\). Also report averages \\( \\overline{P}(A)=\\frac{1}{S}\\sum_s P_A(s), \\ \\overline{Q}(A)=\\frac{1}{S}\\sum_s Q_A(s)\\).\n- Regeneration operators:\n  - Single diffusion regeneration (unconditional or prompted): let \\( \\mathcal{D}_t \\) be noising–denoising for \\(t\\) steps with surrogate diffusion \\(\\theta_{D}^{(1.4)}\\):\n    \\[\n    x' = \\mathrm{RegenDiff}(x;t) = \\mathcal{D}_t(x), \\quad x' = \\mathrm{RegenDiffP}(x;t,p) = \\mathcal{D}_t(x \\mid p).\n    \\]\n  - VAE regeneration: \\( x' = \\mathrm{Dec}(\\mathrm{Enc}(x;\\lambda);\\lambda) \\) with quality factor \\(\\lambda\\), or KL-VAE bottleneck \\(b\\): \\( x' = \\mathrm{Dec}_{\\mathrm{KL}}(\\mathrm{Enc}_{\\mathrm{KL}}(x;b);b)\\).\n  - Rinsing (multi-cycle diffusion): for \\(N\\) cycles and \\(t\\) steps per cycle,\n    \\[\n    x' = \\mathrm{Rinse}(x;N,t) = \\underbrace{\\mathcal{D}_t \\circ \\mathcal{D}_t \\circ \\cdots \\circ \\mathcal{D}_t}_{N\\ \\text{times}}(x).\n    \\]\n    Mixed regeneration appends a denoiser \\( \\mathcal{V} \\): \\( x' = \\mathcal{V}(\\mathrm{Rinse}(x;N,t)) \\).\n- Adversarial embedding attacks (untargeted, l∞-bounded PGD on encoder \\(f\\)):\n  \\[\n  \\max_{\\|x_{\\text{adv}}-x\\|_\\infty \\le \\epsilon} \\ \\| f(x_{\\text{adv}}) - f(x) \\|_2^2, \\quad\n  x_{k+1} = \\Pi_{B_\\infty(x,\\epsilon)}\\big(x_k + \\alpha \\cdot \\mathrm{sign}(\\nabla_{x_k} \\| f(x_k) - f(x) \\|_2^2)\\big).\n  \\]\n  Encoders include ResNet18 (pre-logit), CLIP image encoder, KL-VAE encoders (grey-box f8; black-box f16, SDXL-VAE).\n- Surrogate-detector attacks (targeted PGD on classifier \\(g\\)):\n  \\[\n  \\min_{\\|x_{\\text{adv}}-x\\|_\\infty \\le \\epsilon} \\ L\\big(g(x_{\\text{adv}}), y_{\\text{target}}\\big), \\quad\n  x_{k+1} = \\Pi_{B_\\infty(x,\\epsilon)}\\big(x_k - \\alpha \\cdot \\mathrm{sign}(\\nabla_{x_k} L(g(x_k), y_{\\text{target}}))\\big),\n  \\]\n  with training settings: UnWM&WM (binary watermarked vs. non), Real&WM (ImageNet vs. watermarked), WM1&WM2 (two users/messages).\n\nExample Format:\n- Normalized quality: \\( \\hat{q}_j(x) = 0.1 + 0.8\\frac{\\operatorname{clip}(q_j(x), q_j^{0.1}, q_j^{0.9}) - q_j^{0.1}}{q_j^{0.9}-q_j^{0.1}} \\).\n- Rinsing operator: \\( \\mathrm{Rinse}(x;N,t) = \\mathcal{D}_t^{\\circ N}(x) \\).\n- Embedding PGD step: \\( x_{k+1} = \\Pi_{B_\\infty}\\big(x_k + \\alpha \\cdot \\mathrm{sign}(\\nabla_{x_k}\\|f(x_k)-f(x)\\|_2^2)\\big) \\).\n- Surrogate PGD step: \\( x_{k+1} = \\Pi_{B_\\infty}\\big(x_k - \\alpha \\cdot \\mathrm{sign}(\\nabla_{x_k} L(g(x_k), y_{\\text{target}}))\\big) \\).\n- Attack ranking quality at performance threshold: \\( Q@P^\\star(A) = \\min_{s: P_A(s)\\le P^\\star} Q_A(s) \\).\n- Complexity (per watermark, per dataset): evaluation scales as \\(O(K S (N_{\\text{img}}) (C_{\\text{det}} + J))\\), where \\(K\\) attacks, \\(S\\) strengths, \\(J\\) metrics.\n\nComputational_Properties:\n- Time Complexity:\n  - Evaluation pipeline (per watermark w, dataset d): \\(O(K S N_{\\text{img}}(C_{\\text{det}} + J C_q))\\), where \\(C_{\\text{det}}\\) is detector cost (forward + ROC thresholding) and \\(C_q\\) is per-metric computation (PSNR/SSIM/LPIPS/FID/CLIP-FID/etc.).\n  - Regeneration attacks: Single diffusion \\(O(N_{\\text{img}} \\cdot t \\cdot C_{\\text{U\\!-\\!Net}})\\); Rinsing \\(O(N_{\\text{img}} \\cdot N t \\cdot C_{\\text{U\\!-\\!Net}})\\); VAE \\(O(N_{\\text{img}} \\cdot C_{\\text{enc}} + C_{\\text{dec}})\\).\n  - Adversarial embedding PGD: \\(O(N_{\\text{img}} \\cdot T_{\\text{PGD}} \\cdot C_{\\text{enc-grad}})\\).\n  - Surrogate-detector training: \\(O(E \\cdot |D_{\\text{train}}| \\cdot C_{\\text{clf-grad}})\\); attack inference \\(O(N_{\\text{img}} \\cdot T_{\\text{PGD}} \\cdot C_{\\text{clf-grad}})\\).\n- Space Complexity:\n  - Storage for attacked images and metrics: \\(O(N_{\\text{img}} K S)\\).\n  - Model parameters: encoders/VAEs/diffusion U-Net weights (hundreds of MB to GB depending on backbone). Intermediate activations during PGD scale with image resolution and encoder depth.\n- Parallelization:\n  - Embarrassingly parallel across images, attacks, and strengths; batchable metrics and detector inference. Diffusion steps parallelize over spatial tensors; PGD loops vectorize over batches.\n  - Distributed evaluation trivially partitions datasets and attack sets; reduction needed only for global quantiles and aggregation.\n- Hardware Compatibility:\n  - GPU acceleration recommended for diffusion/VAEs, LPIPS, CLIP-FID, and PGD (requires backprop through encoder/classifier). CPU sufficient for simple metrics (PSNR/SSIM/JPEG) and aggregation.\n  - Memory bandwidth critical for diffusion rinsing (multiple forward passes); ensure mixed precision and gradient checkpointing for PGD to fit larger batches.\n- Training vs. Inference:\n  - Most attacks are inference-only; surrogate-detector attacks add a brief training phase (few epochs on small datasets). Evaluation metrics computed in inference; identification uses the same pipeline with different performance definition.\n- Parameter Count:\n  - Pipeline adds no trainable parameters; attack algorithms reuse pretrained backbones (e.g., ResNet18 ~11M, CLIP ViT-B/32 ~88M, VAE/diffusion per chosen variant). Surrogate classifiers add one backbone with standard parameterization.\n- Numerical Stability:\n  - ROC thresholding at FPR=0.1% benefits from sufficient negative sample size; use score calibration and tie-breaking rules to avoid quantization artifacts. Quantile normalization mitigates outlier effects by clipping to [10%, 90%] range; invert “higher-is-better” metrics before aggregation to maintain monotonicity.\n  - PGD stability depends on step size \\(\\alpha\\) and budget \\(\\epsilon\\); use projected updates, gradient sign for l∞ and iteration caps to prevent divergence; diffusion rinsing uses fixed timesteps to avoid catastrophic quality collapse.\n- Scaling Behavior:\n  - Linear scaling in number of attacks K, strengths S, images \\(N_{\\text{img}}\\), and metrics J. Rinsing cost scales linearly in cycles N and timesteps t. PGD scales linearly in iterations \\(T_{\\text{PGD}}\\); transferability improves with grey-box encoders (higher effectiveness at similar cost).\n  - Aggregation across datasets adds a constant factor; global quantile estimation requires one pass over all attacked images (streaming quantile estimators can be used for very large N).",
        "IMPLEMENTATION_GUIDANCE": "Integration_Strategy:\n- Architect the evaluation as a modular pipeline with three interfaces so existing watermark systems can plug in without refactoring:\n  - EMBED(θG, m, prompt, config) → x: wraps the generator to produce watermarked images. Implement as a Python callable/class with a standard signature; for post-processing watermarks, θG produces x0, then EMBED applies the encoder to x0.\n  - DECODE(x) → m′ or score: returns decoded message and/or a scalar detection score (e.g., similarity or confidence). If the detector natively returns p-values, expose both score and p.\n  - VERIFYα(m′, m, α) → {0,1}: provide a reference implementation that evaluates the hypothesis test; for the benchmark’s TPR@0.1%FPR, parameterize Verify to instead compute scores and derive thresholds from a non-watermarked reference set.\n- Implement an Attack base class with apply(x, strength) -> x′ and metadata indicating category and strength scale. Derive subclasses:\n  - Distortion attacks (Rotation, ResizedCrop, Erase, Brightness, Contrast, GaussianBlur, GaussianNoise, JPEG) using torchvision transforms. Compose combos with a DistortionCombo class that applies a list of transforms at a shared relative strength s ∈ [0,1].\n  - Regeneration attacks using HuggingFace diffusers (Stable Diffusion v1.4 as the surrogate) and VAEs (bmshj2018, KL-VAEs). Create RegenDiff, RegenDiffPrompted, RegenVAE, RegenKLVAE, RinseNxDiff (N=2,4) classes. Provide MixedRegen variants that append VAE denoising after rinsing.\n  - Adversarial attacks: Embedding (PGD on an encoder f), and SurrogateDetector (train a ResNet18 binary classifier; then PGD-targeted attacks to flip labels).\n- Metrics module:\n  - Compute performance metrics: TPR@x%FPR for detection; multi-class accuracy for identification. Implement ROC-based thresholding: compute a scalar detector score per image (expose a “score_fn” wrapper for each watermark detector). Derive the threshold τ so that FPR on the non-watermarked reference set ≈ x% (e.g., 0.1%). Then compute TPR on watermarked set at τ.\n  - Compute image quality metrics: PSNR, SSIM, NMI, FID (Inception-V3), CLIP-FID (CLIP ViT-L/14), LPIPS (alex or vgg backbone), Aesthetics and Artifacts (ImageReward).\n- Normalization/Aggregation:\n  - For each quality metric q, compute global 10% and 90% quantiles over all attacked images (across all attack types, watermark methods, and datasets). Normalize to [0.1, 0.9] by linear mapping between these bounds, and clamp outside. Ensure orientation is “degradation ascending” (invert metrics where larger is better).\n  - Aggregate by averaging within four categories (similarities, distribution distances, perceptual, quality assessments), then average the four category means into Normalized Quality Degradation (NQD).\n- Performance-vs-Quality 2D plots:\n  - For each watermark/dataset/attack, plot TPR@0.1%FPR (y) vs NQD (x) across strengths. Provide unified plots that average across DiffusionDB, MS-COCO, and DALL·E3 by first computing NQD per dataset then averaging.\n- Leaderboards:\n  - Compute Q@0.95P and Q@0.7P: NQD where the attack curve crosses TPR=0.95 and 0.7 (use linear interpolation). If the curve does not cross, record “inf” or “-inf” as in the paper. Also compute Avg P and Avg Q (averages over strengths).\n  - Rank attacks per watermark by Q@0.95P, Q@0.7P, Avg P, Avg Q (with 0.01 tie buffers).\n- Code-level changes and compatibility:\n  - PyTorch-first implementation; use torchvision, diffusers, torchmetrics, lpips, scikit-image for SSIM, pytorch_msssim for MS-SSIM, open_clip for CLIP, and pytorch-fid for FID. Provide TensorFlow/JAX wrappers for detectors/encoders if needed but keep attacks in PyTorch for consistency.\n  - Add adapters for each watermark to expose score_fn(image)->float and decode(image)->message to standardize performance metric computation. For Tree-Ring, wrap DDIM inversion; for Stable Signature, wrap the trained extractor and compare decoded bits; for StegaStamp, wrap the decoder network.\n  - Migration path: if your current evaluation relies on AUROC or p-values, retain raw scores but switch to thresholding by fixing FPR at 0.1% using a dedicated non-watermarked reference set and compute TPR accordingly. Adapt plotting utilities to display NQD instead of raw per-metric curves.\n- Dependencies and kernels:\n  - Enable diffusers memory optimizations (xFormers attention, torch.cuda.amp for fp16/bfloat16).\n  - Preload encoders (CLIP ViT-L/14, ResNet18 pre-logit) and VAE weights (KL-VAEs f8/f16, SDXL-VAE) from official repos. Cache latents/embeddings to avoid recomputation across PGD iterations.\n\nParameter_Settings:\n- Distortion attacks (5 strength levels each):\n  - Rotation: 9°–45° clockwise (step ≈ 9°).\n  - Resized-Crop: crop area 10%–50% (uniform in area), then resize to original.\n  - Random Erasing: erase 5%–25% area, value=128 (gray), rectangle aspect 0.3–3.3.\n  - Brightness: multiply by 1.2–2.0 (clip to [0,255]).\n  - Contrast: factor 1.2–2.0.\n  - Gaussian Blur: kernel size 4–20 px (odd kernels only; choose 5 levels).\n  - Gaussian Noise: σ ∈ [0.02, 0.10] on [0,1]-normalized pixels.\n  - JPEG: quality 90→10 (decreasing).\n- Regeneration:\n  - Regen-Diff: surrogate SD v1.4, timesteps 40–200 (five evenly spaced levels; e.g., 40, 80, 120, 160, 200).\n  - Regen-DiffPrompted: same timesteps as above; require original prompt string.\n  - Rinse-2xDiff: two cycles, timesteps per cycle 20–100.\n  - Rinse-4xDiff: four cycles, timesteps per cycle 10–50.\n  - Regen-VAE (bmshj2018): “quality” levels 1–7 (lower is stronger compression).\n  - Regen-KLVAE: bottleneck sizes {4, 8, 16, 32}.\n  - Mixed regen (optional): Rinse-4xDiff-50 followed by VAE levels 1–7 or KL-VAEs as above.\n- Adversarial Embedding (untargeted PGD on f):\n  - Encoders: ResNet18 pre-logit, CLIP ViT-L/14 image encoder, KL-VAE encoder f8 (grey-box), KL-VAE f16, SDXL-VAE.\n  - ε ∈ {2/255, 4/255, 6/255, 8/255}, l∞-bounded; iterations=200; step size α=0.05×ε; random start in l∞ ball; project each step.\n  - Loss: maximize ||f(xadv) − f(x)||2. For VAEs, use latent z; for CLIP/ResNet, use pre-logit features.\n- Surrogate Detector training and attack:\n  - Architectures: ResNet18 binary classifier.\n  - Settings:\n    - AdvCls-UnWM&WM: train with 1500 watermarked + 1500 non-watermarked images (from the same generator) using prompts “A photo of a {ImageNet class}”.\n    - AdvCls-Real&WM: 7500 real ImageNet + 7500 watermarked generator images.\n    - AdvCls-WM1&WM2: 7500 images watermarked with message m1 + 7500 images with message m2 (two users), identical prompts.\n  - Training: 10 epochs, lr=1e-3 (SGD or Adam), batch size=128, cosine LR schedule optional; validate on 5000 images (balanced) until ≈99–100% val accuracy.\n  - PGD targeted attack: ε ∈ {2/255, 4/255, 6/255, 8/255}, iterations=50, step size α=0.01×ε, objective=CrossEntropy(f(xadv), ytarget), l∞-projection.\n- Performance metric computation:\n  - Detection TPR@0.1%FPR:\n    - Compute detector scores s on non-watermarked references (N≈5000). Set τ at the 99.9th percentile for the positive-class threshold in a score orientation consistent with your detector (flip if lower is “more watermarked”).\n    - Compute TPR as the fraction of watermarked images with s exceeding τ (orientation-aware).\n  - Identification: multi-class accuracy over K users. For large-K (e.g., 1e6) simulations, assign watermarked images all to user1, sample K−1 random messages for other users at evaluation, and assess correct attribution. Repeat over 10 random sets and average.\n- Quality metrics:\n  - PSNR, SSIM, NMI: implement with skimage or torchmetrics; 8-bit inputs normalized to [0,1] for consistency.\n  - FID: Inception-V3 pool3 features; use 2048-d embeddings and compute μ, Σ for attacked vs references; report standard FID.\n  - CLIP-FID: CLIP ViT-L/14 pooled features; compute μ, Σ analogously.\n  - LPIPS: backbone “alex” or “vgg”, default weights; normalize inputs.\n  - Aesthetics/Artifacts: ImageReward metrics; batch size 32 for throughput.\n- Normalization:\n  - For each metric q, compute q10 and q90 over the union of all attacked images. Map to s = 0.1 + 0.8 × (clip(q, q10, q90) − q10) / (q90 − q10). For “higher is better” metrics (PSNR, SSIM), reverse to represent degradation (use sdeg = 1 − s when needed), then ensure NQD increases with degradation.\n- Critical vs robust parameters:\n  - Critical: ε, iterations, and encoder choice in adversarial embedding; timesteps/cycles in rinsing; NQD quantile bounds (q10/q90) must be computed on the same global pool; threshold τ for FPR control.\n  - Robust: exact brightness/contrast ranges (stay within paper ranges), LPIPS backbone choice, optimizer choice in surrogate training (so long as val accuracy saturates).\n- Scale-dependent settings:\n  - Small models/datasets (<1000 images): consider fewer attack strengths (3 levels) and reduce PGD iterations to 50–100 to keep runtime manageable; recompute q10/q90 on your pool (not from paper).\n  - Large-scale (>5000 images per dataset): precompute embeddings (CLIP, Inception) and cache to disk (FP16) to accelerate FID/CLIP-FID; shard evaluation to avoid GPU OOM.\n\nApplication_Conditions:\n- Beneficial scenarios:\n  - You need a standardized, apples-to-apples robustness comparison across watermark types (post-processing vs in-processing), especially under realistic low-FPR constraints.\n  - You want to rank attack potency while maintaining image utility (quality-aware benchmarking).\n  - You operate in environments where false positives are costly (publishing platforms, moderation pipelines).\n- Hardware requirements:\n  - GPU recommended (≥12 GB VRAM) for regeneration attacks and PGD; CPU-only feasible for distortions and some metrics but regeneration will be slow.\n  - For rinsing and mixed regeneration at 512×512: 16–24 GB VRAM (A5000/RTX 4090) recommended; A100/H100 improves throughput and allows larger batches/FP16.\n  - Disk space: store 15K reference + attacks; expect 50–200 GB depending on caching.\n- Scale considerations:\n  - The unified NQD becomes reliable once you include ≥3 datasets and ≥20 attacks; with fewer, expect less stable quantile normalization—recompute q10/q90 each run.\n  - Identification benchmarks are more sensitive as K increases; attacks that only “disturb” latents will degrade identification faster than detection as K→1e6.\n- Task compatibility:\n  - Detection tasks: TPR@0.1%FPR metric preferred; identification tasks: multi-class accuracy; both use the same attack suite and NQD aggregation.\n  - Surrogate detector attacks are practical when an adversary can collect sufficient watermarked images; embedding attacks transfer best when encoders are similar to the victim’s.\n- Alternative comparisons:\n  - Choose WAVES (TPR@0.1%FPR + NQD) over AUROC or bit-accuracy when false positives are costly and you must quantify the performance-quality trade-off.\n  - Use regeneration attacks when the target uses public VAEs or decoders; prefer embedding/grey-box attacks when you can access the victim’s VAE encoder.\n- Resource constraints:\n  - If GPU-limited, prioritize distortions (cheap) and single regeneration (40–80 steps); skip rinsing and mixed regen; use ε=4/255, 100-step PGD for a representative adversarial embedding baseline.\n\nExpected_Outcomes:\n- Performance improvements and degradation ranges (representative, across paper’s three datasets):\n  - Tree-Ring:\n    - Grey-box embedding (KL-VAE f8): TPR@0.1%FPR can drop near 0.0 at ε=8/255 (NQD ≈ 0.25–0.30).\n    - Surrogate detector (UnWM&WM or WM1&WM2): TPR@0.1%FPR ≈ 0.49–0.35 at ε=8/255 (NQD ≈ 0.14–0.14), effective removal; spoofing non-watermarked images fails.\n    - Rinsing 2×–4×: TPR@0.1%FPR ≈ 0.51–0.44 (NQD ≈ 0.36–0.47); 4× with high steps will visibly degrade images.\n    - Distortions: mostly robust at reasonable strengths; rotation/crops can reduce TPR with higher NQD.\n  - Stable Signature:\n    - Regeneration (Diff/Regen-DiffP): destructive (Avg P ≈ 0.0–0.001) at modest NQD ≈ 0.30–0.33; KL-VAE regeneration also strong.\n    - Embedding/surrogate attacks: limited transfer; TPR typically remains ≥0.98.\n  - StegaStamp:\n    - Very robust; only diffusive regeneration impacts TPR (Avg P ≈ 0.91–0.94) at NQD ≈ 0.33–0.48; distortions degrade quality more than performance.\n- Timeline expectations:\n  - Distortions/regeneration immediate effects observable on pre-trained detectors.\n  - Surrogate training requires ~30–60 minutes on a single GPU with 15K samples; PGD attacks then produce immediate TPR drops.\n- Trade-offs:\n  - Regeneration/rinsing: strong performance degradation but greater image quality loss (NQD ↑).\n  - Embedding (grey-box): strong TPR drop with minimal visual changes (NQD ≈ 0.23–0.30), but requires encoder access or close surrogate.\n  - Surrogate attacks: effective for removal, poor for spoofing; transferability issues if surrogate relies on features different from the true detector.\n- Benchmarks vs baselines:\n  - Expect unified rankings similar to Table 3: AdvEmbG-KLVAE8 and AdvCls-WM1&WM2 rank top-5 for Tree-Ring; Regen-Diff/Regen-DiffP rank top-1 for Stable Signature; Distortions rarely rank top for StegaStamp in average performance drop.\n- Failure modes:\n  - Transferability gaps: AdvCls-Real&WM often fails (classifier learns “real vs generated” instead of watermark features).\n  - Prompted regeneration ineffective if prompts are unavailable or misaligned.\n  - NQD instability if quantiles computed on too few attacked samples or biased datasets—results become sensitive to outliers.\n  - Incorrect threshold orientation for TPR@FPR yields inverted metrics; verify with sanity checks.\n- Debugging indicators:\n  - Performance-vs-quality curves should be monotone: stronger attacks → higher NQD and lower TPR; if not, inspect normalization orientation per metric.\n  - Surrogate detectors should reach high validation accuracy (>99%); if adversarial examples don’t transfer, visualize latent differences (Tree-Ring Fourier domain rings) to confirm disturbance of watermark-bearing latents.\n  - For regeneration, CLIP-FID and PSNR should worsen with more timesteps; if they don’t, verify model config (SD v1.4 vs v2.1) and denoising schedule.\n- Hardware-specific outcomes:\n  - A100/H100 with fp16/bfloat16: 2–3× throughput on regeneration; rinsing 4× feasible with batch size ≥4 at 512×512.\n  - RTX 3090/4090 (24 GB): comfortable for all attacks; xFormers attention recommended; PGD at ε=8/255, 200 steps runs in <1s per image (CLIP encoder).\n  - ≤12 GB GPUs: reduce timesteps, avoid 4× rinsing; run metrics on CPU to free GPU memory.\n\nQuality Requirements:\n- Troubleshooting:\n  - If TPR@0.1%FPR remains high for attacks expected to be strong (e.g., Regen-Diff on Stable Signature), confirm FPR threshold τ is calibrated using non-watermarked references from the same distribution; re-check score orientation.\n  - If NQD values are clustered near 0.1 or 0.9, recompute q10/q90 on a broader pool (add more attacked samples) and ensure per-metric clamping.\n  - PGD non-convergence: lower step size (α=0.02×ε), increase iterations to 300, and verify gradients on encoders (turn off no-grad layers).\n- Limitations/risks:\n  - The benchmark exposes vulnerabilities; do not deploy attacks operationally without ethical review and consent.\n  - Public VAEs increase susceptibility to grey-box embedding; proprietary VAEs reduce transferability but not necessarily eliminate it.\n- Hardware/software/scale dependencies:\n  - Regeneration depends on diffusers and xFormers; ensure CUDA/cuDNN versions match PyTorch build. For CLIP-FID, cache features to reduce CPU/GPU thrash.\n  - Scale-dependent quantiles: recompute q10/q90 per run if your dataset/attack pool differs from the reference; do not reuse paper quantiles blindly.\n- Validation procedures:\n  - Reproduce a subset of paper plots: for Tree-Ring on DiffusionDB, plot TPR@0.1%FPR vs PSNR for Regen-Diff and AdvEmbG-KLVAE8; confirm qualitative shapes (Regen-Diff destructive, embedding minimal NQD with strong TPR drop).\n  - Cross-check rankings: compute Q@0.95P and Avg P for Stable Signature; Regen-Diff/Regen-DiffP should rank top; embedding attacks should show inf at Q@0.95P (i.e., curves never cross).\n  - Sanity-check identification: with K=1e6, observe greater vulnerability (accuracy declines faster) vs detection for the same attacks."
    }
]