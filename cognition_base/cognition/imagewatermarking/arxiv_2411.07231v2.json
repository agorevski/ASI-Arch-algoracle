[
    {
        "DESIGN_INSIGHT": "### DESIGN_INSIGHT_HIGH: [Per-Pixel Joint Detection–Decoding for Watermark Segmentation and Multi-Message Recovery]\n- Replaces the traditional global watermark detector/decoder with a pixel-wise predictor that outputs both presence and message bits at each location. This reframing turns watermarking into a segmentation problem, enabling localization, robustness to splicing/inpainting, and extraction of multiple distinct messages from disjoint regions.\n- Key mechanism: the extractor \\( \\mathrm{ext}_{\\theta^*} \\) produces a tensor \\( y = [y^{\\mathrm{det}}, y^{\\mathrm{dec}}] \\in [0,1]^{(1+\\text{nbits}) \\times h \\times w} \\), where \\( y^{\\mathrm{det}}_{i} \\) is a per-pixel watermark presence score and \\( y^{\\mathrm{dec}}_{i,k} \\) is the per-pixel soft prediction of bit \\( k \\). Image-level detection is obtained by thresholding and averaging detection masks:\n  \\[\n  s_{\\mathrm{det}} = \\frac{1}{hw} \\sum_{i=1}^{hw} \\mathbf{1}\\{ y^{\\mathrm{det}}_i > \\tau \\},\n  \\]\n  while single-message decoding is a detection-weighted vote over pixels:\n  \\[\n  \\hat m_k = \\mathbf{1}\\Big\\{ \\frac{1}{\\sum_i \\mathbf{1}\\{y^{\\mathrm{det}}_i>\\tau\\}} \\sum_{i=1}^{hw} \\mathbf{1}\\{y^{\\mathrm{det}}_i>\\tau\\}\\, y^{\\mathrm{dec}}_{i,k} > 0.5 \\Big\\}.\n  \\]\n  For multi-message decoding, local hard messages \\( \\tilde m_{i,k}=\\mathbf{1}\\{y^{\\mathrm{dec}}_{i,k}>0.5\\} \\) are clustered over pixels with \\( y^{\\mathrm{det}}_i>\\tau \\) using DBSCAN on a Hamming distance; cluster centers (chosen among data points) directly yield binary messages.\n- Fundamental difference from prior work: prior robust watermarking takes one global decision (or reserves bits for detection), causing the watermark signal to “fade” as the marked area shrinks and entangling detection with decoding assumptions (independence/equiprobability). Here, detection is a dedicated per-pixel head decoupled from decoding, which preserves detectability under small marked areas and arbitrary splicing, and enables principled multi-message recovery without knowing the number of messages a priori.\n- Computationally, extraction scales as \\( O(hw \\cdot \\text{nbits}) \\) and supports post-processing (global detection/decoding or clustering) without changing the network, while maintaining user-tunable FPR via the threshold \\( \\tau \\) on pixel-level detection.\n\n\n### DESIGN_INSIGHT_MEDIUM: [Robustness-First Pretraining with Geometry-Consistent Masks, followed by JND-Guided Post-Training for Imperceptibility and Multi-Watermarks]\n- Modifies standard end-to-end embedder/extractor training by splitting it into two stable phases: (1) a robustness pretraining that enforces localization and decoding under strong edits; (2) a post-training phase that adds perceptual attenuation and explicit multi-mask supervision to support multiple watermarks per image.\n- Phase 1 mechanism: given an image \\(x\\) and message \\(m\\), an embedder produces \\(x_m\\). Random binary masks \\(r_{\\mathrm{mask}}\\) (rectangles, irregular strokes, semantic segments, full image) create splices \\(x_{\\mathrm{masked}}= r_{\\mathrm{mask}} \\odot x_m + (1-r_{\\mathrm{mask}})\\odot x\\). Realistic transformations \\(T\\) (geometric + valuemetric) are applied to both image and mask to obtain geometry-consistent supervision \\(y^{\\mathrm{det},\\star}\\). The extractor minimizes\n  \\[\n  \\mathcal{L}_{\\mathrm{det}} = -\\frac{1}{hw} \\sum_{i=1}^{hw} \\left[y^{\\mathrm{det},\\star}_i \\log y^{\\mathrm{det}}_i + (1-y^{\\mathrm{det},\\star}_i)\\log(1-y^{\\mathrm{det}}_i)\\right],\n  \\]\n  \\[\n  \\mathcal{L}_{\\mathrm{dec}} = -\\frac{1}{\\text{nbits} \\cdot \\sum_i y^{\\mathrm{det},\\star}_i} \\sum_{i: y^{\\mathrm{det},\\star}_i=1} \\sum_{k=1}^{\\text{nbits}} \\left[m_k \\log y^{\\mathrm{dec}}_{i,k} + (1-m_k)\\log(1-y^{\\mathrm{dec}}_{i,k})\\right],\n  \\]\n  with total loss \\( \\mathcal{L} = \\lambda_{\\mathrm{det}}\\mathcal{L}_{\\mathrm{det}} + \\lambda_{\\mathrm{dec}}\\mathcal{L}_{\\mathrm{dec}} \\). This trains the extractor on both watermarked and non-watermarked pixels—unlike prior work—improving detection robustness and calibration.\n- Phase 2 mechanism: a Just-Noticeable-Difference (JND) heatmap \\( \\mathrm{JND}(x) \\) modulates the watermark at embedding time, yielding\n  \\[\n  x_m = x + \\alpha_{\\mathrm{JND}} \\, \\mathrm{JND}(x) \\odot \\delta_\\theta(x,m),\n  \\]\n  and training continues so robustness is recovered under perceptual attenuation. To enable multiple watermarks, the image is partitioned into 1–3 disjoint masks with distinct messages; detection uses the union mask as \\(y^{\\mathrm{det},\\star}\\) and decoding loss sums over per-mask messages. This prevents the single-message collapse (constant message across regions) and teaches separability of local payloads.\n- Difference from prior adversarial/perceptual schemes (e.g., GAN-based contradictory losses): the two-stage curriculum isolates robustness learning from imperceptibility, avoids unstable min–max training, and explicitly supervises geometry-consistent localization and multi-message separation—capabilities not handled by prior global-decoder pipelines.\n\n\n### DESIGN_INSIGHT_MEDIUM: [Latent-Space Lookup-Table Embedder with Resolution-Agnostic Watermark Signal and ViT Pixel-Decoder Extractor]\n- Replaces heavy/high-resolution embedding with a compact latent-space mechanism and a fixed-resolution pipeline. The embedder is a VAE-style autoencoder that injects a message embedding in the encoder’s latent space and decodes an additive watermark signal; the extractor is a ViT encoder plus a pixel decoder that outputs per-pixel detection and bits.\n- Embedder mechanism: an image \\(x \\in \\mathbb{R}^{3\\times h\\times w}\\) is mapped by \\( \\mathrm{enc}_\\theta \\) to \\( z \\in \\mathbb{R}^{d_z \\times h' \\times w'} \\) (downsampling factor \\( f \\)). A binary message \\( m \\in \\{0,1\\}^{\\text{nbits}} \\) indexes a learnable lookup table \\( T_\\theta \\in \\mathbb{R}^{\\text{nbits}\\times 2\\times d_{\\mathrm{msg}}} \\); per-bit embeddings \\( T_\\theta(k, m_k, :) \\) are averaged and broadcast to \\( z_{\\mathrm{msg}} \\in \\mathbb{R}^{d_{\\mathrm{msg}}\\times h' \\times w'} \\). Concatenating \\([z, z_{\\mathrm{msg}}]\\) and decoding yields a bounded watermark signal \\( \\delta_\\theta(x,m) \\in [-1,1]^{3\\times h \\times w} \\), added as\n  \\[\n  x_m = x + \\alpha \\, \\delta_\\theta(x,m) \\quad \\text{or} \\quad x_m = x + \\alpha_{\\mathrm{JND}} \\, \\mathrm{JND}(x)\\odot \\delta_\\theta(x,m).\n  \\]\n  Averaging over bits provides a stable, position-aware message conditioning without spatially tiling key vectors, and constrains the decoder to distribute payload globally while remaining localizable by the extractor.\n- Resolution-agnostic embedding: for arbitrary input size \\(H\\times W\\), the image is anisotropically resized to \\(h\\times w\\) for computing \\( \\delta_\\theta \\); the signal is then bilinearly upsampled back to \\(H\\times W\\) and added to the original image. The extractor always operates on resized \\(h\\times w\\) inputs. This keeps training and inference at fixed low resolution while supporting high-resolution images with negligible overhead—contrasting with prior methods that require high-res training, tiling, or heavy per-resolution re-training.\n- Extractor mechanism: a ViT encoder processes patches (e.g., \\(16\\times 16\\)) into latent tokens; a lightweight pixel decoder upsamples to \\(h\\times w\\), and a final linear head predicts \\((1+\\text{nbits})\\) channels per pixel. Compared to CNN-only decoders, the ViT backbone aggregates long-range context crucial for recovering consistent bit vectors across sparse or transformed watermarked regions, improving robustness under geometric changes.",
        "EXPERIMENTAL_TRIGGER_PATTERNS": "**Task_Performance_Signatures:**\n- Image watermark detection/decoding on COCO (256×256 to mid-res via resizing)\n  - Expect ≥99.5–100% TPR and ≈100% bit accuracy under no transform at FPR ≈0.04%.\n  - Geometric robustness: expect ≈99.3% TPR and ≈91–92% bit accuracy averaged over flip, crop (up to 50%), rotation (±10°), perspective (0.1–0.5). Typical per-transform ranges:\n    - Horizontal flip: TPR ≈100%, bit acc ≈100%.\n    - Crop 50% + resize: TPR ≈99.6%, bit acc ≈96%.\n    - Perspective 0.5: TPR ≈100%, bit acc ≈96%.\n  - Valuemetric robustness: expect ≈100% TPR and ≈99.9% bit accuracy across brightness/contrast/saturation/hue, median and Gaussian filtering, JPEG 80–50 (bit acc ≥99–100% except JPEG50 ≈99%).\n  - Inpainting (LaMa): expect ≈98% TPR and ≈99% bit accuracy; detection masks show non-watermarked inpainted regions.\n  - Splicing (10% watermarked area): expect ≈100% TPR and ≈95% bit accuracy; ≥+10–40 point TPR and ≥+20–40 point bit-accuracy gains vs classical baselines.\n- Image watermark detection/decoding on DIV2K (high resolution, inference via low-res resize)\n  - Expect generalization without high-res training: None/valuemetric/inpainting TPR ≈100%, bit acc ≈99.8–99.9; geometric TPR ≈96% with bit acc ≈89%; splicing TPR ≈99.5% with bit acc ≈94%.\n- Localization (pixel-level detection on COCO, controlled rectangular masks)\n  - mIoU ≥0.9 when watermarked area is 10–80% without downstream transforms.\n  - After 25% upper-left crop + resize: mIoU remains ≥0.85 for 10–80% area.\n  - Bit recovery from small areas: expect ≈31/32 bits from only 10% area at 256×256; ≈25/32 bits when area equals 2.5% of full image (i.e., 10% of a 25% crop).\n- Multiple-watermark extraction (up to 5 disjoint 10% regions, 32 bits each, COCO)\n  - After second-stage training (JND + multi-mask): DBSCAN cluster count ≈ number of embedded watermarks (e.g., detected clusters ≈5 when 5 are present).\n  - Bit accuracy per cluster ≈0.94–0.98 for 1–5 concurrent watermarks; mIoU of detection ≈0.98 across counts.\n  - Under flip + contrast 1.5: average mIoU ≈0.85, mean bit accuracy across 5 messages ≥0.95; with additional JPEG-80, expect degradation to ≈0.60 bit accuracy and reduced cluster count.\n- Imperceptibility (COCO and DIV2K)\n  - Post-training perceptual quality: PSNR ≈38.3–38.8 dB, SSIM ≈0.99, LPIPS ≈0.03–0.04; competitive with strong baselines at similar PSNR.\n  - Stage-1 to Stage-2 shift: PSNR improves from ≈24–25 dB to ≈36–38 dB after JND fine-tuning with negligible robustness loss.\n- Comparative robustness advantage\n  - Expect dominant gains vs baselines on splicing/inpainting:\n    - COCO splicing: WAM TPR 100% / bit acc 95% vs TrustMark 83% / 57%, FNNS 39% / 89%.\n    - COCO inpainting: WAM 98% / 99% vs TrustMark 39% / 83%, FNNS 89% / 97%.\n  - Performance remains competitive or superior on most geometric/valuemetric categories while training exclusively at low resolution.\n- Operating thresholds and false positives\n  - With image-level score sdet threshold ≈0.07, FPR ≈0.04% on COCO val; DIV2K FPR ≈0%.\n  - Pixel detection threshold τ tuned to desired per-pixel FPR; τ ≈0.5 used for multi-watermark clustering.\n- High-resolution inference efficiency\n  - Expect similar detection/decoding/localization performance at high resolution due to resizing-based watermark extraction; no large-model finetuning required.\n- Degradation bounds and known weak spots\n  - Diffusion purification or strong VAE compression (PSNR ≈25 dB) reduces bit accuracy to ≈50–55%.\n  - Very large watermarked area (≈95%) can bias detector toward “all watermarked.”\n  - Multi-watermark extraction degrades under combined JPEG + strong valuemetric changes (cluster counts drop, bit acc ≈0.6).\n- Non-applicable NLP/comprehension benchmarks (control signals)\n  - Language modeling (lambada_openai, wikitext, ptb), reading comprehension (squad_completion, squad_v2, narrativeqa), commonsense reasoning (hellaswag, piqa, social_iqa, commonsenseqa), factual QA (arc_easy, arc_challenge, boolq, openbookqa), context resolution (winogrande, winograd): expect unchanged performance (within ±0.2% absolute) as tasks are out-of-domain; any deviation indicates evaluation confounds.\n  - Other tasks swde, fda: not directly applicable; robustness gains are instead reflected through heavy augmentation suites and splicing/inpainting protocols (expect “similar accuracy” principle: negligible quality drop vs no-aug while maintaining robustness to strong aug).\n\n**Architectural_Symptoms:**\n- Training characteristics\n  - Two-stage convergence pattern:\n    - Stage 1 (robustness + localization only): rapid decrease of detection and decoding BCE; near-perfect localization/decoding under strong augmentations; watermark perceptibility high (PSNR ≈24–25 dB).\n    - Stage 2 (JND + multi-messages): initial slight robustness dip upon enabling JND, then recovery to Stage-1 robustness within a few epochs; perceptual metrics improve to PSNR ≈36–38 dB. Decoding ceases to collapse to a single message across regions; per-region message entropy increases.\n  - Stability indicators: smooth, low-variance loss curves; no adversarial objectives; no NaN occurrences; stable when adding multi-mask supervision (prevents “constant message” failure).\n  - Detection calibration: pixel-level FPR smoothly controllable via τ; image-level FPR settable via sdet threshold; empirically ~10^-4–10^-3 feasible without sacrificing TPR on COCO.\n- Runtime/memory behaviors\n  - Resolution handling: constant-time embed/extract at fixed 256×256 regardless of original image size (anisotropic resize in, bilinear-resized watermark out); inference memory remains effectively constant w.r.t. original resolution.\n  - Model sizes: embedder ≈1.1M params (fast user-side embedding), extractor ≈96M (ViT-B + pixel decoder). Embedding throughput high; extraction dominated by ViT-B but single forward pass per image.\n  - Training efficiency: low-res only (256×256) yields substantial compute savings vs high-res training while retaining high-res inference performance; reported full training in ≈30 GPU-days for one run (batch size 16/GPU on 8×V100).\n  - Clustering overhead: DBSCAN over ≤256×256 decoded pixels; runtime negligible relative to network forward; memory footprint small; sensitivity to ε, minsamples hyperparameters observable in cluster counts.\n- Profiling signatures\n  - Perceptual heatmap (JND) effect: difference images show distortion concentrated in textured/high-CM regions and blue channel; PSNR jump from ≈25 to ≈38 dB after Stage 2.\n  - Detector masks: crisp binary-like maps that zero out inpainted regions; under cropping, predicted mask aligns with transformed ground-truth (mIoU ≥0.85 at 10–80% coverage).\n  - Multi-watermark operation: DBSCAN cluster count ≈ true number of watermarked regions; cluster centroids correspond to binary messages; under JPEG+valuemetric stacks, cluster count and bit accuracy jointly drop (diagnostic of robustness boundary).\n  - Thresholded global detection: sdet distributions exhibit clear separation between watermarked and clean images; at sdet≈0.07, FPR≈0.04% with TPR≈100% (COCO).\n- Scaling properties\n  - Robustness/stability maintained when increasing number of watermarks during inference beyond training (trained on up to 3; generalizes to 5 with minor degradation).\n  - Embedding strength control (αJND) provides tunable robustness–imperceptibility trade-off: increasing αJND from 2.0 to 3.0 yields bit accuracy gains under harsh combos (+3–4 points) at the cost of ≈3–4 dB PSNR loss; lowering αJND reduces visible artifacts but degrades robustness.\n- Potential neutral/negative effects to monitor\n  - Visual artifacts: repetitive patterns may appear in large bright/texture regions despite JND; indicates room for improved HVS modeling or watermark regularizers.\n  - Over-detection when >90–95% area watermarked (bias toward full-mask predictions).\n  - Robustness limits: diffusion-based purification or strong VAE compression (PSNR ≈25 dB) reduces bit accuracy to ≈50–55%; multi-watermark clustering fails under combined JPEG+valuemetric stress (average bit acc ≈0.60).\n- Hardware utilization\n  - High GPU utilization during extraction (ViT-B); no OOM at batch sizes typical for 256×256 and ViT-B; memory growth linear in batch size and independent of original image size due to fixed-resolution processing.",
        "BACKGROUND": "Title: WATERMARK ANYTHING WITH LOCALIZED MESSAGES\n\nHistorical Technical Context:\nFor nearly three decades, image watermarking progressed from traditional signal processing to deep learning. Early “spatial-domain” and “transform-domain” methods embedded spread-spectrum signals into pixels or transform coefficients (e.g., DCT, DWT, DFT), optionally guided by human visual system (HVS) models to improve imperceptibility. As robustness requirements evolved, learning-based approaches emerged. Two deep-learning families became dominant: (1) post-hoc, end-to-end embedder/extractor architectures (e.g., HiDDeN) that learn to add a perturbation and recover bits after augmentations; and (2) “fixed-encoder” schemes (e.g., SSL watermark, FNNS) that optimize image pixels to match a pre-trained network’s representation keyed to a message. In parallel, generative-model watermarking arose for diffusion/GANs, enabling generation-time watermarking (e.g., Stable Signature, Tree-Ring) with strong robustness to certain distortions but limited post-hoc applicability.\n\nDespite advances in segmentation (from FCN to ViT-based decoders and promptable systems like SAM), watermarking remained mostly a global decision task: detectors predict a single “watermarked or not,” and decoders assume globally embedded messages. This left splicing, inpainting, and partial edits as failure modes: when only a small area is watermarked, global detectors either miss the watermark or unfairly flag the entire image. Semi-fragile tamper localization methods existed, but balancing fragility (to edits) with robustness (to benign transforms) proved difficult, and they typically did not robustly decode multi-bit payloads.\n\nMotivated by regulations on AI content provenance and common real-world manipulations (copy-move, splicing, inpainting/outpainting), the field needs localized watermarking: detect where the watermark is, and decode distinct messages from different regions. AudioSeal demonstrated localized watermarking in audio but did not support multiple concurrent messages. EditGuard localized tampering via fragile+robust embedding but was brittle to geometric changes and produced a single global message. This paper reframes image watermarking as a segmentation-and-decoding problem and introduces architectures and training protocols explicitly designed for localization and multi-message extraction.\n\nTechnical Limitations:\n- Lack of localization and multi-message capability: Prior image watermarking produced one global decision/message. Under splicing/inpainting, signal strength scaled with area, causing failure when the watermarked region was small. No standard way existed to segment watermarked pixels or decode multiple distinct payloads.\n- Geometric robustness gaps: Many robust methods degrade under crops, perspective changes, or rotations. Operating purely at image-level yields accuracy drops when watermarked regions are partially removed or spatially transformed.\n- Detection-decoding coupling and unreliable FPR: Combining detection by reserving ndet bits assumes i.i.d. decoded bits from non-watermarked images (yielding a nominal FPR = 2^{-ndet}). Empirically, decoded bits are neither independent nor equiprobable, leading to miscalibrated false positives/negatives.\n- Adversarial/perceptual training instability: Prior methods that jointly optimize imperceptibility (perceptual losses) and robustness (adversarial/attack models) can be unstable or require delicate tuning. Training high-resolution models exacerbates instability and compute/memory needs.\n- Scaling and compute constraints: Pixel-level decoding over high-resolution images would naively scale as O(HW·nbits) in both compute and memory. Training end-to-end at high resolution is expensive; many methods thus limit resolution or suffer generalization issues when upscaled.\n- Payload–robustness trade-off: Methods with high payloads (>100 bits) often lack robustness to crops/splicing/outpainting; highly robust systems often limit payload size and provide no localization, hindering practical multi-source provenance.\n\nPaper Concepts:\n- Watermark Anything Model (WAM): A post-hoc embedder/extractor pair (embθ, extθ) trained jointly for localized detection and pixel-level decoding. The embedder predicts an additive signal δθ(x, m) and produces\n  xm = x + α · δθ(x, m)\n  and, after post-training with an HVS prior,\n  xm = x + αJND · JND(x) ⊙ δθ(x, m),\n  where α, αJND control distortion strength and JND(x) is a perceptual heatmap. The extractor outputs per-pixel detection and bit logits. Architecturally: a lightweight VAE-like autoencoder for embedding (with a message lookup table Tθ ∈ R^{nbits×2×dmsg}) and a ViT-encoder + pixel-decoder (SETR/SAM-style) for extraction.\n\n- Pixel-level detection and image-level score: The extractor outputs y = [ydet, ydec] with ydet ∈ [0,1]^{1×h×w} and ydec ∈ [0,1]^{nbits×h×w}. A pixel i is watermarked if ydet_i > τ. The image-level detection score is\n  s_{det} := (1/(h·w)) ∑_{i=1}^{h·w} 1{ ydet_i > τ } ∈ [0,1],\n  allowing decisions based on the fraction of detected pixels (user-chosen threshold).\n\n- Pixel-level decoding and global majority vote: For a single message, decode bit k via a detection-weighted vote:\n  ˆm_k = 1 if ( (1/∑_i 1{ydet_i>τ}) ∑_{i=1}^{h·w} 1{ydet_i>τ} · ydec_{i,k} ) > 0.5; else 0.\n  This ties decoding to localized detection, naturally down-weighting unwatermarked pixels.\n\n- Multi-message clustering with DBSCAN: For multiple watermarked regions, first threshold per-pixel bits: \\tilde{m}_{i,k} = 1{ ydec_{i,k} > 0.5 } on pixels with ydet_i > τ. Cluster the set { \\tilde{m}_i } using DBSCAN with Hamming distance, parameters (ε, minsamples). DBSCAN returns cluster labels and centroids chosen from input binary words, enabling unknown-number-of-messages decoding and spatially coherent grouping.\n\n- Two-stage training with splicing-aware objectives: Stage 1 (robustness/localization) minimizes\n  ℓ(θ) = λ_{det} ℓ_{det}(θ) + λ_{dec} ℓ_{dec}(θ),\n  with pixel-wise BCE detection loss\n  ℓ_{det} = − (1/(h·w)) ∑_i [ ydet,i^⋆ log ydet_i + (1−ydet,i^⋆) log(1−ydet_i) ],\n  and pixel/bit BCE decoding loss only over watermarked pixels:\n  ℓ_{dec} = − (1/(nbits · ∑_i ydet,i^⋆)) ∑_i ydet,i^⋆ ∑_{k=1}^{nbits} [ m_k log ydec_{i,k} + (1−m_k) log(1−ydec_{i,k}) ].\n  Training samples use splicing masks rmask and downstream transforms; the augmented input is\n  x_masked = rmask ⊙ xm + (1−rmask) ⊙ x.\n  Stage 2 (imperceptibility/multi-message) fine-tunes with JND weighting and multiple disjoint masks/messages per image (1–3 during training), summing decoding losses across messages and using the union mask for detection.\n\n- Just-Noticeable-Difference (JND) heatmap: A pixel-wise HVS model combining luminance adaptation LA(x) and contrast masking CM(x):\n  LA and CM are computed from local luminance and gradients; the combined heatmap\n  H(x) = LA(x) + CM(x) − γ · min(LA(x), CM(x)),\n  then channel-scaled (e.g., stronger on blue) and applied multiplicatively to δθ(x, m). This hides energy where the HVS is less sensitive, improving imperceptibility without adversarial/perceptual instabilities.\n\nExperimental Context:\nThe paper evaluates watermarking as a joint detection–localization–decoding problem with emphasis on robustness to real-world manipulations (geometric, valuemetric, splicing, inpainting) and on imperceptibility. Imperceptibility is measured with PSNR, SSIM, and LPIPS on COCO (low/mid-res) and DIV2k (high-res). Robustness is assessed via True Positive Rate (TPR), False Positive Rate (FPR), and Bit Accuracy under controlled augmentations (crop, resize, rotation, perspective, flips; brightness/contrast/saturation/hue; Gaussian/median blur; JPEG). Localization quality is measured by mIoU against known splicing masks. Multi-message performance is evaluated by DBSCAN’s discovered clusters and per-cluster bit accuracy when up to five 10% regions (non-overlapping) carry distinct 32-bit payloads.\n\nKey configuration details: training resolution h×w=256×256 with latent downsampling f=8; nbits=32; embedder ≈1.1M params; extractor ViT-Base ≈96M params; Stage-1 training for robustness/localization (AdamW, 300 epochs, strong augmentations, α=0.3); Stage-2 fine-tuning for imperceptibility and multi-message (200 epochs, αJND=2, up to 3 messages/masks). For high-resolution images, a resize–embed–resize procedure preserves training-time operating conditions while enabling inference on original resolutions. For multi-message extraction, DBSCAN uses ε=1 and minsamples=1000 (≈2% of pixels) with τ=0.5.\n\nOverall findings: With comparable imperceptibility to baselines (e.g., COCO PSNR≈38–40 dB, SSIM≈0.99, LPIPS≈0.04), WAM attains high robustness across standard distortions and uniquely strong performance on splicing and inpainting. On COCO, WAM achieves TPR≈100% and Bit Acc≈95.3% under splicing (only 10% area watermarked) and remains competitive or superior under geometric/valuemetric noise. On DIV2k, similar trends hold, despite training only at 256×256. For localization, WAM delivers high mIoU across watermarked-area sizes and remains robust after crop+resize; for multi-message decoding, after fine-tuning it accurately extracts up to five distinct 32-bit messages placed in 10% regions (≈0.98 bit accuracy and ≈0.98 mIoU), surpassing methods that provide a single global decision and message. The evaluation philosophy prioritizes robustness and locality over maximal payload, demonstrates generalization to high-resolution use via resizing, and highlights a practical detection/decoding separation for calibrated decisions at user-defined operating points.",
        "ALGORITHMIC_INNOVATION": "Core_Algorithm:\n- Replace global, image-level watermark detection/decoding with a pixel-wise formulation: the extractor outputs per-pixel detection score and per-pixel nbits-bit message logits, turning watermarking into a segmentation-plus-decoding task.\n- Embedder is an autoencoder-like network: encode image to a latent, concatenate an averaged lookup-table embedding of the binary message, decode an additive residual watermark δ(x, m) and add it to the input. During post-training, modulate δ with a Just-Noticeable-Difference (JND) heatmap before addition.\n- Training is two-stage: (1) robustness pre-training via random splicing masks and heavy geometric/valuemetric transformations, optimizing per-pixel detection and per-pixel decoding losses; (2) imperceptibility and multi-watermark post-training with JND attenuation and multiple disjoint masks/messages per image.\n- At inference, high-resolution images are handled by resizing to a fixed h×w for embedding/extraction and bilinearly resizing the residual; multiple localized messages are recovered by clustering hard-decoded per-pixel bit vectors with DBSCAN, requiring no prior on the number of watermarks.\n\nKey_Mechanism:\n- The core insight is to decouple watermark presence from its pixel footprint by predicting a message at every pixel, making the global decision a robust aggregation over localized evidence. Splicing-based data augmentation enforces invariance to transformations and teaches the extractor to localize and decode under partial visibility.\n- JND-weighted residuals concentrate distortion where the human visual system is insensitive, restoring imperceptibility without destabilizing training. Multi-mask supervision forces spatial consistency of distinct messages and enables generalization to multiple, small watermarked regions.\n\nMathematical_Formulation:\n- Embedder and JND attenuation:\n  - Encoder: z = enc_θ(x) ∈ ℝ^{d_z×h'×w'}, downsample factor f = h/h' = w/w'.\n  - Message LUT: T_θ ∈ ℝ^{nbits×2×d_msg}, with per-bit embeddings e_k = T_θ(k, m_k, :) ∈ ℝ^{d_msg}. Average and tile:\n    \\[\n    z_{\\text{msg}} = \\mathrm{repeat}\\Big(\\frac{1}{\\text{nbits}}\\sum_{k=1}^{\\text{nbits}} e_k\\Big)\\in \\mathbb{R}^{d_{\\text{msg}}\\times h'\\times w'}.\n    \\]\n  - Residual: δ_θ(x,m) = \\tanh\\big(\\mathrm{dec}_θ([z; z_{\\text{msg}}])\\big) ∈ [-1,1]^{3×h×w}.\n  - Watermarked image:\n    \\[\n    x_m = x + \\alpha \\,\\delta_θ(x,m)\\quad\\text{(pre-training)},\\qquad\n    x_m = x + \\alpha_{\\text{JND}}\\, \\mathrm{JND}(x)\\odot \\delta_θ(x,m)\\quad\\text{(post-training)}.\n    \\]\n- Augmenter and ground truth mask:\n  - Splice and transform: given random mask r_{\\text{mask}}\\in\\{0,1\\}^{h×w},\n    \\[\n    x_{\\text{masked}} = r_{\\text{mask}}\\odot x_m + (1-r_{\\text{mask}})\\odot x,\\quad\n    \\tilde{x} = \\mathcal{T}(x_{\\text{masked}}),\\quad y^{\\star}_{\\text{det}}=\\mathcal{T}(r_{\\text{mask}}).\n    \\]\n- Extractor outputs:\n  \\[\n  y = \\mathrm{ext}_{\\theta^*}(\\tilde{x}) \\in [0,1]^{(1+\\text{nbits})×h×w},\\quad\n  y_{\\text{det}}\\in[0,1]^{h×w},\\; y_{\\text{dec}}\\in[0,1]^{\\text{nbits}×h×w}.\n  \\]\n- Detection and decoding:\n  - Pixel detected if y^i_{\\text{det}} > \\tau. Image-level soft score:\n    \\[\n    s_{\\text{det}}=\\frac{1}{hw}\\sum_{i=1}^{hw}\\mathbf{1}\\{y^i_{\\text{det}}>\\tau\\}.\n    \\]\n  - Single-message decoding (weighted by detection mask):\n    \\[\n    \\hat{m}_k=\\mathbf{1}\\left\\{\\frac{1}{\\sum_i \\mathbf{1}\\{y^i_{\\text{det}}>\\tau\\}}\\sum_{i=1}^{hw}\\mathbf{1}\\{y^i_{\\text{det}}>\\tau\\}\\, y^i_{\\text{dec},k} > \\tfrac{1}{2}\\right\\}.\n    \\]\n  - Multi-message decoding: hard local bits \\(\\tilde{m}^{(i)}_k=\\mathbf{1}\\{y^i_{\\text{dec},k}>0.5\\}\\) for pixels i with y^i_{\\text{det}}>\\tau; cluster { \\(\\tilde{m}^{(i)}\\)} by DBSCAN in Hamming space (ε, minsamples) to obtain binary centroids as decoded messages.\n- Objectives (per image):\n  \\[\n  \\ell_{\\text{det}}=-\\frac{1}{hw}\\sum_{i=1}^{hw}\\Big[y^{\\star}_{\\text{det},i}\\log y^i_{\\text{det}}+(1-y^{\\star}_{\\text{det},i})\\log(1-y^i_{\\text{det}})\\Big],\n  \\]\n  \\[\n  \\ell_{\\text{dec}}=-\\frac{1}{\\text{nbits}\\sum_i y^{\\star}_{\\text{det},i}}\\sum_{i=1}^{hw} y^{\\star}_{\\text{det},i}\\sum_{k=1}^{\\text{nbits}}\\Big[m_k\\log y^i_{\\text{dec},k} + (1-m_k)\\log(1-y^i_{\\text{dec},k})\\Big],\n  \\]\n  \\[\n  \\ell=\\lambda_{\\text{det}}\\ell_{\\text{det}}+\\lambda_{\\text{dec}}\\ell_{\\text{dec}}.\n  \\]\n  For multi-mask post-training with messages {m^{(u)}} and masks {r^{(u)}}, use y^{\\star}_{\\text{det}}=\\bigvee_u r^{(u)} in \\(\\ell_{\\text{det}}\\), and sum \\(\\ell_{\\text{dec}}^{(u)}\\) over u with supervision restricted to pixels of r^{(u)}.\n- Complexity (dominant extractor, ViT-base, patch size p):\n  - Let N=(h/p)(w/p) tokens, hidden dim d. Self-attention per layer: O(N^2 d), MLP: O(N d^2). Head outputs: O(hw·nbits).\n\nComputational_Properties:\n- Time Complexity:\n  - Embedder: O(hw·C_e) with small constant due to shallow encoder/decoder and f=8 latent; JND map O(hw).\n  - Extractor (dominant): For L transformer layers, O(L(N^2 d + N d^2)) with N=(h/p)(w/p). Pixel decoder upsampling O(hw·d').\n  - High-resolution embedding: resize to h×w (O(HW)), compute δ at h×w, bilinear upsample δ to H×W (O(HW)); extraction always at fixed h×w cost independent of H×W.\n  - DBSCAN for M detected pixels: O(M log M) with spatial index (worst-case O(M^2) without indexing). Distance is Hamming or ℓ2 over {0,1}^{nbits}.\n- Space Complexity:\n  - Parameters: ≈1.1M (embedder) + 96M (extractor). Activation memory dominated by ViT (O(L·N·d) for tokens and O(L·N^2) for attention maps); pixel decoder adds O(hw·d').\n  - Temporary masks and JND maps: O(hw). For multi-watermark, store U masks (U≤3 in training) and messages, negligible vs. ViT.\n- Parallelization:\n  - Highly parallelizable convolutions and MLPs; self-attention uses batched GEMMs across tokens; per-pixel heads are trivially parallel.\n  - Masked BCE losses computed with elementwise masking; augmentation pipeline (splicing + transforms) parallelizable per sample.\n  - DBSCAN runs per image; can be parallelized across images; within-image neighbor search benefits from GPU-accelerated kNN or bitset Hamming acceleration.\n- Hardware Compatibility:\n  - Optimized for GPUs (ViT backbone); mixed precision (FP16/BF16) stable due to sigmoid/BCE objectives and normalization; JND involves simple convolutional kernels/filters.\n  - Memory bandwidth sensitive in attention; benefits from flash-attention kernels for N=256 (h=w=256, p=16 → N=16×16).\n- Training vs. Inference:\n  - Training adds backprop graph and strong augmentations; post-training includes multiple masks/messages but same computational class.\n  - Inference fixed-cost extractor (independent of input resolution due to resize) and lightweight DBSCAN. Embedding adds a single forward pass through compact autoencoder + JND map computation.\n- Parameter Count:\n  - Embedder ≈1.1M (fast, client-side); Extractor ≈96M (ViT-Base + pixel decoder), output head scales linearly with nbits via final 1×1 linear layer.\n- Numerical Stability:\n  - BCE losses with per-pixel masking normalize by count of watermarked pixels, mitigating class imbalance when r_{\\text{mask}} is small.\n  - Sigmoid outputs; apply small ε in JND luminance branch to avoid sqrt(0). Post-training with JND avoids conflicting objectives of adversarial training.\n  - Thresholds τ for detection and y_{\\text{dec}}=0.5 for bit binarization are calibrated on held-out data; majority/averaging aggregation reduces variance.\n- Scaling Behavior:\n  - Complexity scales quadratically with token count N (thus linearly with image area at fixed p once resized to h×w). Since all processing occurs at fixed h×w, runtime is stable across input resolutions; only a bilinear up/downsample adds O(HW).\n  - Output dimension scales with nbits; decoding and DBSCAN cost grow linearly with nbits and number of detected pixels M.\n  - Multi-watermark capability generalizes beyond training U (tested up to 5 messages), with clustering cost increasing with number/area of watermarked regions.",
        "IMPLEMENTATION_GUIDANCE": "Integration_Strategy:\n- System components and data flow\n  - Insert a post-hoc embedder and an extractor pair into your image I/O pipeline.\n    - Ingestion: RGB image x ∈ [0,1]^{3×H×W} or [0,255] uint8 -> convert to float32 and normalize to [0,255] for JND.\n    - Embed: resize x to h×w (default 256×256), run embθ(x,m) to produce δ ∈ [-1,1]^{3×h×w}, bilinear-upsample δ to original H×W, add scaled δ to original x (with or without JND), clip to valid range, save as watermarked image.\n    - Extract: resize incoming image to h×w, run extθ(x̂) to get y = [ydet, ydec] ∈ [0,1]^{(1+nbits)×h×w}, post-process for detection/mask/message(s).\n- Modules to add/modify\n  - Add WAMEmbedder (encoder, T lookup, decoder) and WAMExtractor (ViT encoder + pixel decoder).\n  - Replace any existing “global message” decoder with pixel-wise extractor and add clustering step for multi-watermark decoding.\n  - Insert a JND modulation layer between decoder output and residual add in Phase 2.\n  - Add an Augmenter that supports splicing masks + geometric and valuemetric transforms; ensure mask coordinates are transformed consistently.\n  - Add DBSCAN clustering on per-pixel hard-decoded messages for multi-watermark extraction.\n- Code-level changes (PyTorch)\n  - Implement encθ and decθ as light LDM/VQGAN-style blocks. Use groupnorm+swish residual blocks, average-pool Down, bilinear Up + Conv2d (avoid ConvTranspose2d to prevent checkerboard).\n  - Message embedding table Tθ with shape (nbits,2,dmsg); code to gather Tθ[k, mk] and average over bits; repeat to spatial (h′,w′) before concatenation with z.\n  - JND(x): compute LA and CM with fixed kernels (Klum, Sobel), combine as H = LA + CM − γ·min(LA,CM); set per-channel scaling (αR,αG,αB) = (1,1,2); ensure differentiability with epsilon in sqrt.\n  - Extractor: ViT-B encoder (patch size 16) + pixel decoder with 3 Up stages (×4,×2,×2) to 256×256; final linear to (1+nbits) channels; sigmoid at inference only.\n  - Training step: y = extθ(augment(splice(embθ(x,m), x, rmask))); compute ℓdet (pixel CE) and ℓdec (masked pixel-bit BCE).\n  - Detection/decoding APIs:\n    - sdet = mean(1{ydet>τ}); image-level detection if sdet > sdet_threshold.\n    - Single-message decode via weighted average: m̂k = round( sum(1{ydet>τ}·ydec[:,k]) / max(1, sum(1{ydet>τ})) ).\n    - Multi-message: Iw = {i | ydet_i>τ}; ẑ_i = 1{ydec_i>0.5}; run DBSCAN(ẑ_i) with Hamming metric; cluster centers are binary messages.\n- Framework compatibility\n  - PyTorch 1.13+ / 2.x recommended; torchvision for transforms; timm for ViT or custom blocks; scikit-learn for DBSCAN; optional PyTorch AMP for mixed precision.\n  - TensorFlow/JAX: port blocks 1:1; ensure identical resize/interp modes (bilinear) and mask transform pipelines; keep Sobel/JND kernels constant.\n- Migration path\n  - From HiDDeN-like pipelines: replace global decoder with WAMExtractor, add detection head, and change loss from global BCE to pixel-wise segmentation + pixel-bit BCE.\n  - From TrustMark/SSL/FNNS: keep your data and augmentations; add splicing and mask handling; switch to fixed 256×256 training with anisotropic resize for high-res inference.\n- Dependencies/hardware features\n  - No custom CUDA kernels required. Use standard PyTorch ops. For speed, enable torch.compile (PyTorch 2.x) and autocast (fp16/bf16).\n  - DBSCAN can be CPU; for batch processing, vectorize Hamming distance or pre-filter by spatial connected components to reduce points.\n- Pipeline integration\n  - Phase 1 (robustness/localization): train embedder+extractor jointly, no JND in forward, α used as scalar strength.\n  - Phase 2 (imperceptibility/multi): add JND modulation and multiple message masks; fine-tune both modules.\n  - Inference: choose operating mode per product need (global detect, pixel mask, single message, multi messages); configure τ and sdet_threshold via held-out calibration to achieve target FPR.\n\nParameter_Settings:\n- Architecture\n  - Resolution: h×w = 256×256 (fixed training); high-res supports H,W up to ~4k with bilinear upsample of δ.\n  - Downsample factor f = 8, so h′=w′=32.\n  - Embedder channels: residual blocks m=4 with d = [32,32,32], d′=64; latent dz=4; dmsg=32.\n  - Decoder last activation: tanh; δ ∈ [-1,1].\n  - Extractor ViT-Base: d=768, patch=16, blocks=12; pixel decoder upscales by ×16 (4×2×2).\n- Hyperparameters (Phase 1)\n  - nbits: 32 (recommended initial). Message table Tθ ∈ R^{32×2×32}.\n  - Loss weights: λdec=10, λdet=1 (critical; if λdec too small, message accuracy suffers; too large, detection degrades).\n  - Strength α = 0.3 (without JND) during Phase 1.\n  - Optimizer: AdamW (β1=0.9, β2=0.999, weight_decay=0.05-0.1). LR schedule: warmup 5 epochs from 1e-6 → 1e-4, cosine decay to 1e-6 over 300 epochs.\n  - Batch size: 16 per 32GB V100/A100; on 24GB GPUs use 8 with grad-accum=2; AMP recommended.\n- Hyperparameters (Phase 2)\n  - JND γ in [0.5, 1.0]; set γ=0.7-0.9; per-channel scale (1,1,2) fixed.\n  - αJND = 2.0 (baseline). Trade-off knob: 1.0 → PSNR~44 dB, lower robustness; 2.0 → ~38 dB; 3.0 → ~35 dB, highest robustness.\n  - Multi-mask count K ∈ {1,2,3} with probs {0.6,0.2,0.2}. Use rectangles or dataset segmentation masks; non-overlapping constraint in training.\n  - Fine-tune 200 epochs; LR restart to 1e-5 with cosine to 1e-6.\n- Augmentations (must have; Phase 1)\n  - Splicing: rmask from {full, rectangle, irregular brush, segmentation}; invert with p=0.5; ensure same geometric transform applied to mask.\n  - Geometric: random resize (0.5-1.5), crop (0.33-1.0), rotate (-10,+10), perspective (0.1-0.5), horizontal flip.\n  - Valuemetric: JPEG Q∈[40,80], Gaussian blur k∈{3..17 odd}, median k∈{3..7}, brightness [0.5,2.0], contrast [0.5,2.0], saturation [0.5,2.0], hue [-0.1,0.1].\n- Detection thresholds\n  - Pixel threshold τ:\n    - For general detection/localization: choose τ via calibration to achieve pixel-wise FPR target (e.g., 1e-4 to 1e-3). Typical τ ≈ 0.5 for multi-watermark clustering.\n  - Image-level sdet threshold (proportion of pixels with ydet>τ):\n    - For COCO-like distribution, sdet_threshold ≈ 0.07 gives FPR ≈ 4e-4.\n- DBSCAN for multi-message\n  - Distance: Hamming on binary ẑ_i ∈ {0,1}^{32}.\n  - ε ∈ [1,3] (Hamming distance); start with ε=1 for strict grouping.\n  - minsamples ≈ 2% of image pixels; for 256×256 → 65536 px, set minsamples=1000-1500. Smaller boosts recall; larger reduces false clusters.\n  - Pre-filter: only include pixels with ydet>τ (τ≈0.5). Optionally erode mask by 1-2 px to reduce boundary noise.\n- Initialization\n  - Tθ: normal(0, 0.02); enc/dec/ViT: standard Kaiming/ViT initializations; last linear bias for detection head initialized to -3.0 (encourages low positives early).\n- Critical vs robust parameters\n  - Critical: λdec/λdet balance, τ and sdet_threshold calibration, αJND (imperceptibility/robustness trade), DBSCAN minsamples.\n  - Robust defaults: ViT-Base config, patch size 16, f=8, dmsg=32, γ≈0.7-0.9, augment ranges as above.\n- Hardware-dependent settings\n  - A100 40GB/80GB: batch 32-48 (AMP), grad checkpointing for ViT; training time ~1.5× faster than V100.\n  - Consumer GPUs 12-16GB: batch 4-6, grad-accum 4-8; enable xformers/flash-attn for ViT if available.\n\nApplication_Conditions:\n- Beneficial scenarios\n  - Mixed provenance images, splicing, inpainting, object-level AI insertions, edits where only 2-10% of pixels are watermarked.\n  - Regulatory watermarking requiring localization and low false positives at image-level.\n  - Multi-tool attribution: identifying multiple 32-bit keys in different regions.\n- Hardware requirements\n  - Training: 8× V100 32GB or 4× A100 40GB recommended; Phase 1 ~2 days, Phase 2 ~1-1.5 days at 256×256.\n  - Inference: CPU/GPU; 96M-param extractor dominates (ViT-B). Real-time feasible at 256×256 on a single T4 (10-15 ms).\n- Scale considerations\n  - Train at 256×256 for robustness and compute economy; use anisotropic resize for high-res inference (up to ~4k on 16-32GB GPUs).\n  - For payloads >32 bits, prefer multi-watermark across regions rather than increasing nbits; training for nbits>32 becomes unstable and reduces robustness.\n- Task compatibility\n  - Strong on zero-bit detection and 32-bit multi-bit decoding with localization; excels under geometric/valuemetric transforms, splicing, and inpainting.\n  - Less suitable when a single global high-capacity (>100 bits) payload is required without localization needs; consider EditGuard/RoSteALS but expect crop fragility.\n- Alternative comparisons\n  - Versus generation-time watermarks (Tree-Ring/Stable Signature): choose WAM for post-hoc marking of camera/legacy content, localized detection, and multi-message extraction; choose generation-time for near-zero distortion and LDM integration.\n- Resource constraints\n  - If training compute is limited: freeze extractor ViT after 150-200 epochs and only continue training embedder for stability; reduce augment ranges (narrow perspective/rotation) at the cost of robustness.\n\nExpected_Outcomes:\n- Performance improvements (vs typical global watermarking)\n  - Detection under geometric transforms: TPR 96-99% at FPR ≈ 4e-4 (COCO), with bit accuracy ~89-92% under strong geometric sets; significant gains over DCT/DWT/HiDDeN-type baselines.\n  - Splicing (10% area watermarked): TPR ~100%, bit accuracy ~94-96%.\n  - Inpainting (LaMa on 30-35% area): TPR ~98-100%, bit accuracy ~99%.\n- Imperceptibility\n  - With αJND=2: PSNR ~38-39 dB, SSIM ~0.99, LPIPS ~0.03-0.04.\n  - Trade-off (JPEG80+50% crop+brightness1.5): αJND 1.0 → PSNR ~44 dB / bit acc ~71%; 2.0 → ~38 dB / ~89%; 3.0 → ~35 dB / ~92%.\n- Multi-watermark extraction\n  - Up to 5× 32-bit messages in 5×10% tiles: mIoU ≈0.98 for masks; bit accuracy ≈0.94-0.98; number of clusters detected ≈K with ε=1, minsamples=1000.\n  - Robust under horizontal flip + contrast 1.5 (mIoU ~0.85, avg bit acc >0.95); likely to fail when also adding JPEG80 to the same image.\n- Timeline expectations\n  - Phase 1: robustness/localization emerge by ~100-150 epochs; full performance after ~300.\n  - Phase 2: imperceptibility restored within first 50-80 epochs; multi-message separation stabilizes by ~120-150 epochs.\n- Failure modes\n  - Visual artifacts: repetitive patterns noticeable in very bright/flat regions or large textured fur; mitigate with stronger JND or watermark regularizers (e.g., total variation on δ).\n  - Purification/autoencoding: diffusion-based purification or strong VAE compression (PSNR ≈25 dB) significantly degrade bit accuracy across all post-hoc methods, including WAM.\n  - Overwhelming coverage: when ~95% of image is watermarked, extractor may classify nearly all pixels as watermarked; calibrate τ higher or add class-balance loss.\n  - Overlapping different messages in same pixels (full overlap): detection still positive, but decoding may collapse to dominant pattern; report “multiple/ambiguous” state when clusters are not separable.\n- Debugging indicators\n  - Training sanity: Phase 1 PSNR ~24-26 dB (visible watermark), perfect localization on training masks, bit acc >99% on unaugmented, >90% on strong aug sets.\n  - Phase 2: PSNR rises to ~36-39 dB with no significant drop in COCO robustness; pixel-wise ROC shows separability; τ calibration yields desired pixel FPR on held-out data.\n  - Multi-watermark: cluster count matches K with <5% noise points; centroid messages stable across random seeds.\n- Hardware-specific outcomes\n  - AMP provides ~1.5-2× speedup on A100 with no accuracy loss; bf16 safe if numerics stable in ViT attention; keep BCE losses in fp32 for stability.\n  - torch.compile yields 10-20% speedup for extractor on PyTorch 2.x; ensure static shapes (256×256) for maximal gain.\n\nTroubleshooting and validation procedures:\n- Calibration of thresholds\n  - Use 5k-10k negative images (non-watermarked) to set τ for pixel FPR target (1e-4 to 1e-3). Then sweep sdet_threshold to match image FPR e.g., 4e-4 on COCO; typical sdet_threshold ≈ 0.05-0.1.\n- Augmentation alignment\n  - If mIoU collapses after perspective/crop: verify you apply identical spatial transforms to the mask (same parameters, same interpolation) and correct inverse mapping for perspective.\n- JND correctness\n  - Confirm JND heatmap brighter on textured edges and lower on flat regions; verify per-channel scaling doubles blue channel; visualize δ·JND for sanity.\n- DBSCAN tuning\n  - If too few clusters: lower minsamples from 1500→1000, or relax ε from 1→2.\n  - If too many clusters/noise: raise minsamples to 2000-3000, or erode mask by 1-2 px before clustering.\n- High-resolution path\n  - If high-res bit accuracy drops: ensure δ is computed on resized 256×256 input and bilinearly upsampled back; do not run embedder at native res; maintain extractor resize to 256×256 at inference.\n- Reproducibility checks\n  - Metrics to report: PSNR/SSIM/LPIPS on COCO and DIV2K; TPR/BitAcc under grouped aug sets; mIoU vs watermarked area curve; multi-watermark cluster counts/bit acc vs K; ROC AUC for global detection.\n\nSoftware/hardware notes:\n- PyTorch >=1.13 (or 2.x), torchvision, timm (optional), scikit-learn for DBSCAN.\n- GPU: V100 32GB/A100 40GB recommended for reported batch sizes; consumer 12-24GB requires smaller batch with grad accumulation.\n- Mixed precision (fp16/bf16) recommended; keep loss accumulations in fp32.\n\nOperational recommendations:\n- Default operating point for production\n  - nbits=32, αJND=2.0, τ=0.5 (pixel mask for multi), sdet_threshold≈0.07 (global detection), DBSCAN ε=1, minsamples=1000 at 256×256.\n- For higher imperceptibility (e.g., stock photography)\n  - αJND in [1.2,1.6], expect PSNR 40-42 dB and 5-10% robustness reduction under worst-case augment combos.\n- For maximum robustness (e.g., social media redistribution)\n  - αJND in [2.5,3.0], accept PSNR 34-36 dB; increase minsamples to 1500; allow ε=2 if DBSCAN fragmentation appears after compression.\n\nSecurity/abuse considerations:\n- Expect adversarial purification to remove watermarks; WAM focuses on robustness to common, non-adversarial edits (crop/resize/JPEG/filters/splicing/inpainting).\n- Avoid exposing τ and Tθ publicly; rotate message keys regularly; for overlapping keys detected in same region, treat as tamper risk and escalate."
    }
]