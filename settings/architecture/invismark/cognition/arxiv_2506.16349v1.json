[
    {
        "DESIGN_INSIGHT": "### DESIGN_INSIGHT_HIGH: [Reverse Cycle-Consistent Tokenizer–Detokenizer Finetuning (E′, D) for Robust Token-Level Watermarking]\nThis work replaces the standard forward-cycle-consistent (FCC) training of VQ tokenizers with a targeted finetuning of the decoder D and a detector-side encoder replica \\(E'\\) to enforce reverse cycle-consistency (RCC) needed for reliable token-level watermark detection in autoregressive image models. The original encoder \\(E\\), quantizer \\(Q_C\\), and codebook \\(C\\) remain frozen to preserve model semantics and avoid retraining the generator.\n\nThe key mechanism optimizes the detokenization–retokenization loop to make the re-tokenized sequence match the original generated tokens even under common transformations. Given hard latents \\(\\hat{z}=C_s\\) (from codebook lookups), the RCC loss enforces agreement between the original latents and the latents re-encoded by \\(E'\\) after detokenization and augmentation:\n\\[\nL_{\\mathrm{RCC}}(s) \\;=\\; \\mathbb{E}_{a\\sim \\mathcal{A}} \\, \\big\\| \\hat{z} - E'\\!\\big( a\\!\\left(D(\\hat{z})\\right) \\big) \\big\\|_2^2,\n\\]\nwhere \\(\\mathcal{A}\\) is a stochastic augmentation set spanning valuemetric and weak geometric edits (with strength ramp-up). A regularization term retains decoder fidelity to its initial behavior via a mixture of pixel-space MSE and perceptual LPIPS:\n\\[\nL_{\\mathrm{reg}}(s) \\;=\\; \\big\\| D(\\hat{z}) - D_0(\\hat{z}) \\big\\|_2^2 + L_{\\mathrm{LPIPS}}\\!\\left(D(\\hat{z}), D_0(\\hat{z})\\right),\n\\]\nand the combined objective is\n\\[\nL(s) \\;=\\; L_{\\mathrm{RCC}}(s) + \\lambda \\cdot L_{\\mathrm{reg}}(s).\n\\]\nTo cope with non-differentiable JPEG, a straight-through estimator is applied: \\(x' = x_{\\text{aug}} + \\mathrm{nograd}(x_{\\text{aug}}, \\mathrm{JPEG} - x_{\\text{aug}})\\).\n\nThis fundamentally differs from prior VQ training, which optimizes only FCC (reconstruction quality) and ignores RCC; and from text LLM watermarking, where RCC is largely satisfied by BPE tokenizers. Enforcing RCC at the tokenizer layer enables generation-time, token-level watermarks (LLM-style) to persist through decode–retokenize steps and typical edits, without modifying the autoregressive model itself. Watermark survival is measured via token match\n\\[\n\\mathrm{TM}(s, s') \\;=\\; \\frac{1}{T} \\sum_{i=1}^{T} \\mathbf{1}\\!\\left(s_i = s'_i\\right), \\quad s' = T_m\\!\\big(D_m(s)\\big),\n\\]\nwhich is significantly improved by this finetuning, translating directly into higher statistical watermark power at low false-positive rates.\n\n---\n\n### DESIGN_INSIGHT_MEDIUM: [Localized Quadrant Synchronization Layer – Transformation Estimation and Inversion to Restore Token Sequences]\nInstead of multi-hypothesis testing over many geometric transforms (which inflates FPR), the paper adds a synchronization layer that locally embeds fixed messages to estimate and invert geometric operations before applying the token-level detector. A localized watermark module \\( \\mathcal{L} \\) writes four 32-bit messages \\(\\{0^{32},\\, 0^{16}1^{16},\\, 1^{16}0^{16},\\, 1^{32}\\}\\) into four quadrants, reserving thin central strips to simplify boundary estimation. At detection, per-pixel message probabilities and Hamming-thresholded decodes are aggregated, and a grid search over rotation angles is used to fit orthogonal lines that best separate message clusters, also testing horizontal flips. The resulting transform estimate \\((\\theta, i, j, \\text{isFlip})\\) is inverted, and the recovered image is re-tokenized for the original watermark test.\n\nThe mechanism differs from prior post-hoc synchronization by coupling a localized, multi-bit spatial signal to a generation-time, token-level watermark with rigorous p-values. Whereas classical methods rely solely on the localized signal for provenance, here synchronization is only used to undo geometric perturbations that would otherwise permute VQ tokens and erase the statistical pattern. This design yields strong geometric robustness with modest degradations under certain valuemetric edits (when synchronization is partially corrupted), and is aided by the RCC finetuning’s tolerance to small transform estimation errors. The approach is practical (single invert-then-test path) and preserves theoretical guarantees inherited from the token-level test.\n\n---\n\n### DESIGN_INSIGHT_MEDIUM: [Unified KGW Adaptation to VQ Image Tokens with Exact Binomial Test and Codebook-Aware Splits]\nThe paper adapts the Kirchenbauer–Geiping–Wen (KGW) LLM watermark to VQ image tokens, enabling generation-time semantic perturbations detectable with rigorous statistics. At step \\(i\\), a secret key \\(\\xi\\) and context of size \\(h\\) pseudo-randomly partition the vocabulary \\(V\\) into a greenlist \\(G_i\\) (size \\(\\gamma|V|\\)) and redlist \\(R_i\\), boosting green logits by \\(\\delta\\). The detector computes\n\\[\nS \\;=\\; \\sum_{i=h+1}^{T} \\mathbf{1}\\!\\left(s_i \\in G_i\\right),\n\\]\nand applies the exact binomial tail test under \\(H_0\\) (no watermark):\n\\[\np\\text{-value}(S, T, h, \\gamma) \\;=\\; \\Pr\\!\\left(X \\ge S \\,\\middle|\\, X \\sim \\mathrm{Binomial}(T-h, \\gamma)\\right) \\;=\\; I_{\\gamma}\\!\\big(S, T-h-S+1\\big),\n\\]\nwith rigorous p-values that combine across interleaved modalities (text and image) after deduplicating scored (context, token) pairs. For images, using \\(h=0\\) (fixed split) is viable because VQ tokenizers are opaque and inaccessible to adversaries, unlike text.\n\nA core architectural refinement addresses low codebook utilization in VQGANs: when the number of “alive” codes \\(n_{\\text{alive}}\\ll|V|\\), uniform global splits yield a random green ratio among alive codes governed by a hypergeometric distribution\n\\[\nP_g(|V|, n_{\\text{alive}}, \\gamma) \\;=\\; \\frac{\\binom{\\gamma|V|}{g} \\binom{|V|-\\gamma|V|}{n_{\\text{alive}}-g}}{\\binom{|V|}{n_{\\text{alive}}}},\n\\]\nmaking the effective \\(\\gamma\\) under \\(H_0\\) biased. The paper introduces stratified partitioning—sampling green/red separately on alive and dead codes—to fix the effective green ratio to \\(\\gamma\\) and keep the exact test sound, especially important for \\(h=0\\). Compared to diffusion/post-hoc image watermarks that rely on neural extraction with heuristic thresholds, this unified token-level scheme provides statistically principled, modality-agnostic provenance with improved robustness when combined with RCC finetuning and synchronization.",
        "EXPERIMENTAL_TRIGGER_PATTERNS": "Task_Performance_Signatures:\n- Image generation quality (FID-50k, ImageNet-1K, 256×256 and 512×512)\n  - Expect stable image quality after RCC finetuning: FID remains within ±1% of baseline for Base/FT/FT+Augs; reported values: Taming unwatermarked FID ≈16.7; FT and FT+Augs ≤16.7; Chameleon baseline ≈19.7 and unchanged for FT/FT+Augs; RAR-XL baseline ≈11.5 and unchanged for FT/FT+Augs.\n  - Small quality cost with synchronization: FT+Augs+Sync increases FID modestly (Taming ≈17.3; negligible change on Chameleon/RAR-XL).\n  - Degradation bound when over-strengthening watermark: δ=4 causes visible quality loss (e.g., FID rises from ≈16.3 to ≈18.8 on Taming). Use δ≈2 for “no visible degradation.”\n\n- Watermark detection power (TPR@1% FPR, p-values from exact binomial test)\n  - Base (no RCC finetuning) already achieves near-perfect TPR on clean outputs: ≈0.98–1.00 across Taming/Chameleon/RAR-XL at FPR=1%.\n  - RCC finetuning (FT+Augs) substantially boosts robustness to valuemetric transforms at same TPR/FPR operating point:\n    - Taming: Valuemetric TPR rises from 0.26 (Base) → 0.92 (FT+Augs). Neural Compression: 0.48 → 0.79. Diffusion purification (DiffPure): 0.43 → 0.70.\n    - Chameleon: Valuemetric 0.50 → 0.89; Neural Compression 0.82 → 0.88; DiffPure stays high ≈0.82→0.82.\n    - RAR-XL: Valuemetric 0.22 → 0.98; Neural Compression 0.27 → 0.95; DiffPure 0.29 → 0.79.\n  - Geometric robustness requires synchronization: FT+Augs+Sync lifts geometric TPR from ≈0.01–0.03 to ≈0.64–0.82 across models at 1% FPR, while preserving clean-image TPR ≈0.98–1.00.\n  - Quantitative detection signature: watermarked generations yield extremely low p-values (often <1e−30) on clean and synchronized images; p-values are uniform on unwatermarked images.\n\n- Token-level reverse cycle-consistency (Token Match, TM)\n  - Expect TM improvement after RCC finetuning on clean images: median/mean TM rises from ≈0.66 (Base) to >0.80 (FT/FT+Augs) for Taming; similar improvement distributions on Chameleon and RAR-XL.\n  - Under JPEG Q=25, TM improves from ≈0.31 (Base) to >0.6–0.7 with FT+Augs; geometric transforms alone still drive TM near zero without synchronization.\n\n- Joint watermarking of interleaved text+image (Chameleon)\n  - On text-only detection, TPR at 1% FPR decays with paraphrasing (random token substitutions). With joint scoring (image+text), TPR recovers:\n    - At 10% corrupted text tokens: text-only TPR ≈0.9 vs joint TPR ≈1.0 (clean image) and ≳0.94 with weak image noise (σ≈0.1).\n    - With strong image degradation (σ≈0.3), joint TPR can drop below text-only; prefer text-only in this regime.\n  - Overall pattern: improvements are most pronounced when image watermark signal quality ≥ text signal quality and when the image contributes many tokens (e.g., 1024 image tokens + 200–300 text tokens).\n\n- Audio (Moshi/Mimi RVQ case study)\n  - Quality: MOSNet near baseline (unwatermarked ≈3.80; FT ≈3.83; FT+Augs ≈3.73). PESQ ≈4.3 w.r.t. Base.\n  - Detection: clean TPR@1% FPR ≈0.97–0.99; valuemetric robustness improves with FT+Augs (e.g., bandpass/high/low-pass and MP3 reach ≈0.95–0.99). Time-frequency robustness remains limited without a dedicated synchronization method (typical TPR 0.2–0.4 at 1.1× speed or 70–90% cropping).\n\n- Compatibility with standard NLP benchmarks (unchanged LM capability)\n  - Because the technique modifies sampling (green/red bias) without changing LLM weights for text, expect negligible change (within ±1% relative) on lambada_openai, wikitext, ptb, squad_completion, squad_v2, narrativeqa, hellaswag, piqa, social_iqa, commonsenseqa, arc_easy, arc_challenge, boolq, openbookqa, winogrande, winograd, swde, fda when text watermarking (KGW) is applied with δ≈2 and γ≈0.25; detection p-values remain theoretically calibrated.\n\n- Scaling and context conditions\n  - Improvements scale with image token count: larger images (e.g., 512×512) or models scoring more tokens yield stronger detection (lower p-values) at the same γ.\n  - h (context window) sensitivity: h∈{0,1} recommended for images; h>1 degrades robustness (valuemetric TPR drops e.g., from 0.92 → 0.69 on Taming at h=3) and makes removal easier.\n  - δ tuning: δ≈2 balances power and quality; δ=1 reduces robustness; δ=4 improves robustness but harms FID and may introduce artifacts.\n\nArchitectural_Symptoms:\n- Training characteristics (RCC finetuning of detokenizer D and encoder replica E′ only)\n  - Loss curves: LRCC steadily decreases over 10 epochs; with augmentation ramp (A1→A3) expect monotonic or staircase-like improvements; Lreg stabilizes early, indicating decoder outputs remain close to original.\n  - Stability: no NaNs with Adam lr≈1e−4 and λ≈1; lr≈1e−3 or λ≈0.1 can induce visible artifacts; λ≈10 underfits RCC. Finetuning all VQ components (including codebook) maximizes robustness but produces clear visual artifacts—avoid if generation quality is a constraint.\n  - Token Match as an auxiliary metric: observe TM rising from ≈0.6–0.7 to >0.8 on validation generations within first few epochs; plateau thereafter.\n  - Data/augmentation schedule: progressive increase of JPEG, blur, noise, slight rotations/crops; JPEG trained via straight-through estimator; expect improved valuemetric robustness without harming clean TPR.\n  - Compute budget signature: small finetune footprint—Taming ≈2h on 16×V100; Chameleon ≈2.5h on 8×H200; RAR-XL ≈0.5h on 8×H200.\n\n- Runtime/memory behaviors (detection pipeline)\n  - Memory scaling: identical to baseline tokenizer/detokenizer runs; per-image detection O(T) in token count; synchronization adds one localized watermark extraction pass and grid search over rotations (±20°), increasing wall time but not peak memory. In the authors’ end-to-end evaluations, FT/FT+Augs+Sync runs were ≈3× longer than Base/FT/FT+Augs (e.g., ≈30 min vs ≈1.5 h for 1000 images with Taming on similar GPU counts).\n  - Throughput: clean-image detection throughput roughly matches standard tokenization throughput; synchronization degrades throughput by the rotation sweep and line-fitting step; GPU utilization remains high during localized extraction.\n  - Hardware utilization: no additional OOM risks versus baseline tokenization; localized extraction networks fit in memory with standard batch sizes for 256–512 px images.\n\n- Profiling and calibration signatures\n  - Exact-test calibration: p-values on unwatermarked data uniformly distributed in [0,1]; if p-values drift high on very long sequences, verify green ratio calibration with alive codes (stratified split over alive/dead codes) and use γ′=Galive/nalive in Eq. 2 when |V|≫nalive and γ·nalive non-integer.\n  - Clean vs watermarked separation: median p-values on watermarked images far below 1e−9; on unwatermarked images cluster around ≈0.5. After synchronization, geometric transforms (flip/rotate/crop+resize) regain low p-values comparable to clean case.\n  - Geometric vs valuemetric trade: if moderate JPEG/noise triggers false-positive synchronization (estimating a transform where none exists), expect sudden TPR drops; mitigation signals include localization confidence thresholds (e.g., pixel-wise msg Hamming distance ≤6 bits and watermark prob >0.5) and a minimum covered-area check (≥70%).\n  - Decoder drift bounds: PSNR between original-decoder and finetuned-decoder outputs remains high (Taming ≈37.6 dB; Chameleon ≈39.5 dB; RAR-XL ≈29 dB), indicating small functional deviation; large PSNR drops usually correlate with over-aggressive λ↓ or lr↑.\n  - Joint-modality signature: when combining text and image tokens, global p-values decrease as token counts grow (e.g., 5× more tokens can compensate a 1% absolute drop in green ratio); if image signal is severely degraded (e.g., heavy noise σ≈0.3), joint detection can harm final p—prefer text-only scoring in that regime.\n  - Audio case study: expect MOSNet and PESQ near baseline post-finetune; clean TPR remains ≈0.99; weak gains for time-frequency edits without specialized synchronization. If time-frequency robustness remains low while valuemetric robustness improves, it indicates LRCC has converged but synchronization is the limiting factor.\n\n- Comparisons against baselines (sanity checks)\n  - Post-hoc image watermark baselines: CIN/MBRS strong on valuemetric but near-zero geometric or attack robustness; WAM strong geometric robustness but weak to purification/compression. The proposed method should outperform any single post-hoc method on the union of geometric+attack robustness at similar visual quality.\n  - Diffusion-model generation-time baselines (Tree-Ring, Stable Signature, Gaussian Shading) are not applicable to AR token models; if ported, expect mismatched robustness profiles—use as a qualitative reference only.\n\n- Failure/edge-case indicators\n  - h>1 in image watermarking yields fragile detection under small edits (TPR drops by 20–50% across robustness suites) and should be avoided.\n  - δ too low (≈1) produces weak bias, poor robustness; δ too high (≈4) yields detector wins but visible artifacts and FID regressions.\n  - Synchronization pattern loss: if localized messages cover <70% area or rotation fit cost is unstable across candidate angles, skip synchronization to avoid corrupting tokens.",
        "BACKGROUND": "Title: Watermarking Autoregressive Image Generation\n\nHistorical Technical Context:\nAutoregressive generative modeling has progressively unified multiple modalities—text, images, audio—under Transformer architectures by converting data into discrete token sequences. For images, the dominant paradigm employs vector quantization (VQ) tokenizers such as VQ-VAE, VQGAN, ImprovedVQGAN, and FSQ, which encode images into codebook indices reconstructed by a learned decoder. Early autoregressive image systems (e.g., DALL-E, Parti, VAR, RAR) demonstrated scalable quality improvements consistent with established Transformer scaling laws, motivating interleaved multimodal generation in mixed-modal models such as Chameleon, Janus, and AnyGPT. In text, tokenization is typically byte-pair encoding (BPE), and large language model (LLM) watermarking matured around generation-time, key-based statistical detection (e.g., KGW’s green/red scheme) with exact or asymptotic tests and provable false-positive guarantees.\n\nWatermarking for images historically emphasized post-hoc pixel-space embedding (e.g., Trustmark, MBRS, CIN, WAM), providing modality-agnostic insertion but reduced robustness to content-changing attacks like diffusion purification and neural compression. Generation-time image watermarks were mostly developed for diffusion models (e.g., Tree-Ring, Stable Signature, Gaussian Shading) and are inapplicable to autoregressive token sequences. Prior works did not address token-level watermarking for autoregressive image models. A key unexamined assumption was cycle-consistency: while forward cycle-consistency (FCC) holds by design (detokenizing then tokenizing real images preserves content well), reverse cycle-consistency (RCC)—retokenizing model-generated images—was not characterized, yet is crucial for detecting token-level watermarks on outputs after real-world transformations.\n\nTechnical Limitations:\n- Lack of reverse cycle-consistency (RCC) in VQ tokenizers: retokenizing decoded model outputs alters, on average, ~34% of image tokens (TM≈0.66 for Taming), and drops near zero under geometric changes (flip, rotation, crop), erasing token-level watermarks. This undermines statistical detection that assumes tokens are preserved between generation and post-hoc tokenization.\n- Geometric robustness gap: localized pixel-level changes induced by flips, rotations, and crops disrupt local token assignments, causing substantial token mismatch. Naïvely testing multiple transformed copies inflates false positives; the paper addresses synchronization to revert unknown transforms without multi-testing FPR inflation.\n- Valuemetric and codec brittleness: JPEG compression, blur, noise, and neural compressors (e.g., BMSHJ18, MBT18; SD/SDXL/FLUX VAEs; DC-AE) disturb tokenization significantly, reducing TM and weakening watermark power. Prior post-hoc watermarks are often brittle under diffusion purification, which semantically alters content.\n- Statistical soundness across modalities: unified detection over interleaved text and images requires deduplication of (context, token) pairs and consistent green-ratio γ to maintain binomial-test validity. Codebook dead/alive imbalance can bias fixed h=0 green splits if not stratified over alive codes.\n- Security–robustness tradeoffs in context size h: larger h increases removal susceptibility (small edits change downstream splits), while h=0 yields fixed splits that are insecure in text but may be acceptable for opaque image tokenizers; the paper explores both and secures statistical testing via exact p-values and deduplication.\n- Practical deployment constraints: JPEG is non-differentiable; improving RCC under real-world transformations requires training-time augmentations and straight-through estimation. Synchronization must preserve quality and watermark power while adding negligible overhead and avoiding instability under moderate distortions.\n\nPaper Concepts:\n- Reverse Cycle-Consistency (RCC): RCC measures whether re-tokenizing a detokenized sample recovers the original token sequence. Let image tokens be \\( s=(s_1,\\dots,s_T)\\in V^T \\), detokenization \\( \\hat{x}=D(C_s) \\) and retokenization \\( s' = T(\\hat{x}) \\). The token match metric is\n  \\[\n  \\mathrm{TM}(s,s') = \\frac{1}{T}\\sum_{i=1}^{T}\\mathbf{1}(s_i = s'_i),\n  \\]\n  and RCC holds if \\( \\mathrm{TM} \\approx 1 \\). The paper shows RCC is weak for images, motivating targeted finetuning.\n- KGW Green/Red Watermarking (LLM-adapted to images): With secret key \\( \\xi \\) and context size \\( h \\), the vocabulary \\( V \\) is pseudo-randomly partitioned per step \\( i \\) into green \\( G_i \\) (size \\( \\gamma |V| \\)) and red \\( R_i \\). Logits for \\( G_i \\) are boosted by \\( \\delta \\). The detection score is\n  \\[\n  S = \\sum_{i=h+1}^{T}\\mathbf{1}(s_i\\in G_i),\n  \\]\n  and under the null \\( H_0 \\), \\( S \\sim \\mathrm{Binomial}(T-h,\\gamma) \\). The exact p-value is\n  \\[\n  p=\\Pr\\big(X\\ge S\\mid X\\sim \\mathrm{Binomial}(T-h,\\gamma)\\big)=I_{\\gamma}(S,\\,T-h-S+1).\n  \\]\n  Adapting KGW to image tokens enables principled p-values for autoregressive images.\n- RCC Finetuning Objective: Keep encoder \\( E \\), quantizer \\( Q_C \\) and codebook \\( C \\) fixed; finetune decoder \\( D \\) and an encoder replica \\( E' \\) to align soft latents post-encode. With hard latents \\( \\hat{z}=C_s \\) and augmentation \\( a\\sim\\mathcal{A} \\), the loss is\n  \\[\n  L_{\\mathrm{RCC}}(s) = \\big\\|\\hat{z} - E'(a(D(\\hat{z})))\\big\\|_2^2,\\quad\n  L_{\\mathrm{reg}}(s) = \\|D(\\hat{z})-D_0(\\hat{z})\\|_2^2 + \\mathrm{LPIPS}\\big(D(\\hat{z}),D_0(\\hat{z})\\big),\n  \\]\n  combined as \\( L=L_{\\mathrm{RCC}}+\\lambda L_{\\mathrm{reg}} \\). Straight-through estimation is used for non-differentiable JPEG. This boosts TM and watermark power while preserving quality.\n- Watermark Synchronization Layer: A localized watermark embeds fixed messages \\( \\{m_q\\}_{q=1}^4 \\) in four quadrants, creating a synchronization pattern. At detection, an algorithm sweeps rotation angles and fits orthogonal axes that partition recovered messages, estimating flip/rotation/crop, and inverting them before tokenization and KGW detection. This restores geometric robustness without multi-testing FPR inflation.\n- Unified Multimodal Detection with Deduplication: For interleaved text and images, tokenize each sample \\( x^{(i)}\\to s^{(i)} \\), compute \\( S^{(i)} \\), and sum over modalities, deduplicating repeated (context, token) pairs to maintain independence assumptions in the binomial model. The global test uses shared \\( \\gamma \\) and exact p-values; scoring more tokens at comparable green ratio substantially improves detection strength.\n- Stratified Green Split over Alive Codes: When the VQ codebook size \\( |V| \\) greatly exceeds the number of alive codes \\( n_{\\mathrm{alive}} \\), uniform splitting can yield a green ratio among alive codes unequal to \\( \\gamma \\), biasing \\( H_0 \\). Stratifying the partition to enforce \\( \\gamma n_{\\mathrm{alive}} \\) green alive codes aligns empirical green frequency with the binomial model, especially important for \\( h=0 \\).\n\nExperimental Context:\nThe paper evaluates generation-time watermarking for autoregressive image models with a focus on robustness, statistical soundness, and quality preservation. Primary tasks are generative image and interleaved multimodal generation, emphasizing robustness to valuemetric (blur, noise, JPEG, brighten) and geometric (flip, rotation, crop) transformations, and removal attacks (diffusion purification via DiffPure; neural compression via CompressAI codecs and VAEs from diffusion pipelines). Metrics include TPR at fixed FPR (primarily 1%), exact p-values, token match TM for RCC, FID for image quality (50k samples), PSNR/LPIPS differences to assess decoder drift, and in audio extensions, MOSNet and PESQ. Models: Taming (VQGAN, 256×256, |V|=16384, f=16), Chameleon-7B (512×512, |V|=8192, f=16), and RAR-XL (955M, 256×256, |V|=1024, f=16). Standard parameters use \\( \\delta=2 \\), \\( \\gamma=0.25 \\), with \\( h=1 \\) for image token KGW on Taming and RAR-XL, and \\( h=0 \\) for Chameleon images; \\( h=1 \\) for text. RCC finetuning trains \\( D \\) and \\( E' \\) for 10 epochs with augmentation curricula; synchronization embeds four 32-bit messages and performs rotation/flip/crop estimation.\n\nOverall goals: demonstrate the first token-level watermark for autoregressive image generation with theoretically grounded exact p-values, resolve RCC as the key barrier via finetuning, and recover geometric robustness via synchronization. The work shows TPR≈1 at FPR=1% for base models and improved low-FPR power post-finetuning; quality preserved (e.g., FID unchanged for Base/FT/FT+Augs, slight increase with synchronization), and robustness persists under realistic transformation strength and attacks. Additionally, joint multimodal detection significantly improves power when text is paraphrased or partially corrupted, provided the image watermark signal is not severely degraded. The audio case study extends the methodology to RVQ token streams, highlighting modality-specific RCC and synchronization challenges.",
        "ALGORITHMIC_INNOVATION": "**Core_Algorithm:**\n- Replace standard autoregressive (AR) sampling with a KGW-style, key-conditioned greenlist/redlist watermark applied to VQ image tokens: at each decoding step i, compute a pseudorandom green set Gi ⊂ V from secret key ξ and context si−h:i−1, then boost logits of Gi by δ before sampling.\n- Finetune only the image tokenizer’s decoder D and an auxiliary encoder E′ (replica of E) to enforce reverse cycle-consistency (RCC): after detokenizing hard latents C s, pass through augmentations a ∈ A, then re-encode with E′ to match the original hard latents; keep the original encoder E, codebook C, and quantizer QC fixed to preserve AR model semantics.\n- Add a post-hoc watermark synchronization layer: embed a fixed set of localized messages (quadrant pattern) into the final image using a localized watermarking module; at detection, estimate and invert geometric transforms (flip/rotation/crop) from the recovered messages, then retokenize with the finetuned (E′, D) and run the KGW detector for a unified, exact p-value.\n- Perform statistically sound detection across samples and modalities by deduplicating scored (context, token) pairs and using an exact binomial test; stratify green/red splits over “alive” VQ codes to keep the nominal green ratio γ under the null.\n\n**Key_Mechanism:**\n- The method works by ensuring that tokens produced during AR sampling survive a detokenize→(unknown image transforms)→retokenize loop: RCC finetuning drives E′(a(D(C s))) ≈ C s, making the green/red structure detectable after the image has left the model. This preserves the token-level watermark statistics.\n- A localized message acts as a synchronization beacon: it enables reliable estimation of flips/rotations/crops so that geometric perturbations (which otherwise scramble VQ token order) are reverted before detection. Combined with an exact binomial test on green counts, this yields theoretically grounded p-values at low FPR.\n- Freezing E, QC, and C preserves the AR model’s learned token semantics; adjusting only D and E′ improves reverse consistency without invalidating the transformer’s logits or the codebook geometry.\n\n**Mathematical_Formulation:**\n- VQ tokenization and detokenization. For an image x and modality m:\n  - Encoder E maps x to soft latents z = E(x) ∈ R^{T×d}.\n  - Quantizer QC picks nearest codebook entry in C ∈ R^{|V|×d}:\n    \\[\n    s_i = \\arg \\min_{j \\in \\{1,\\dots,|V|\\}} \\| z_i - C_j \\|_2^2, \\quad \\hat z_i = C_{s_i}.\n    \\]\n  - Decoder D reconstructs \\(\\hat x = D(\\hat z)\\).\n- Reverse cycle-consistency (RCC) and token match:\n  \\[\n  s' = T_m(D_m(s)), \\qquad \\mathrm{TM}(s, s') = \\frac{1}{T}\\sum_{i=1}^T \\mathbf{1}(s_i = s'_i).\n  \\]\n- RCC finetuning objective. Sample augmentation \\(a \\sim \\mathcal{A}\\) with prob. \\(p_{\\text{aug}}\\), else identity:\n  \\[\n  L_{\\text{RCC}}(s) = \\left\\| \\hat z - E' \\big( a\\big(D(\\hat z)\\big) \\big) \\right\\|_2^2, \\quad \\hat z = C s.\n  \\]\n  Quality regularizer (keep D close to initial D_0):\n  \\[\n  L_{\\text{reg}}(s) = \\| D(\\hat z) - D_0(\\hat z) \\|_2^2 + L_{\\text{LPIPS}} \\big( D(\\hat z), D_0(\\hat z) \\big).\n  \\]\n  Joint loss:\n  \\[\n  L(s) = L_{\\text{RCC}}(s) + \\lambda \\, L_{\\text{reg}}(s).\n  \\]\n  JPEG in A is handled by a straight-through estimator on xaug − JPEG(xaug).\n- KGW watermarking for AR sampling and exact detection. With context size h ∈ {0,1} for images (and h=1 for text), define the greenlist via a PRF:\n  \\[\n  G_i = \\mathcal{G}\\big(\\xi, s_{i-h:i-1}\\big), \\quad |G_i| = \\gamma |V|.\n  \\]\n  Logit modification for vocabulary token v at step i:\n  \\[\n  \\ell'_{i}(v) = \\ell_{i}(v) + \\delta \\cdot \\mathbf{1}(v \\in G_i).\n  \\]\n  Detection score over T tokens:\n  \\[\n  S = \\sum_{i=h+1}^{T} \\mathbf{1}\\big(s_i \\in G_i\\big).\n  \\]\n  Under H0 (no watermark), \\(S \\sim \\text{Binomial}(T-h, \\gamma)\\); exact p-value:\n  \\[\n  p = \\Pr\\big(X \\ge S \\,\\big|\\, X \\sim \\text{Binomial}(T-h,\\gamma)\\big) = I_{\\gamma}\\big(S, T-h-S+1\\big).\n  \\]\n  When VQ has nalive “alive” codes (nalive ≪ |V|), stratify G_i over alive codes to match γ; otherwise the green ratio within alive codes follows a hypergeometric law and biases p-values. Optionally test with \\(\\gamma' = |G_i \\cap \\text{alive}|/n_{\\text{alive}}\\).\n- Synchronization layer (local message embedding and transform estimation). Embed four fixed 32-bit messages \\(\\{m_k\\}_{k=1}^4\\) in quadrants using a localized embedder L with a central dead band of width μ:\n  \\[\n  x_{\\text{sync}} = \\mathrm{Embed}\\big(x; \\{(m_k, \\text{mask}_k)\\}\\big).\n  \\]\n  At detection, extract per-pixel messages and confidences, then search over angles \\(\\theta \\in \\Theta\\) and horizontal/vertical separators (i,j) to minimize a misclassification cost that counts pixels whose recovered message disagrees with its expected quadrant after undoing \\(\\theta\\) and optional horizontal flip:\n  \\[\n  (\\hat\\theta,\\hat i,\\hat j,\\widehat{\\text{flip}}) = \\arg\\min_{\\theta,i,j,\\text{flip}} \\; \\mathcal{C}_{\\theta,i,j,\\text{flip}}\\big(M(x')\\big).\n  \\]\n  Apply inverse transform \\(T^{-1}_{\\hat\\theta,\\widehat{\\text{flip}},\\text{crop}}\\) to x′ before retokenization and KGW detection.\n\nExample Format:\n- Attention score: \\( a_{st} = q_s^\\top k_t \\lambda^{s-t} \\exp(i\\theta(s-t)) \\) [not used here; AR logits modified instead].\n- State update: \\( s_i = s_{i-1} + \\phi(K_i) V_i^T \\), \\( z_i = z_{i-1} + \\phi(K_i) \\) [not used; VQ tokens used].\n- Complexity: O(T|V|) for logit modification vs. O(T|V|) baseline softmax; detection adds one encoder/quantizer pass O(CNN(x)) + nearest-code argmin.\n\n**Computational_Properties:**\n- Time Complexity:\n  - Generation with watermark: per step i, computing Gi is O(|V|) to form a bitmask (PRF + mask generation can be O(|V|), but implemented as a seeded shuffle + threshold to O(|V|)); adding δ to green logits is O(γ|V|). The dominant softmax over |V| remains O(|V|). Overall AR sampling remains O(T|V|).\n  - Detection without geometry: one E′ forward + QC nearest-neighbor + counting S. Let CE′ be the encoder cost per image and Q the cost of nearest-code search; total O(CE′ + Q + T). With standard vectorized argmin, Q is linear in |V| per spatial code location and fused on GPU.\n  - Synchronization detection: localized extraction O(N) for N pixels, angle sweep over A candidates and line-fit search is O(A N). Overall sync cost O(A N) and trivially parallelizable.\n  - Finetuning (one step): forward/backward through D and E′ plus augmentation; cost O(CD + CE′), no AR transformer involved.\n- Space Complexity:\n  - Generation: negligible extra memory beyond logits (a boolean mask over V at each step can be streamed); O(|V|) temporary.\n  - Detection: store feature maps of E′ and codebook C (|V|×d); memory O(Act(E′)) + O(|V| d). Localized extraction stores probability maps O(N).\n  - Finetuning: parameters of D and E′ only; same order as original tokenizer; no storage for M.\n- Parallelization:\n  - Watermark logit boost is elementwise over the vocabulary and fully vectorizable on GPUs; context-hash PRF per step is cheap and parallel.\n  - E′ and D are convolutional; standard data-parallel and model-parallel training; augmentations are per-sample and parallel.\n  - Synchronization: angle candidates and quadrant separators can be evaluated in parallel; message extraction is per-pixel parallel.\n- Hardware Compatibility:\n  - GPU-friendly: all heavy ops are convs and vectorized reductions; nearest-code search can leverage matrix ops or faiss-like routines.\n  - JPEG within training uses straight-through estimator (no gradients through codec), implemented by stop-gradient on the residual; bandwidth-dominated but small vs CNNs.\n  - CPU inference feasible for detection but slower due to CNN encoder; GPU recommended.\n- Training vs. Inference:\n  - One-time RCC finetuning (hours on multi-GPU) vs. amortized inference; no AR transformer retraining.\n  - At inference, two optional paths: (a) no-geometry: E′+QC+KGW detection (fast), (b) with geometry: sync extraction + angle search + (a).\n- Parameter Count:\n  - AR model M unchanged. Trainable pieces: decoder D (same size as original) and auxiliary encoder E′ (same size as E). No increase in M’s parameters; storage impact equals one extra encoder checkpoint plus updated decoder.\n- Numerical Stability:\n  - Exact binomial test (regularized incomplete beta) avoids normal approximations on short sequences; deduplicate (context, token) pairs to maintain independence assumptions.\n  - Stratified green/red split over alive VQ codes preserves nominal γ under H0; otherwise use \\(\\gamma' = |G \\cap \\text{alive}|/n_{\\text{alive}}\\) in the test.\n  - JPEG STE prevents gradient explosions by blocking non-differentiable parts; LPIPS regularizer keeps reconstructions perceptually close, mitigating drift of D.\n  - Choice of δ trades watermark power vs. quality; δ=2 found quality-preserving, δ≥4 may degrade FID.\n- Scaling Behavior:\n  - Detection power improves with the number of (non-duplicate) tokens scored; multimodal unification sums S and T−h across modalities, giving lower p-values when either modality contributes reliable tokens.\n  - Robustness: RCC finetuning yields strong valuemetric robustness (blur/noise/JPEG) and resilience to neural compression and moderate diffusion purification; synchronization adds geometric robustness (flip/rotation/crop) at small cost in valuemetric robustness when sync is partially corrupted.\n  - Image size scales linearly in tokens (e.g., 512×512 with f=16 yields 1024 tokens); the method benefits from larger images due to more tokens without changing asymptotic complexity.\n- Implementation-critical Details / Pitfalls:\n  - Freeze E, QC, and C during finetuning to avoid breaking M’s learned token distribution and code semantics.\n  - Use augmentation curriculum A (increasing strength) with small rotations and crops to absorb minor sync-estimation errors.\n  - For synchronization, require minimum recovered area and per-pixel confidence/hamming-distance thresholds to avoid false transform estimates; assume crops that retain at least one corner (quadrant design).\n  - Avoid multiple-detector testing over many transformed variants (inflates FPR); use a single invert-then-detect pathway guided by the synchronization estimate.\n  - Always deduplicate (context, token) events across samples before computing the unified p-value.",
        "IMPLEMENTATION_GUIDANCE": "Integration_Strategy:\n- Watermarked sampling (generation-time):\n  - Insert a “greenlist logit booster” into your autoregressive sampler right before softmax. Modify the logits vector ℓ ∈ R|V| at every generation step i by adding δ to indices in the greenlist Gi.\n  - Implement greenlist Gi via a cryptographic PRNG keyed with secret key ξ:\n    - For text: seed = HMAC-SHA256(ξ, bytes(prev h text tokens)).\n    - For images: seed = HMAC-SHA256(ξ, bytes(prev h image tokens)); for h=0, seed = HMAC-SHA256(ξ, bytes(model_id || modality_id)).\n    - Draw a pseudorandom permutation of the vocabulary V and take the first ⌊γ·|V|⌋ as Gi (or stratified over alive codes; see below).\n  - Code-level hook (PyTorch): in the sampling loop, after computing logits and temperature scaling:\n    - green_idx = greenlist(context, key, γ, vocab, alive_mask)\n    - ℓ[green_idx] += δ\n    - probs = torch.softmax(ℓ / τ, dim=-1)\n  - Maintain consistent γ across all modalities to allow unified exact p-value testing later.\n  - Deduplicate scored (context, token) pairs across samples to preserve statistical soundness (store hashes of (context bytes, token id), only count first occurrence).\n\n- Reverse cycle-consistency (RCC) finetuning:\n  - Components to modify: finetune decoder D and an encoder replica E′; freeze original encoder E, quantizer QC, and codebook C.\n  - Data prep: precompute sequences of image tokens s for a large image set (e.g., ImageNet), and hard latents ˆz = C[s]. Do not use model-generated tokens to avoid distribution drift; optional: mix with a small fraction if desired.\n  - Losses:\n    - RCC loss: LRCC(s) = E_{a∼A} || ˆz − E′(a(D(ˆz))) ||^2_2\n    - Regularization loss: Lreg(s) = || D(ˆz) − D0(ˆz) ||^2_2 + LPIPS(D(ˆz), D0(ˆz))\n    - Total: L = LRCC + λ·Lreg\n  - Augmentations A for robustness (valuemetric and weak geometric); use straight-through estimator (STE) for non-differentiable ops like JPEG:\n    - x_aug = a(D(ˆz)); x′ = x_aug + (JPEG(x_aug) − x_aug).detach()\n  - Training integration (PyTorch):\n    - Wrap D and E′ in nn.Module; optimizer on D.parameters() + E′.parameters(); ensure .requires_grad(False) for E, QC, and C.\n    - Use distributed data parallel (DDP) for speed; prefetch token batches; cache ˆz tensors.\n  - Migration path:\n    - Existing VQ/VQGAN pipelines (Taming, RAR, etc.): load pretrained weights, clone E to E′, freeze E and C, finetune D and E′; no changes to the transformer or sampling code beyond the small watermark booster.\n    - Chameleon-like mixed-modal models: the same for image tokenizer; for text, apply the booster only (no tokenizer changes).\n\n- Watermark synchronization layer (post-hoc, for geometric transforms):\n  - Dependency: localized watermarking module that supports per-pixel message embedding/extraction (e.g., WAM from Sander et al.).\n  - Embedding:\n    - Embed four fixed 32-bit messages {032, 016116, 116016, 132} in four quadrants; reserve central cross (μ-pixel strips) unmarked to ease line fitting.\n    - μ = 18 for 256×256 images, μ = 36 for 512×512.\n  - Extraction + transform estimation:\n    - From the localized extractor, for each pixel keep its most probable message if p_watermark ≥ 0.5 and Hamming distance ≤6 to one of the four messages.\n    - Require coverage: ≥1 pixel per message and total marked area ≥70% of the image; otherwise skip synchronization to avoid false corrections.\n    - For rotation sweep θ ∈ [−20°, 20°] (step 1–2°), rotate the message grid back by −θ; fit best axis-aligned pair of lines (row i, col j) separating expected message pairs:\n      - Columns: (m1,m2) vs (m3,m4); Rows: (m1,m3) vs (m2,m4).\n      - Compute costs as mispositioned pixels; evaluate also horizontally flipped grid; aggregate best (i, j, flip) across pairs weighted by pixel counts; choose θ with minimum total cost.\n    - Inversion: apply estimated inverse rotation, flip if flagged, and undo crop by downscaling/padding to native resolution before tokenization for detection.\n    - Fallback: if synchronization estimate fails (low coverage or high cost), do not alter the image; run detection without synchronization.\n  - Placement in pipeline:\n    - Generation: after detokenization, embed the sync signal (keeps the original watermark intact in practice).\n    - Detection: first attempt synchronization (recover original geometry), then tokenize and run exact p-value detector.\n\n- Exact p-value detector:\n  - Use exact binomial tail p-value: p = Pr(X ≥ S | X ~ Binomial(T−h, γ)) = Iγ(S, T−h−S+1).\n  - Implementation: torch.special.betainc or scipy.special.betainc; compute in float64; back-off to mpmath if T is very large (>1e6 tokens).\n  - Unified multi-modal detection:\n    - Tokenize each suspect sample x(i) to s(i), compute S(i), T(i), h(i), deduplicate pairs, sum S, T, and h over modalities, and compute a single p-value with global γ.\n  - Alive-code stratification (image VQ with dead codes):\n    - Identify alive codes (indices used by tokenizer): scan a large corpus and collected generations; build a boolean alive_mask of size |V|.\n    - When h=0 (fixed split), sample green tokens separately within alive and dead sets to ensure exactly γ·nalive alive greens; prevents hypergeometric bias and overly conservative tests.\n\n- Framework compatibility:\n  - PyTorch: recommended; LPIPS, WAM/Localized watermarking, CompressAI, HuggingFace diffusers; DDP or FSDP for finetuning.\n  - TensorFlow/JAX: port losses and augmentations; ensure JPEG STE equivalents; for exact p-values use TensorFlow Probability or JAX/scipy.special.\n  - Crypto PRNG: use Python’s hashlib HMAC-SHA256; or libsodium/pyNACL; precompute seeds for speed.\n\n- Dependencies and optional tools:\n  - LPIPS (Perceptual loss), CompressAI (neural compression evaluation), HuggingFace diffusers (VAEs for robustness tests), WAM (localized watermarking), SciPy (betainc), PyTorch Image Ops (JPEG, blur, etc.).\n  - Security PRNG: HMAC-SHA256, AES-CTR, or ChaCha20; ensure deterministic behavior across platforms.\n  - For audio extension: MOSHI/Mimi tokenizer, multi-resolution STFT loss.\n\nParameter_Settings:\n- Watermark parameters (images):\n  - γ (green ratio): 0.25 default; range 0.2–0.35. Keep fixed across modalities for unified detection.\n  - δ (logit boost): 2 default; range 1–3 for quality-preserving; 4 for stronger robustness but expect noticeable FID increase (~+2).\n  - h (context size): text h=1; images h=0 (fixed split) or h=1; avoid h>1 (fragile to edits). For closed-tokenizer deployments, h=0 for images is acceptable.\n  - Temperature τ: keep original model setting; ensure δ adjustment is applied after temperature scaling.\n  - Alive stratified split: enforce exactly γ·nalive alive green tokens when h=0; recompute alive_mask if tokenizer or data changes.\n\n- RCC finetuning (images):\n  - Optimizer: Adam; lr = 1e-4; StepLR decay ×0.9 per epoch; epochs = 10.\n  - Batch size: 64 total (e.g., 4/GPU on 16×V100 for 256×256; 8/GPU on 8×H200 for 512×512).\n  - λ (regularization weight): 1.0 default; range 0.5–2.0. Too low (≤0.1) risks decoder drift; too high (≥10) reduces RCC gains.\n  - Augmentations A schedule:\n    - Epoch 1: identity only.\n    - Epoch 2: A1 — JPEG Q ∈ {90, 80, 70}; blur k ∈ {1, 3}; noise σ ∈ {0.005–0.02}; brighten × ∈ {1.1–1.2}; rotation ±1°; crop keep ∈ {0.8, 0.9}.\n    - Epochs 3–6: A2 — JPEG Q ∈ {80, 60, 40}; blur k ∈ {3, 5}; noise σ ∈ {0.02–0.06}; brighten × ∈ {1.2–1.4}; rotation ±1–3°; crop keep ∈ {0.5–0.9}.\n    - Epochs 7–10: A3 — JPEG Q ∈ {40, 30, 20}; blur k ∈ {5, 7, 9}; noise σ ∈ {0.06–0.1}; brighten × ∈ {1.4–2.0}; rotations same as A2.\n  - JPEG STE: x′ = x_aug + (JPEG(x_aug) − x_aug).detach()\n  - Initialization: E′ init from E0; D from D0; use weight decay 1e-6; LPIPS pretrained weights frozen.\n\n- Synchronization layer:\n  - Messages: four fixed 32-bit codes as above; μ = 18 (256×256), μ = 36 (512×512).\n  - Extraction thresholds: p_watermark ≥ 0.5; Hamming distance ≤ 6; coverage ≥ 70%; require all four messages present.\n  - Rotation sweep: [−20°, 20°], step 1° (adjust to 0.5° if images commonly rotated); flip decision via cost comparison on flipped grid.\n  - Crop handling: supports crops that preserve at least one corner; pad/downscale back to native size before tokenization.\n\n- Detector parameters:\n  - Use torch.float64; validate with SciPy betainc; fallback bigints/mpmath for T > 1e6.\n  - Dedup: maintain a Bloom filter or set of 128-bit hashes of (context, token); expect up to 5–10% duplicates for large flat regions.\n\n- Hardware-dependent settings:\n  - V100 (16GB): batch 4/GPU for 256×256; mixed precision AMP O2; disable large rotation sweeps (keep step 2°).\n  - A100/H100/H200: batch 8/GPU for 256×256; enable larger augmentation grids; run synchronization on 50 GPUs for fastest throughput (as in paper).\n  - CPU-only detection: acceptable; precompute greenlists; vectorize token counting; use NumPy/SciPy; avoid rotation sweeps >20° to keep latency reasonable.\n\n- Scale-dependent settings:\n  - Small models (|V| ≤ 2k): γ in 0.25–0.5; δ in 1–2; h=1 (images) for added security; fewer tokens reduces power, consider joint modality detection.\n  - Large models (|V| ≥ 8k): γ ≈ 0.25; δ = 2; h=0 (images) OK; ensure stratification if nalive ≪ |V|.\n\n- Audio extension (optional):\n  - Streams to watermark: first 4 RVQ streams.\n  - δ = 2; γ = 0.25; h = 0.\n  - Loss replace LPIPS by multi-resolution STFT; λ = 0.01–0.02 (FT+Augs), 0.001 (FT without augs).\n  - Augmentations: band/low/high-pass, white/pink noise (σ 0.001–0.01), smooth window 0.001–0.005, time shift 0.3–10 ms.\n  - Training: 200 epochs × 1000 steps, batch 64, 2×H200 (~1 day).\n\nApplication_Conditions:\n- Beneficial scenarios:\n  - Autoregressive image generation with VQ tokenizers (VQGAN/ImprovedVQGAN/FSQ) at 256×256 or 512×512; downsampling factor f≈16.\n  - Mixed-modal generation (text+image) where joint detection is needed to maintain power under partial content corruption (e.g., paraphrased text).\n  - Providers wanting out-of-model, zero-bit watermarks with exact p-values and robust detection after common real-world transformations (JPEG, blur, flips).\n\n- Hardware requirements:\n  - Finetuning: ≥8×H200 or 16×V100; 32–64 GPU-hours per model; RAM/GPU mem sufficient for batch 64 across devices.\n  - Detection-only: CPU/GPU optional; synchronization adds compute (rotation sweeps and per-pixel extraction).\n\n- Scale considerations:\n  - The approach becomes especially advantageous when image token counts are high (e.g., 1024 tokens for 512×512 at f=16), boosting detection power even if the green ratio degrades slightly.\n  - For short text only (≤128 tokens), use joint detection with images to maintain TPR at low FPR.\n\n- Task compatibility:\n  - Works with class-conditional, text-conditional, and multimodal autoregressive models (Taming, Chameleon 7B, RAR-XL).\n  - Not applicable to continuous-token or diffusion-AR hybrids unless you can map to discrete tokens (Limitation in paper).\n\n- Alternatives:\n  - Post-hoc image watermarks (CIN, MBRS, Trustmark, WAM) are stronger under valuemetric transforms but brittle under adversarial purification and neural compression, and lack exact-p-value guarantees.\n  - Diffusion-time watermarks (Tree-Ring, Stable Signature, Gaussian Shading) are not directly applicable to autoregressive token models.\n\n- Resource constraints:\n  - If you cannot finetune tokenizer: use Base watermark only; expect TPR≈1 at FPR 1% on clean images but poor robustness to transformations.\n  - If synchronization is too expensive: run detection over small set of candidate flips/rotations but control multiple-testing to avoid FPR inflation.\n\nExpected_Outcomes:\n- Performance improvements (images):\n  - RCC (Token Match TM) on original images improves from ~0.66 (Base) to ≥0.8 (FT variants).\n  - Watermark power: TPR ~1.0 at FPR 1% on clean images across all variants; improved power at lower FPR with finetuning.\n  - Valuemetric robustness (TPR@1% FPR, averaged):\n    - Taming: Base 0.26 → FT+Augs 0.92; FT+Augs+Sync 0.83.\n    - Chameleon: Base 0.50 → FT+Augs 0.89; FT+Augs+Sync 0.76.\n    - RAR-XL: Base 0.22 → FT+Augs 0.98; FT+Augs+Sync 0.89.\n  - Geometric robustness (with Sync):\n    - Taming: 0.82; Chameleon: 0.64; RAR-XL: 0.82.\n  - Neural compression robustness (TPR@1% FPR):\n    - Taming: Base 0.48 → FT+Augs ~0.79; Sync 0.80.\n    - Chameleon: Base 0.82 → FT+Augs ~0.88; Sync 0.86.\n    - RAR-XL: Base 0.27 → FT+Augs ~0.95; Sync 0.94.\n  - Diffusion purification robustness (e.g., t=0.1):\n    - Taming: Base 0.43 → FT ~0.70; FT+Augs ~0.70; Sync ~0.69.\n    - Similar qualitative gains on other models; note very large t excessively alters images.\n\n- Quality (FID/PSNR):\n  - FID unchanged for Base, FT, FT+Augs: ≤ baseline FID (e.g., Taming 16.7; Chameleon 19.7; RAR-XL 11.5).\n  - FT+Augs+Sync minor FID increase for Taming (~17.3) due to localized watermark; negligible on Chameleon/RAR-XL with tuned μ.\n  - PSNR between original decoder and finetuned decoder: Taming ~37.6 dB; Chameleon ~39.5 dB; RAR-XL ~29 dB (larger changes but still good visual quality).\n\n- Timeline:\n  - Immediate watermarking benefits upon enabling booster in sampling.\n  - RCC finetuning yields robustness gains after ~2–2.5 hours of training (images) on recommended hardware.\n  - Synchronization delivers geometric robustness after integrating embedder/extractor; tuning thresholds may take 1–2 days.\n\n- Trade-offs:\n  - δ ↑ improves robustness but risks quality degradation (FID increase); δ=2 is good default.\n  - Synchronization adds compute and may slightly reduce valuemetric robustness due to occasional misestimation under moderate noise; mitigated by stricter thresholds.\n  - h=0 (images) simplifies and strengthens detection but may lower security vs adaptive attackers if tokenizer internals are exposed.\n\n- Failure modes:\n  - Mis-synchronization: moderate valuemetric noise corrupts the sync signal leading to incorrect inverse transform and broken detection; detect by low coverage or high line-fit cost; skip inversion.\n  - Dead code bias: nalive ≪ |V| without stratification biases green ratio and makes tests overly conservative; fix by stratified splits.\n  - Over-finetuning: λ too low or lr too high causes decoder drift (visible artifacts), reduced PSNR, and potential quality loss; adjust λ to 0.5–2.0 and lr 1e-4.\n  - h>1: brittle watermark; low TPR under small edits; keep h ≤ 1.\n  - Combined removal attacks (destroy sync + geometric): still challenging; acknowledge limitation.\n\n- Debugging indicators:\n  - Uniform p-value distribution on unwatermarked content (≈U[0,1]); deviations indicate parameter mismatch (e.g., wrong γ) or dedup failure.\n  - TM histograms: expect shift from ~0.66 to ≥0.8 after FT; low TM implies RCC not improved.\n  - PSNR(old D vs new D): ≥35 dB for 256×256 indicates minimal decoder drift.\n  - Joint detection: p-values drop when combining text and image tokens, even if image has slightly lower green ratio, due to more tokens.\n  - Synchronization visuals: clear quadrant extraction; line-fit cost low; restored p-values (e.g., from ~0.6 to ≤1e−38 in examples).\n\n- Hardware-specific outcomes:\n  - V100: expect 2–3× longer synchronization stages vs H200; finetuning ~2 hours (Taming) at batch 64 across 16 GPUs.\n  - H200: faster finetuning (0.5–2.5 hours depending on model); larger augmentation grids feasible; synchronization sweeps at 1° steps without latency spikes.\n\nQuality Requirements:\n- Provide numeric settings:\n  - γ=0.25; δ=2; h=0 (images), h=1 (text); λ=1.0; lr=1e-4; epochs=10; batch=64; JPEG Q schedule (90→20); blur k (1→9); noise σ (0.005→0.1); rotation ±1–3°; crop keep 0.5–0.9; μ=18/36; extraction p≥0.5; Hamming≤6; coverage≥70%; rotation sweep ±20°; PSNR targets ≥35 dB.\n- Troubleshooting:\n  - If p-values on unwatermarked data skew high for long sequences, verify γ·nalive is an integer; otherwise adjust test to γ′=⌊γ·nalive⌋/nalive.\n  - If detection fails on flipped/rotated images, confirm sync embedding was applied and thresholds aren’t too strict; widen rotation sweep to ±30° only if necessary.\n  - If TM drops after FT without augmentations, add A-schedule; avoid catastrophic forgetting.\n  - If joint detection underperforms, disable noisy modality (e.g., heavily JPEG’d image) and rely on text-only detection.\n- Limitations/risks:\n  - Not applicable to continuous-token or hybrid diffusion-AR models without discrete token mapping.\n  - Attack combining sync disruption + geometric transform may defeat detection; document policy and monitor for such patterns.\n- Validation procedures:\n  - RCC validation: compute TM(s, T(D(s))) over 1000 generations; target ≥0.8 mean on originals.\n  - Statistical validation: run detector on 50k unwatermarked images; verify uniform p-value distribution; sanity-check extreme long-sequence behavior.\n  - Robustness suite: replicate blur/noise/JPEG/brighten/rotation/flip/crop/DiffPure/neural compression curves; target TPR@1% FPR per Table 2 ranges.\n  - Quality: measure FID on 50k generations; confirm no increase except minor for Sync; PSNR(old/new D) on 1000 pairs.\n- Actionable deployment notes:\n  - Keep ξ secret; rotate keys per product/version; log γ, δ, h for audit.\n  - Store alive_mask and stratified splits per tokenizer version; recompute if tokenizer changes.\n  - Instrument generation service to tag modality and token counts; maintain dedup for detector side.\n  - Offer a detection API returning exact p-value, S, T, h, γ, and modality breakdown; for joint cases, return combined and per-modality p-values."
    }
]