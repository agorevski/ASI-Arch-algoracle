[
    {
        "DESIGN_INSIGHT": "### DESIGN_INSIGHT_HIGH: [Distribution-Preserving Quantile Sampling (DPQS) – Watermark-Driven Gaussian Sampling With Indistinguishable Latents]\n- Replaces the standard latent noise draw \\(z_T \\sim \\mathcal{N}(0,I)\\) in diffusion models with a quantile-sliced sampling procedure that encodes watermark bits while provably preserving the latent distribution. The rest of the denoising and decoding pipeline is left unchanged.\n- Key mechanism: per-latent-dimension partitioning of the standard Gaussian CDF into \\(2^l\\) equal-probability bins, where an \\(l\\)-bit symbol \\(y \\in \\{0,\\dots,2^l-1\\}\\) (derived from the randomized watermark) selects the bin. Sampling is done by shifting a uniform variate within the bin and mapping via the Gaussian quantile function:\n  \\[\n  p(z_T^{(s)} \\mid y=i)=\n  \\begin{cases}\n  2^l \\cdot f(z_T^{(s)}) & \\mathrm{ppf}\\!\\left(\\tfrac{i}{2^l}\\right) < z_T^{(s)} \\le \\mathrm{ppf}\\!\\left(\\tfrac{i+1}{2^l}\\right) \\\\\n  0 & \\text{otherwise}\n  \\end{cases}\n  \\]\n  With \\(u \\sim \\mathcal{U}(0,1)\\), sampling is\n  \\[\n  z_T^{(s)} = \\mathrm{ppf}\\!\\left(u + \\frac{i}{2^l}\\right),\n  \\]\n  and inversion is\n  \\[\n  i = \\big\\lfloor 2^l \\cdot \\mathrm{cdf}(z_T^{(s)}) \\big\\rfloor.\n  \\]\n  Since \\(y\\) is uniform, the marginal recovers the original Gaussian: \\(\\sum_i p(z_T^{(s)} \\mid y=i) p(y=i)=f(z_T^{(s)})\\).\n- Fundamental difference from prior approaches: unlike Tree-Ring, which alters the latent frequency structure and thus the sampling randomness, or fine-tuning/backdoor methods that modify model parameters, DPQS encodes information entirely within the sampling step while rigorously preserving \\(z_T\\)’s distribution. This keeps the downstream denoising trajectory and output distribution unchanged, yielding performance-lossless watermarking.\n- Computationally, DPQS adds only per-element cdf/ppf calls on top of \\(O(n)\\) Gaussian sampling, keeping the same asymptotic complexity while enabling high-capacity embedding (per-dimension \\(l\\) bits) and exact invertibility via the same Gaussian CDF/PPF primitives.\n\n\n### DESIGN_INSIGHT_MEDIUM: [Cipher-Driven Watermark Randomization and Security Reduction – Performance-Lossless Under Chosen Watermark Tests]\n- Augments the DPQS sampler by replacing unknown watermark-bit statistics with a stream-cipher–randomized bitstream \\(m=E_K(s_d)\\) (e.g., ChaCha20), ensuring per-element \\(l\\)-bit symbols \\(y\\) are i.i.d. uniform. This removes any distributional bias that could leak distinguishability.\n- Key mechanism: computationally secure randomization makes the DPQS-driving sequence pseudorandom, so the induced latent \\(z_T^{(s)}=S(m)\\) is distributionally identical to \\(z_T=S(r)\\) obtained from truly random bits \\(r\\). The indistinguishability game is formalized under chosen-watermark tests: for any PPT tester \\(A\\) and key \\(K\\),\n  \\[\n  \\Big| \\Pr[A(X_s)=1] - \\Pr[A(X)=1] \\Big| < \\mathrm{negl}(\\rho),\n  \\]\n  where \\(X_s=D(Q(S(m)))\\) and \\(X=D(Q(S(r)))\\). By contradiction, a non-negligible gap \\(\\delta\\) yields a PPT distinguisher \\(A_{D,Q,S}\\) between the cipher output \\(m\\) and truly random \\(r\\):\n  \\[\n  \\Pr[A_{D,Q,S}(m)=1]-\\Pr[A_{D,Q,S}(r)=1]=\\delta,\n  \\]\n  violating stream-cipher security. Hence Gaussian Shading is performance-lossless under chosen watermark tests.\n- Difference from prior work: previous watermarking either (i) alters pixels post hoc (degrading quality), (ii) fine-tunes weights (changing model behavior), or (iii) injects structured latent artifacts (constraining randomness). Here, indistinguishability is tied to standard cryptographic assumptions, decoupling watermark capacity/robustness from any change in model parameters or output distribution.\n- This cryptographic-architectural synthesis provides a principled route to “provable performance-losslessness,” making the watermark untestable by any polynomial-time detector unless the underlying cipher can be broken.\n\n\n### DESIGN_INSIGHT_MEDIUM: [Semantic Latent Diffusion and Majority-Vote Coding – Robust, High-Capacity, Training-Free Watermarking]\n- Keeps the diffusion model architecture and weights intact, but restructures watermark placement and recovery around the latent grid. The method replaces localized texture tagging with global semantic binding by diffusely replicating watermark bits across channels and spatial locations, and recovers them via inversion rather than a learned extractor.\n- Key mechanism: “watermark diffusion” forms \\(s_d\\) by tiling the base watermark \\(s\\) across latent dimensions \\(c \\times h \\times w\\) with factors \\((f_c, f_{hw})\\). For per-dimension embedding rate \\(l\\), capacity is\n  \\[\n  k \\;=\\; \\frac{l \\cdot c \\cdot h \\cdot w}{f_c \\cdot f_{hw}^2} \\;\\; \\text{bits},\n  \\]\n  achieving, e.g., 256 bits in SD’s \\(4\\times 64\\times 64\\) latent at \\((f_c=1,f_{hw}=8,l=1)\\). Extraction inverts the ODE sampler (DDIM inversion) to estimate \\(z_T^{(s)}\\), decodes indices via \\(i=\\lfloor 2^l \\cdot \\mathrm{cdf}(z_T^{(s)}) \\rfloor\\), decrypts, and performs majority voting over the \\(f_c \\cdot f_{hw}^2\\) replicated bits.\n- Fundamental difference from prior approaches: robustness is achieved by binding information to the whole semantic latent rather than post-hoc pixel perturbations or frequency rings. Because the watermarked latent follows the exact Gaussian distribution and is fed through the unchanged denoiser/decoder, image quality and text alignment are preserved while the replication/vote scheme confers resilience to compression, filtering, resizing, and partial erasures.\n- The pipeline is plug-and-play and training-free: DPQS sampling plus any ODE-based sampler (e.g., DPMSolver, DDIM, DEIS, PNDM, UniPC) for generation; DDIM inversion for recovery. Complexity and inference latency remain effectively unchanged relative to standard diffusion sampling, aside from lightweight cdf/ppf and bitwise vote operations.",
        "EXPERIMENTAL_TRIGGER_PATTERNS": "Task_Performance_Signatures:\n- Stable Diffusion image generation parity (performance-lossless)\n  - Expect indistinguishable visual quality vs. non-watermarked SD:\n    - FID on SD v2.1: baseline 25.23±0.18 vs. Gaussian Shading 25.20±0.22; two-sample t-value ≈0.357 (below 2.101 threshold).\n    - CLIP-Score on SD v2.1: baseline 0.3629±0.0006 vs. Gaussian Shading 0.3631±0.0005; t-value ≈0.687.\n  - No finetuning or parameter edits; generation quality and diversity remain statistically unchanged across prompts.\n- Watermark detection (single-bit) with controlled FPR\n  - With FPR set to 1e-6: TPR ≈0.996–0.998 averaged over nine noises and SD v1.4/v2.0/v2.1; TPR=1.000 on clean images.\n  - With FPR set to 1e-13: TPR ≥0.99 for 8/9 noises; brightness factor=6 yields ≈0.953.\n  - Theoretical FPR via binomial model closely matches empirical FPR on 50k natural images; thresholding via B1/2(τ+1,k−τ) predicts observed FPR.\n- Traceability (multi-bit, 256 bits default; fc=1, fhw=8, l=1)\n  - Bit accuracy on clean images ≈0.9999 across SD v1.4/v2.0/v2.1.\n  - Bit accuracy under nine noises averaged ≈0.972–0.975; consistently outperforms baselines by ≈+7% vs. best alternative (RivaGAN).\n  - Scale to N=1e6 users with ≥0.99 identification accuracy in 7/9 noise settings using 1 image; brightness worst case ≈95.47% for 1 image, ≥99% with 2 images.\n- Robustness by distortion type (single image, default noise strengths in the paper)\n  - High robustness (bit accuracy typically ≥0.98; TPR ≥0.99): JPEG QF 25, Gaussian blur r=4, median filter k=7, resize to 25%+restore, salt & pepper p=0.05.\n  - Degradation patterns: random crop (60%) and Gaussian noise (σ=0.05) show the sharpest declines but remain ≥0.96 bit accuracy at default settings; brightness factor=6 is the weakest case (detection TPR ≈0.97–0.99, traceability ≈0.95–0.96).\n- Capacity–robustness trade-offs\n  - Diffusion extent improves robustness at lower capacity:\n    - fc-fhw=(1-16) 64 bits: adversarial bit acc. ≈0.996.\n    - fc-fhw=(1-8) 256 bits (default): ≈0.972 (best balance).\n    - fc-fhw=(1-4) 1024 bits: ≈0.877; fc-fhw=(1-2) 4096 bits: ≈0.730.\n  - Increasing per-dimension rate l harms robustness:\n    - l=2 (512 bits): adversarial ≈0.865; l=3 (768): ≈0.794; l=4 (1024): ≈0.733; l=5 (1280): ≈0.691.\n- Sampler invariance and inversion tolerance\n  - Denoisers (DDIM/PNDM/DEIS/UniPC/DPMSolver) show clean ≈1.0 and adversarial ≈0.963–0.972 bit accuracy.\n  - Inference vs. inversion step mismatch (10/25/50/100 vs. 10/25/50/100): bit accuracy remains ≥0.9994 in all cross-combinations.\n- Compression/inversion attacks\n  - Learned compression (Cheng/BMSHJ/VQ-VAE/KL-VAE): at ≈31 dB PSNR, expect TPR ≥0.95 and bit accuracy ≥0.92; resilience markedly better than Stable Signature at the same PSNR.\n  - Inversion attack via latent flipping: reliable extraction for flip rate <0.4; beyond ≈0.4, watermark accuracy drops and image semantics visibly change (attack becomes destructive rather than clean removal).\n- Comparative robustness vs. baselines (FPR=1e-6)\n  - Detection TPR under aggregate noises: Ours ≈0.997–0.998 vs. Tree-Ring ≈0.894–0.906; Stable Signature ≈0.496–0.505.\n  - Traceability bit accuracy (aggregate noises): Ours ≈0.972–0.975 vs. RivaGAN ≈0.899–0.912.\n- Efficiency expectations\n  - Training-free (0 additional training steps); plug-and-play at sampling time.\n  - Generation throughput ≈ baseline sampler (overhead from cdf/ppf quantiles is negligible vs. denoising steps).\n  - Extraction requires DDIM inversion (e.g., 50 steps), adding ≈1× extra denoising-time pass only for verification workflows.\n- Contextual conditions where gains are strongest\n  - Large-scale user attribution (N up to 1e6) and high-distortion distribution (JPEG, blur, resize), and when prompts are unknown at verification (empty-prompt inversion with guidance=1).\n  - Works best with ODE-based samplers; not designed for purely stochastic reverse samplers without ODE inversion.\n- NLP/QA/Reasoning benchmarks (image-only method; expect no impact)\n  - Language modeling: lambada_openai, wikitext, ptb → not applicable; expect no change (±0%).\n  - Reading comprehension: squad_completion, squad_v2, narrativeqa → not applicable; no change.\n  - Commonsense reasoning: hellaswag, piqa, social_iqa, commonsenseqa → not applicable; no change.\n  - Factual QA: arc_easy, arc_challenge, boolq, openbookqa → not applicable; no change.\n  - Context resolution: winogrande, winograd → not applicable; no change.\n  - Other tasks: swde, fda → not applicable; no change.\n\nArchitectural_Symptoms:\n- Training characteristics\n  - No training phase; loss curves are absent. A correct setup shows:\n    - No optimizer steps, no gradient updates, and identical network weights to the baseline SD.\n    - Immediate deployability; switching watermarked vs. non-watermarked generation does not change model checkpoints.\n- Runtime/memory behaviors during generation\n  - Memory footprint and GPU utilization mirror the chosen sampler’s baseline (DDIM/DPMSolver/etc.); vectorized cdf/ppf adds negligible extra memory.\n  - Throughput parity with baseline denoising (difference typically within measurement noise). No additional sampling steps are introduced.\n  - No new OOM occurrences vs. baseline at the same batch size, resolution, and sampler.\n- Runtime behaviors during extraction\n  - DDIM inversion introduces an additional ODE pass (e.g., 50 steps). Expect extraction runtime ≈ denoising runtime for the same step count.\n  - Empty prompt with guidance=1 is sufficient; mismatched inversion vs. inference steps minimally affects bit recovery (≥0.9994 observed).\n- Profiling signatures confirming distribution preservation\n  - z_T histogram/Q-Q plots match N(0, I) (e.g., KS test fails to reject null vs. baseline z_T; practitioner should see no detectable shift).\n  - Latent-driven binning statistic i=⌊2^l·cdf(z_T)⌋ is uniformly distributed on [0,2^l−1] per dimension; entropy near l bits per location confirms successful randomization.\n  - On non-watermarked images, decrypted bitstreams behave as Bernoulli(0.5); empirical FPR vs. τ aligns with binomial theory and the measured curves on 50k natural images.\n- Stability indicators\n  - Robust to sampler choice (DDIM/PNDM/DEIS/UniPC/DPMSolver) with negligible performance spread under distortions (adversarial bit accuracy ≈0.963–0.972).\n  - Robust to guidance scale disparity (user generation 2–18 vs. extraction at 1): bit accuracy ≈0.999+.\n  - No NaNs or divergence introduced; watermarking occurs only in the initial latent sampling (ppf/cdf), independent of the denoiser’s numerical stability.\n- Scaling properties and capacity tuning symptoms\n  - Increasing watermark diffusion (larger fhw and/or smaller fc) visibly improves robustness at the expense of capacity (e.g., 64-bit mode reaches ≈0.996 under noise).\n  - Increasing l (bits per latent location) increases capacity but produces monotonic robustness decline; operational sweet spot at 256 bits (fc=1, fhw=8, l=1).\n- Negative/neutral indicators to monitor\n  - Brightness-heavy edits (factor ≈6) are the weakest case: detection TPR can fall to ≈0.95; if this drops <0.9, revisit inversion hyperparameters or thresholds.\n  - Non-ODE samplers or incorrect inversion usage (e.g., using stochastic reverse chains without deterministic inversion) will degrade extraction; symptoms include sub-binomial Acc distributions and high false negatives.\n  - Key management errors (wrong stream key) yield uniformly random bitstreams and fail traceability; detection ROC collapses to chance.\n- Hardware-specific observations\n  - GPU kernel time dominated by denoising; cdf/ppf calls are vectorizable and numerically stable in FP32; CPU offload unnecessary.\n  - Under compression or crop attacks, verification remains compute-bound by inversion; no atypical memory spikes should appear beyond standard denoising buffers.",
        "BACKGROUND": "Title: Gaussian Shading: Provable Performance-Lossless Image Watermarking for Diffusion Models\n\nHistorical Technical Context:\nDiffusion-based generative models emerged as a leading paradigm for high-fidelity image synthesis, displacing earlier GAN-centric pipelines in many applications. The Denoising Diffusion Probabilistic Model (DDPM) formalized forward and reverse Markov chains transforming data to and from Gaussian noise, later accelerated by implicit ODE-based samplers such as DDIM and solvers like DPMSolver, PNDM, DEIS, and UniPC. To increase efficiency, Latent Diffusion Models (LDMs) pushed the diffusion process into a compressed latent space Z via encoder-decoder pairs, enabling text-to-image generation with strong prompt alignment via classifier-free guidance.\n\nIn parallel, digital watermarking advanced from classical spatial and transform-domain methods (DWT/DCT, SVD) toward deep watermarking and model-integrated approaches. For diffusion models, three families became standard: post-processing (inserting signals into pixel space after generation), fine-tuning-based (embedding marks via retraining decoders or implanting backdoors), and latent-representation-based (e.g., Tree-Ring patterns in frequency space). While these methods improved robustness or invisibility, they often degraded generative performance, altered model parameters, or constrained sampling randomness.\n\nThe problem space evolved with growing societal demand for attribution, copyright protection, and traceability of AI-generated images. Operators needed plug-and-play watermarking that preserves the generative distribution and user experience while offering resilience to common manipulations (compression, cropping, noise, blur) and supporting multi-bit capacity for user-level attribution. Prior solutions struggled to meet these simultaneously, motivating a distribution-preserving, training-free approach with formal guarantees.\n\nTechnical Limitations:\n- Performance degradation from watermarking: Prior post-processing and fine-tuning-based methods introduce measurable biases in FID/CLIP-Score and visible artifacts. Latent pattern methods (e.g., Tree-Ring) perturb the Gaussian noise distribution, reducing sampling randomness and diversity.\n- Computational and deployment overhead: Fine-tuning-based watermarking requires training or decoder retraining, incurring O(E·P·D) compute (epochs E, parameters P, data D), plus memory overhead for gradients and optimizer state, complicating deployment and model updates.\n- Distribution mismatch in latent space: Constraining zT away from zT ∼ N(0, I) breaks the standard LDM sampling assumptions, leading to semantic shifts and reduced image-text alignment.\n- Inefficient sampling for interval-constrained noise: Naïve rejection sampling to hit quantile-defined intervals has expected cost O(1/paccept), which grows with the number of intervals 2^l (embedding l bits per element), making it impractical at scale.\n- Robustness gaps under lossy operations: Watermarks tied to localized textures or pixel-space features are susceptible to JPEG, blur, filtering, resizing, cropping, noise injection, and compression attacks, reducing detection TPR and multi-bit accuracy.\n- Limited capacity and traceability scaling: Some methods cap watermark bits (e.g., 32–48 bits), constraining user attribution. Furthermore, false positive rates scale approximately linearly with the number of users, FPR(τ, N) ≈ N·FPR(τ), requiring careful statistical thresholding.\n\nPaper Concepts:\n- Latent Diffusion Model (LDM): A diffusion framework operating in a latent space Z. An image x ∈ ℝ^{H×W×3} is mapped to z0 = E(x) ∈ ℝ^{h×w×c}, and generated images are decoded via x = D(z0). Sampling starts from zT ∼ N(0, I), with iterative denoising to z0 and decoding x = D(z0).\n  Mathematically, forward noise addition obeys:\n  q(zt | z0) = 𝓝(zt | β_t z0, σ_t^2 I), with β_t = √α_t, σ_t^2 = 1 − α_t.\n  Reverse denoising models p_θ(zt−1 | zt) = 𝓝(zt−1; μ_θ(zt, t), Σ(zt, t)).\n\n- DDIM Inversion: An approximate inverse to the ODE-driven denoising that reconstructs latent noise from generated images. Given z′_t, one step of inversion is:\n  z′_{t+1} = √α_t z′_t + (√(1 − α_{t+1}) − √(α_t − α_{t+1})) · ϵ(z′_t, c, t),\n  enabling recovery of z′_T ≈ z_T to read watermark bits embedded in the initial latent noise.\n\n- Distribution-Preserving Sampling: A core mechanism that embeds l-bit integers y ∈ {0,…,2^l−1} per latent dimension while preserving z_T ∼ N(0, I). Let f be the Gaussian pdf, cdf its cumulative, and ppf the quantile (inverse cdf). Partition f into 2^l equal probability intervals. For y = i, sample:\n  u ∼ U(0, 1),  z^s_T = ppf(u + i / 2^l),\n  ensuring p(z^s_T) = f(z^s_T), i.e., the marginal remains exactly Gaussian. Extraction uses:\n  i = ⌊ 2^l · cdf(z^s_T) ⌋.\n\n- Watermark Diffusion and Capacity: A k-bit watermark s is spatially and channel-wise diffused across latent tensor dimensions (c × h × w). With channel diffusion factor f_c, height/width diffusion factor f_hw, and l bits per element, the effective capacity is:\n  k = l · c · h · w / (f_c · f_{hw}^2),\n  with voting across replicated blocks restoring s after inversion and decryption, improving robustness under local distortions.\n\n- Stream-Cipher-Driven Randomization (ChaCha20): To ensure known and uniform bit distribution for interval mapping, the diffused watermark s_d is encrypted with a stream key K using a computationally secure cipher (e.g., ChaCha20), producing ciphertext m that is a pseudorandom bitstream:\n  m ← E(K, s_d),  m ∼ Uniform over {0,1}^k.\n  This enables exact uniform y and distribution-preserving sampling. Security: distinguishing m from true randomness in polynomial time is computationally infeasible.\n\n- Performance-Lossless Under Chosen Watermark Tests: The indistinguishability game formalizes “no performance loss.” For any polynomial-time tester A and key K, the method is performance-lossless if:\n  |Pr[A(X_s) = 1] − Pr[A(X) = 1]| < negl(ρ),\n  where ρ is the security parameter length. Proof sketch: If distinguishable, by substituting sampling/denoising/decoding subroutines, one could distinguish the pseudorandom m from truly random r in polynomial time, contradicting ChaCha20’s security. Hence, X_s and X are indistinguishable, preserving performance.\n\nExperimental Context:\nThe evaluation prioritizes generative robustness and attribution reliability while controlling for performance preservation. Two operational regimes are tested:\n- Detection (single-bit watermarking): Determines if an image originates from the operator’s model. The statistic is bit-match count Acc(s, s′) with thresholds calibrated via the binomial model and regularized incomplete beta function. Reported as TPR at fixed very low FPR (e.g., 10^{-6} to 10^{-13}).\n- Traceability (multi-bit watermarking): Attributes images to individual users via multi-bit watermarks (256 bits in the main setting). Accuracy accounts for missed detections and misattributions, with database comparison across N users; FPR scales approximately as N·FPR(τ).\n\nRobustness is assessed under nine representative manipulations: JPEG (quality factor), random crop/drop, Gaussian blur (radius), median filter (kernel size), additive Gaussian noise (σ), salt-and-pepper noise (p), resize+restore, and brightness changes. Compression/inversion attacks are also studied using autoencoders (VQ-VAE, KL-VAE, Cheng, Bmshj) and controlled latent flipping. Performance preservation is measured statistically via t-tests on FID and CLIP-Score across multiple batches, comparing watermarked vs. watermark-free generations (e.g., SD v2.1: FID ≈ 25.20±0.22, CLIP-Score ≈ 0.3631±0.0005 with the smallest t-values among baselines). Sampling typically uses DPMSolver with 50 steps; inversion uses DDIM inversion with 50 steps and empty prompts to simulate unknown-user settings. The method demonstrates near-perfect detection/traceability under most perturbations (e.g., adversarial-average TPR ≈ 0.997–0.998; adversarial-average bit accuracy ≈ 0.972–0.975 across SD v1.4/v2.0/v2.1), high capacity (256 bits), training-free deployment, and formal performance-lossless guarantees.",
        "ALGORITHMIC_INNOVATION": "Core_Algorithm:\n- Replace the standard i.i.d. Gaussian sampling of the initial latent z_T in latent diffusion with distribution-preserving, watermark-driven quantile sampling for each latent element. No model parameters, architecture, or sampler are modified; the change is a plug-and-play replacement of the z_T sampling step.\n- Steps at embedding: (1) diffuse (tile) a k-bit watermark s across the latent tensor (c×h×w) using spatial/channel replication factors (f_hw, f_c); (2) encrypt the diffused bits with a stream cipher to get a uniformly random bitstream m; (3) for each latent element, take l bits from m to form an integer y∈{0,…,2^l−1}, draw u∼U(0,1), and set that latent element to the inverse-Gaussian-quantile at cumulative probability (y+u)/2^l; (4) denoise with any ODE-based sampler (e.g., DPMSolver, DDIM) and decode with the fixed LDM decoder.\n- Steps at extraction: (1) encode image x′, run DDIM inversion to estimate z′_T; (2) for each element, compute y′=⌊2^l Φ(z′_T)⌋ to recover l bits; (3) decrypt with the same key to get the tiled watermark; (4) collapse copies by majority vote to recover s.\n- Fundamental change vs prior latent watermarking (e.g., Tree-Ring): the latent distribution is not altered toward a pattern; instead, watermark bits only choose quantile bins inside the standard Gaussian, preserving the exact marginal N(0,1) for each latent and hence preserving generation performance.\n\nKey_Mechanism:\n- The core insight is quantile partitioning of the standard normal per latent element: assigning l-bit values to equal-probability bins and sampling uniformly within each bin keeps the marginal exactly N(0,1). Because the ciphertext bits m are pseudorandom, the induced mixture across bins equals the base distribution.\n- This distribution-preserving property makes watermarked and non-watermarked latents (and thus images) computationally indistinguishable under chosen-watermark tests, assuming a secure stream cipher. Diffusing the watermark across the entire latent tensor binds it to global image semantics, yielding robustness to common image perturbations and compression.\n\nMathematical_Formulation:\n- Latent diffusion preliminaries (latent space Z): standard generation samples z_T∼N(0,I), then denoises to z_0 with an ODE-based sampler Q(·) and decodes x=D(z_0).\n- Watermark diffusion and capacity:\n  - Let latent shape be c×h×w and per-element embedding rate be l bits. Use spatial/channel replication factors f_hw (height/width) and f_c (channels). The diffused watermark tensor s_d tiles s so that s_d has shape (l,c,h,w).\n  - Effective payload: k = l·c·h·w / (f_c·f_hw^2) bits.\n- Randomization (computationally secure stream cipher, e.g., ChaCha20):\n  - m = E_K(s_d), where K is a secret key; m is a pseudorandom bitstream, modeled as i.i.d. Bernoulli(1/2).\n- Distribution-preserving sampling for each latent element:\n  - From m, map l bits to y∈{0,…,2^l−1}. Let Φ be the CDF and Φ^{-1} the quantile function of N(0,1). Sample u∼U(0,1) and set\n    \\[\n      z_T^{(s)} = \\Phi^{-1}\\!\\left(\\frac{y + u}{2^l}\\right).\n    \\]\n    Equivalently, conditioning on y=i,\n    \\[\n      p(z_T^{(s)}\\mid y=i)=\\frac{2^l\\,\\phi(z)}{\\mathbb{1}\\{\\Phi^{-1}(i/2^l)\\le z\\le \\Phi^{-1}((i+1)/2^l)\\}},\n    \\]\n    and with y uniform, the marginal satisfies\n    \\[\n      p(z_T^{(s)})=\\sum_{i=0}^{2^l-1} p(z_T^{(s)}\\mid y=i)\\,p(y=i)=\\phi(z),\n    \\]\n    i.e., z_T^{(s)}∼N(0,1), where φ is the standard normal pdf.\n- Denoising and decoding:\n  - Use any ODE-based sampler Q (e.g., DPMSolver) to get z_0^{(s)}; output x^{(s)}=D(z_0^{(s)}).\n- Extraction via inversion and inverse mapping:\n  - Encode and invert: z′_0=E(x′), z′_T≈DDIM-Inv(z′_0). Recover per-element bin index by\n    \\[\n      y'=\\big\\lfloor 2^l\\,\\Phi(z'_T)\\big\\rfloor.\n    \\]\n  - Reassemble bits m′ from all y′, then s′_d = D_K(m′) (stream cipher decryption), and vote across replicated positions to get s′.\n- Lossless-performance guarantee (chosen watermark tests): for any PPT tester A and key K,\n  \\[\n    \\left| \\Pr[A(X^{(s)})=1] - \\Pr[A(X)=1] \\right| < \\mathrm{negl}(\\rho),\n  \\]\n  since distinguishing X^{(s)} from X implies distinguishing the pseudorandom ciphertext m from true randomness after the deterministic map S→Q→D, contradicting the security of the stream cipher.\n- Complexity:\n  - Embedding overhead: per-element Φ^{-1} call and u∼U(0,1), O(n) where n=c·h·w; denoising cost unchanged.\n  - Extraction overhead: DDIM inversion O(T·n) plus per-element Φ and decryption O(k).\n\nComputational_Properties:\n- Time Complexity:\n  - Embedding: O(n) for quantile sampling + O(k) for stream cipher generation; denoising remains O(T·C_θ) (same as base sampler, with T steps and per-step model cost C_θ).\n  - Extraction: O(T·C_θ) for DDIM inversion + O(n) for Φ evaluation + O(k) for decryption and O(k) for majority voting.\n  - Overall overhead vs. standard sampling is dominated by vectorized Φ/Φ^{-1} calls (O(n)), negligible compared to denoising.\n- Space Complexity:\n  - Same activations/memory as the chosen sampler during denoising/inversion. Additional buffers for bit tensors and tiled watermark are O(n·l) bits (typically small; e.g., l=1). No parameter storage change.\n- Parallelization:\n  - All watermark steps (tiling, stream cipher generation, per-element Φ/Φ^{-1}, binning, voting) are elementwise or blockwise and fully parallelizable on GPU/TPU. Random u can be generated in parallel. No cross-element dependencies.\n- Hardware Compatibility:\n  - Uses standard special functions (erf/erfinv or Φ/Φ^{-1}), widely available in CUDA, cuDNN, and scientific libraries; can be implemented via fast rational approximations if needed. Memory access is coalesced (contiguous latent tensors), favorable for GPUs.\n- Training vs. Inference:\n  - Training-free. At inference, embedding adds a vectorized quantile transform; extraction adds DDIM inversion (already common in editing/inversion workflows). No gradient computation is required.\n- Parameter Count:\n  - Zero change. No fine-tuning, no extra modules. The model weights (UNet, VAE encoder/decoder, text encoder) remain intact.\n- Numerical Stability:\n  - Φ^{-1}(p) is ill-conditioned near p∈{0,1}; the method samples within bins using u∈(0,1) so p=(y+u)/2^l stays away from exact 0 or 1. Practical mitigation: clamp p to [ε,1−ε] (e.g., ε=1e−7); prefer small l (e.g., l=1) to avoid extremely narrow bins. DDIM inversion mismatch in step count or sampler introduces small estimation error; empirically robust.\n- Scaling Behavior:\n  - Capacity scales linearly with latent size: k ∝ c·h·w (modulated by f_c, f_hw, and l). Runtime overhead scales O(n) with latent resolution; denoising/inversion scale as the base model. Robustness improves with more replicated votes (smaller f_c, f_hw) at the cost of payload; l can increase capacity but narrows quantile bins and may reduce robustness.\n- Implementation Pitfalls:\n  - Ensure stream cipher keystream length ≥ l·n bits; securely manage K. Use the same ODE time grid for inversion as closely as possible to generation; when unknown, moderate mismatch incurs minor accuracy loss. Use consistent broadcasting for tiling (f_hw, f_c) and an unambiguous element ordering to pack/unpack l-bit chunks.",
        "IMPLEMENTATION_GUIDANCE": "Integration_Strategy:\n- Where to integrate in Stable Diffusion-style pipelines (PyTorch/Diffusers recommended)\n  - Embedder (generation): replace the initial noise sampling z_T ~ N(0, I) with distribution-preserving, watermark-driven sampling. In HuggingFace diffusers StableDiffusionPipeline, modify/override the function that prepares the latents:\n    - For SD v1/v2 pipelines: override StableDiffusionPipeline.prepare_latents to call watermark_sample_latents(shape, dtype, device, key, watermark_bits, fc, fhw, l).\n    - Keep the remainder of the pipeline unchanged (scheduler/U-Net/decoder).\n  - Extractor (verification): add an inversion utility that uses the VAE encoder E and DDIM inversion to estimate z_T from an image. In diffusers, implement a separate module running:\n    - z0 = VAE.encode(image)\n    - zT ≈ DDIM_inversion(z0, steps=inversion_steps, scheduler=DDIMScheduler or a custom inversion loop)\n    - bits = watermark_inverse(zT, l, fc, fhw, key), then majority-vote across replicas to obtain k-bit watermark.\n- Code-level changes (PyTorch/diffusers)\n  - New module watermark.py:\n    - def diffuse_and_encrypt(watermark_bits: Tensor[bool], k: int, c, h, w, fc, fhw, key, nonce) -> ciphertext_bitstream m of length l*c*h*w:\n      - Diffuse: tile watermark_bits into shape [l, c, h, w] by first reshaping to [l, c/fc, h/fhw, w/fhw], then repeat along channel and spatial dims by factors (fc, fhw, fhw).\n      - Encrypt: ChaCha20 keystream XOR with the diffused bits (use PyCryptodome or libsodium; or implement in pure Python via cryptography.hazmat).\n    - def watermark_sample_latents(shape, l, m, device, dtype) -> z_T:\n      - Interpret each l-bit group per latent element as integer y in [0, 2^l-1].\n      - Generate uniform u ~ U(0,1) using the keystream (derive from stream cipher bytes; cast to float in (0,1)).\n      - Compute p = u + y/2^l, clamp p ∈ [eps, 1-eps]; eps = 1e-7 for float32 (1e-9 if float64).\n      - Standard normal quantile: z = sqrt(2) * erfinv(2p - 1).\n      - Reshape to latent tensor [b, c, h, w]; cast to dtype used by the sampler (float16/float32).\n    - def watermark_inverse(z_T, l, fc, fhw, key, nonce) -> recovered bits:\n      - Compute cdf: p = 0.5*(1 + erf(z_T / sqrt(2))).\n      - y_hat = floor(2^l * p) clipped to [0, 2^l-1]; convert to l-bit groups to recover ciphertext bitstream m'.\n      - Decrypt m' with the same key/nonce to get diffused bits; inverse diffuse by majority vote over fc * fhw^2 replicas per bit.\n  - In diffusers.StableDiffusionPipeline:\n    - Generation path: call watermark_sample_latents instead of torch.randn_like for the initial latents.\n    - Extraction path: implement DDIM inversion loop; avoid classifier-free guidance during inversion (use empty prompt and guidance_scale=1).\n- Compatibility considerations\n  - Works with ODE-based samplers (DDIM, DPM-Solver, DEIS, PNDM, UniPC). Do not use ancestral/stochastic samplers for images to be verified (Euler a, PLMS-SDE), because inversion assumes deterministic ODE trajectories.\n  - VAE encode/decode must match the model used for generation (same weights, scaling factors).\n  - Frameworks:\n    - PyTorch: use torch.erf/erfinv; vectorize over [B, C, H, W]. Keep quantile computations in float32 for numerical stability even under AMP; cast back to fp16 afterwards.\n    - TensorFlow/JAX: use tf.math.erf / jax.scipy.special.erfinv; identical formulas.\n- Migration path\n  - Existing watermarking: remove decoder/model fine-tuning; no training required.\n  - Existing SD deployments: wrap the latent sampler with GS sampling; add a verification service exposing an API that accepts images and returns detection/traceability results with thresholds computed for target FPR.\n- Dependencies and specialized libraries\n  - Stream cipher: ChaCha20 (recommend libsodium or PyCryptodome). Use 256-bit keys and 96-bit nonces.\n  - Optional SciPy for statistical tests (binomial tail / incomplete beta) or implement directly.\n  - No custom CUDA kernels required; relies on vectorized erf/erfinv available in standard math libraries.\n- Integration with training and optimization\n  - None required (training-free). For reproducibility, seed the cipher nonce per image (e.g., 96-bit nonce derived from image_id or counter).\n  - For batched generation, maintain a per-sample nonce and per-user watermark allocation.\n\nParameter_Settings:\n- Core watermark parameters (for SD 512x512 with latent 4x64x64)\n  - Default capacity: k = 256 bits with fc=1, fhw=8, l=1.\n  - Recommended ranges:\n    - fc ∈ {1, 2, 4}. Lower fc increases capacity but slightly reduces robustness.\n    - fhw ∈ {4, 8, 16}. Larger fhw increases replication and robustness, reduces capacity quadratically.\n    - l ∈ {1, 2}. l=1 is most robust; l=2 doubles capacity (512 bits) with small robustness drop; l≥3 reduces robustness notably.\n  - Capacity formula: k = l*c*h*w / (fc*fhw^2). For SD (c=4,h=w=64): k = l*16384/(fc*fhw^2).\n- Sampler/inversion settings\n  - Generation sampler: DPMSolver (2nd order) steps = 30–50 for 512x512. Use 50 to match reported results.\n  - Inversion: DDIM inversion steps = generation steps (recommended); acceptable mismatch: ±25 steps with negligible (<0.05%) clean accuracy drop.\n  - Guidance scale during generation: 5–15 typical; tested at 7.5. During inversion: 1.0 (empty prompt).\n- Detection thresholds (Binomial model, p=0.5)\n  - For k=256 bits:\n    - Target FPR 1e-6: τ ≈ 166 matching bits.\n    - Target FPR 1e-9: τ ≈ 174.\n    - Target FPR 1e-12: τ ≈ 182.\n  - Compute exact τ via FPR(τ) = sum_{i=τ+1..k} C(k,i)/2^k or using regularized incomplete beta; precompute and cache.\n- Initialization and numeric precision\n  - cdf/ppf computations: perform in float32; clamp probabilities p to [1e-7, 1-1e-7] for fp32 (use 1e-9 for fp64).\n  - Under AMP: disable autocast for erf/erfinv; re-enable for UNet forward passes.\n- Critical vs robust parameters\n  - Critical:\n    - Sampler determinism (use ODE samplers for generation and inversion).\n    - Inversion step count and scheduler type must align (DDIM-style ODE settings).\n    - Key/nonce consistency during decrypt.\n  - Robust:\n    - Steps in [30, 50]; sampler choice among DDIM/DPMSolver/DEIS/PNDM/UniPC.\n    - Guidance scale at generation; inversion always uses 1.0 with empty prompt.\n- Hardware-dependent settings\n  - 8 GB GPU: batch size 1–2 at 512x512, 50 steps with xFormers memory-efficient attention and attention slicing.\n  - 16–24 GB GPU (e.g., RTX 3090): batch size 4–8 at 512x512, 50 steps.\n  - Prefer fp16 UNet; keep watermark mapping ops in fp32 to avoid quantile instability.\n\nApplication_Conditions:\n- Beneficial scenarios\n  - Operators of closed diffusion APIs (no model weights released) needing copyright detection (single-bit) and user-level traceability (multi-bit).\n  - When preserving generative performance is critical (no FID/CLIP degradation).\n  - Need robustness to common post-processing: JPEG (Q≥25), resize (≥25%), moderate blur/median filters, noise, random crop/drop of moderate strength.\n- Hardware requirements\n  - Any GPU that can run standard SD inference; no extra memory for watermarking beyond minor tensors for cipher/cdf/ppf.\n  - CPU-only feasible but slower; quantile computations are vectorized and cheap.\n- Scale considerations\n  - Works for SD v1.4/v2.0/v2.1 and similar LDMs with latent Gaussian prior. Capacity/replication choices scale with latent size; for 1024x1024 (8x128x128) you can keep fc=1, fhw=16 for similar robustness and k≈256.\n- Task compatibility\n  - Text-to-image and image-to-image pipelines where the initial latent is Gaussian-sampled and the sampler is an ODE solver.\n  - Not recommended for pipelines using stochastic SDE samplers for final images to be verified (inversion mismatch degrades extraction).\n- Alternatives and when to choose\n  - Choose Gaussian Shading over Tree-Ring or post-processing watermarks when performance-lossless generation and high robustness are required without retraining.\n  - If full white-box third-party verification is required without operator keys, consider public, weaker-capacity schemes; GS assumes operator-controlled verification keys.\n- Resource constraints\n  - Ultra-low-latency settings: reduce steps to 30; GS still robust (>95% bit accuracy average under noises) while maintaining high TPR.\n\nExpected_Outcomes:\n- Performance expectations (SD 512x512, 50 steps, fc=1, fhw=8, l=1; k=256)\n  - Detection: TPR ≈ 0.997–1.000 at FPR=1e-6 on clean images; across diverse noises average TPR ≈ 0.996–0.998.\n  - Traceability: Bit accuracy on clean images ≥ 0.9999; under aggregated noises ≈ 0.973–0.975.\n  - Robustness under individual attacks (at moderate intensities shown in the paper):\n    - JPEG (QF=25): TPR ≈ 0.997–1.000; bit accuracy ≈ 0.991–0.988.\n    - Random Crop (retain 60% area): TPR ≈ 1.000; bit accuracy ≈ 0.967–0.981.\n    - Random Drop (80% area kept): TPR ≈ 1.000; bit accuracy ≈ 0.965–0.969.\n    - Gaussian Blur (r=4) / Median filter (k=7): TPR ≈ 1.000; bit accuracy ≈ 0.986/0.999.\n    - Gaussian noise (σ=0.05) / Salt-and-pepper (p=0.05): TPR ≈ 0.996–1.000; bit accuracy ≈ 0.959/0.939.\n    - Resize to 25% then restore: TPR ≈ 1.000; bit accuracy ≈ 0.998.\n    - Brightness (factor=6): TPR ≈ 0.974–0.991; bit accuracy ≈ 0.953.\n- Performance-lossless generation\n  - Statistical t-tests show no significant FID/CLIP degradation vs. baseline SD; expect t-values near 0.36 (FID) and 0.69 (CLIP) versus thresholds around 2.1.\n  - Visual quality and prompt alignment unchanged because latent distribution remains N(0, I).\n- Timelines\n  - Immediate deployment; no training. Inversion adds modest compute (same steps as generation), acceptable for offline verification or batch audits.\n- Trade-offs\n  - Increasing capacity (higher l or lower replication) reduces robustness; recommended l=1 for maximum reliability.\n  - Inversion accuracy degrades for extreme post-processing (heavy crops <40% area, extreme brightness/contrast), potentially reducing TPR/bit accuracy.\n- Failure modes and mitigations\n  - Using stochastic/ancestral samplers for generation: inversion fails. Mitigation: enforce ODE sampler for watermarked outputs.\n  - Scheduler/step mismatch between generation and inversion: mild accuracy drop; mitigate by setting inversion_steps to 50 and using DDIM inversion with the same sigma schedule.\n  - Prompt mismatch during inversion: always use empty prompt and guidance=1; do not rely on user prompts being available.\n  - Numeric instability in ppf/cdf (inf/NaN): clamp probabilities and compute in float32.\n  - Key/nonce misuse (reused nonce across images): weakens security and may create correlations; assign unique nonces (e.g., 96-bit counter).\n  - Model leakage enabling forgery via inversion: an attacker with the exact model can attempt watermark erasure by flipping latent bits; robust up to ≈40% random flips before quality breaks. Protect model weights and infrastructure.\n- Debugging indicators\n  - Clean-image extraction bit accuracy should be ≥0.999 on a test set.\n  - Kolmogorov–Smirnov test comparing z_T histogram vs. N(0,1) should not reject at α=0.05.\n  - Under controlled FPR=1e-6, empirical measured FPR on 50k vanilla images should align with theory within sampling error.\n  - If TPR is low on clean images: verify scheduler type, inversion steps, dtype for quantile ops, and key/nonce alignment.\n- Hardware-specific outcomes\n  - On RTX 3090 (24 GB), 50-step DPMSolver with batch 8 at 512x512 is practical; overhead from watermark sampling is negligible (<1%).\n  - On 8 GB GPUs, enable attention slicing and xFormers memory-efficient attention; watermark overhead still negligible.\n\nQuality Requirements:\n- Troubleshooting checklist\n  - Confirm that watermark_sample_latents replaces all occurrences of torch.randn_like in the initial latent creation for images to be watermarked.\n  - Ensure erfinv/erf use float32, with probabilities clamped to avoid ±inf.\n  - Verify ChaCha20 integration: 256-bit key, 96-bit nonce per image; derive u ∈ (0,1) from keystream bytes (e.g., uint32 / 2^32 + small epsilon).\n  - Majority vote logic: for each original bit, tally across fc*fhw^2 replicas; thresholds strictly > 50% for 1.\n  - Inversion: use empty prompt, guidance_scale=1.0; DDIM inversion steps equal to or higher than generation steps.\n- Validation procedure\n  - Generate 10k watermarked images with random prompts; compute TPR at τ for target FPR using held-out vanilla images to estimate empirical FPR.\n  - Robustness suite: apply nine perturbations (JPEG, crop, drop, blur, median, Gaussian noise, salt-pepper, resize, brightness) at published strengths; report TPR and bit accuracy.\n  - Performance neutrality: compute FID (vs. COCO val subset) and CLIPScore for 10 batches; run two-sample t-test vs. non-watermarked outputs.\n- Security/ops guidance\n  - Watermark assignment: allocate a unique k-bit watermark per user; store mappings in a secure database. Rotate keys periodically; never reuse nonces.\n  - Embed a short image ID in metadata to reconstruct nonce deterministically; do not expose keys.\n  - Verification service should output: contains_watermark (boolean at target FPR), matched_user_id (argmax bit matches), bit_accuracy, and confidence.\n\nThis guidance enables a drop-in, training-free watermark for diffusion models that preserves generation performance while achieving strong detection and traceability robustness, with concrete settings validated on Stable Diffusion v1.4/v2.0/v2.1."
    }
]