[
    {
        "DESIGN_INSIGHT": "### DESIGN_INSIGHT_HIGH: [Invertible Audio–Message Coupling Flow (I-AMCF) – Shared-Parameter Encoder/Decoder for Robust, High-Capacity Audio Watermarking]\nConventional DNN watermarking uses separately trained encoder and decoder networks, which breaks bijectivity and yields suboptimal capacity/imperceptibility trade-offs under attacks. WavMark replaces this with a single invertible neural network (INN) that jointly transforms audio and message in the STFT domain, using exactly the same parameters for encoding and decoding.\n\nKey mechanism: the model lifts audio to a complex-valued spectrogram \\(x_{\\text{spec}}=\\Gamma_{\\text{STFT}}(x_{\\text{wave}})\\in\\mathbb{R}^{B\\times 2\\times W\\times H}\\) and linearly lifts a \\(K\\)-bit message to waveform length then to the same spectrogram shape \\(m_{\\text{spec}}=\\Gamma_{\\text{STFT}}(\\Gamma_{\\text{FC}}(m_{\\text{vec}}))\\). It then stacks \\(n\\) invertible blocks with cross-branch coupling:\n- Forward (encode):\n\\[\nx_{l+1}=x_l+\\phi(m_l),\\quad\nm_{l+1}=m_l\\odot \\exp(\\sigma(\\rho(x_{l+1})))+\\eta(x_{l+1})\n\\]\nwhere \\(\\phi,\\rho,\\eta\\) are dense CNNs, \\(\\sigma\\) is sigmoid, and \\(\\odot\\) is element-wise product. After the final block, the message branch is discarded and \\(\\hat{x}_{\\text{wave}}=\\Gamma_{\\text{ISTFT}}(x^{n}_{\\text{spec}})\\) is emitted.  \n- Backward (decode): given only watermarked audio, the message branch is initialized with \\(z\\sim \\mathcal{N}(0,I)\\) and inverted exactly:\n\\[\nm_l=(m_{l+1}-\\eta(x_{l+1}))\\odot \\exp(-\\sigma(\\rho(x_{l+1}))),\\quad\nx_l=x_{l+1}-\\phi(m_l)\n\\]\nFinally, \\(\\hat{m}_{\\text{vec}}=\\Gamma_{\\text{FC}}^{-1}(\\Gamma_{\\text{ISTFT}}(\\hat{m}_{\\text{spec}}))\\).\n\nFundamental difference from prior encoder–decoder pairs [11,12]: the bijective coupling makes decoding the mathematical inverse of encoding, guaranteeing consistent gradients and symmetry in robustness to distortions. Parameter sharing eliminates encoder–decoder mismatch, enabling substantially higher payload (32 bps) at high imperceptibility (SNR > 36 dB). The formulation implements additive coupling on the audio branch and affine coupling on the message branch with data-dependent scaling \\(\\exp(\\sigma(\\rho(\\cdot)))\\), yielding stable, well-conditioned inverses under common signal attacks without bespoke hand-engineered embedding rules.\n\n\n### DESIGN_INSIGHT_HIGH: [Shift-Augmented Brute-Force Detection (SA-BFD) – Self-Localization Without Synchronization Codes]\nTraditional watermark localization relies on explicit sync codes, which are brittle to desynchronization (e.g., time-stretch), creating a weak link in end-to-end neural watermarking. WavMark replaces external synchronization with a self-localizing Brute-Force Detection (BFD) procedure, enabled by training-time shift augmentation that makes the decoder tolerant to positional error.\n\nKey mechanism: partition the \\(K\\)-bit message into pattern bits \\(p\\) and payload \\(q\\). At inference, slide a 1-second decoding window with step \\(\\Delta\\) over the utterance and run the same INN decoder at each offset \\(t\\), yielding \\(\\hat{p}_t,\\hat{q}_t\\). Select the best alignment by pattern-wise similarity:\n\\[\nt^\\star=\\arg\\max_{t}\\; S(p,\\hat{p}_t),\\quad \\text{then output } \\hat{q}_{t^\\star}\n\\]\nwhere \\(S\\) is e.g., Hamming similarity. Complexity is linear in the number of windows; with \\(\\Delta=0.05\\) EUL and a decoder tolerance of \\(\\pm 0.10\\) EUL, only 20 trials per EUL are needed to localize robustly.\n\nFundamental enabler vs. prior work: a dedicated shift module \\(\\Gamma_{\\text{shift}}(s)\\) is inserted during training to randomly offset the watermarked segment by \\(s\\sim \\mathcal{U}[0,0.1\\,\\text{EUL}]\\) via truncation–concatenation, forcing the INN to decode under misalignment. Unlike sync codes, this end-to-end approach co-trains localization and decoding within the same invertible model, preserving robustness under time-scale changes and resampling. Empirically, SA-BFD approaches an Oracle position with only marginal BER increase and avoids catastrophic failures seen with classic synchronization under time-stretch attacks.\n\n\n### DESIGN_INSIGHT_MEDIUM: [Adaptive Attack-Sampling Curriculum with Perceptual Adversarial Constraint – Minimax Training for Worst-Case Robustness]\nInstead of fixed or uniformly sampled corruptions, WavMark introduces an attack simulator with adaptive sampling weights proportional to validation BER per attack, prioritizing hard cases. Only one attack is applied per item, but item-wise sampling allows diverse corruptions within a batch, stabilizing gradients. A three-stage curriculum eases optimization: (1) learn embedding without attacks under weak perceptual constraints; (2) introduce attacks to build robustness; (3) tighten perceptual constraints to suppress artifacts.\n\nThe joint objective integrates message fidelity, perceptual similarity, and adversarial indistinguishability:\n\\[\n\\begin{aligned}\nL_m &= \\|m_{\\text{vec}}-\\hat{m}_{\\text{vec}}\\|_2^2 = \\big\\|m_{\\text{vec}}-f^{-1}(\\Gamma_{\\text{attack}}(\\Gamma_{\\text{shift}}(f(x,m)),z))\\big\\|_2^2,\\\\\nL_a &= \\|x-\\hat{x}\\|_2^2 = \\|x-f(x,m)\\|_2^2,\\qquad\nL_g = \\log\\big(1-d(\\hat{x})\\big),\\\\\nL_{\\text{total}}&=\\lambda_a L_a + L_m + \\lambda_g L_g\n\\end{aligned}\n\\]\nwhere \\(d(\\cdot)\\) is a 1D-CNN discriminator trained with \\(L_d=\\log(1-d(x))+\\log(d(\\hat{x}))\\). This turns training into a minimax game over perceptual quality while dynamically steering data augmentation toward the current worst-case corruptions, improving average and tail robustness across noise, filtering, resampling, compression, and time-warping.\n\nAt inference, a lightweight “repeated encoding” loop further boosts robustness under loud or dense content: re-apply the INN encoder on a segment until an SNR target is met (e.g., re-encode while SNR > 38 dB), which increases embedding energy just enough to pass the discriminator while preserving global imperceptibility (SNR > 36 dB). This complements the training-time curriculum with a practical, content-adaptive robustness control.",
        "EXPERIMENTAL_TRIGGER_PATTERNS": "Task_Performance_Signatures:\n- Audio watermarking benchmarks and metrics\n  - Segment-based, 1 s EUL, 16 kHz, 32 bps capacity\n    - Expect SNR ≈ 38.5 dB and PESQ ≈ 4.32\n    - Mean BER ≈ 2.35% across 10 attacks (No Attack ≈ 0.65%; RN ≈ 2.16%; SS ≈ 1.32%; LP ≈ 1.46%; MF ≈ 6.41%; RS ≈ 1.58%; AS ≈ 0.71%; LC ≈ 1.19%; QTZ ≈ 3.40%; EA ≈ 0.82%; TS ≈ 4.65%)\n    - Performance pattern: MF and TS are the hardest attacks; BER will concentrate in these two (MF ≈ 6–7%, TS ≈ 4–5%) while others remain ≤ 2% typically\n  - Segment-based capacity sweep\n    - 2 bps: SNR ≈ 35.7 dB, PESQ ≈ 4.21, Mean BER ≈ 0.08%\n    - 9 bps: SNR ≈ 31.2 dB, PESQ ≈ 4.09, Mean BER ≈ 0.12%\n    - Pattern: increasing bps degrades robustness; 32 bps trades to ≈ 2–3% mean BER while maintaining high imperceptibility\n  - Utterance-based, 10–20 s hosts, repeated segments, BFD localization, pattern=10 bits, payload=22 bits\n    - Expect SNR ≈ 36.85 dB, PESQ ≈ 4.21\n    - Mean BER ≈ 0.50% (no-clip) and ≈ 0.48% (clip 0–1 s from start)\n    - Attack-specific BER (clip): RN ≈ 0.35%; SS ≈ 0.00%; LP ≈ 0.00%; MF ≈ 1.57%; RS ≈ 0.22%; AS ≈ 0.00%; LC ≈ 0.00%; QTZ ≈ 0.09%; EA ≈ 0.00%; TS ≈ 1.92%\n    - Compared to Audiowmark baseline: similar capacity (≈ 20–32 bps), SNR within ±1.6 dB, but ≥ 28× robustness gain (Mean BER ≈ 0.48% vs 14.08% in clip; TS attack: ≤ 2% vs ≈ 50%)\n  - Synthetic audio outputs (unseen during training)\n    - VALL-E: SNR ≈ 36.4 dB, PESQ ≈ 4.04, Mean BER ≈ 0.07% (TS ≈ 0.30%)\n    - Spear-TTS: SNR ≈ 37.3 dB, PESQ ≈ 4.20, Mean BER ≈ 0.02% (TS ≈ 0.15%)\n    - MusicGen: SNR ≈ 37.9 dB, PESQ ≈ 4.34, Mean BER ≈ 0.00% across attacks\n    - Pattern: near-zero BER on synthetic speech/music; minor residual errors may appear under TS only\n  - Domain-specific robustness (segment-based 32 bps)\n    - LibriSpeech: Mean BER ≈ 0.29%, PESQ ≈ 4.27, SNR ≈ 39.8 dB\n    - CommonVoice: Mean BER ≈ 1.10%, PESQ ≈ 4.25, SNR ≈ 39.9 dB\n    - AudioSet: Mean BER ≈ 4.18%, PESQ ≈ 4.33, SNR ≈ 34.9 dB\n    - FMA: Mean BER ≈ 3.84%, PESQ ≈ 4.40, SNR ≈ 39.6 dB\n    - Pattern: hardest on high-amplitude non-speech (AudioSet, FMA), especially MF/TS; repeated encoding (target SNR < 38 dB) recovers robustness without audible artifacts (post-encode SNR > 36 dB)\n  - Localization effectiveness (1 s watermark in 3 s audio)\n    - Oracle decode: Mean BER ≈ 2.58%\n    - BFD: Mean BER ≈ 3.12% (within ≈ 0.5% absolute of Oracle on average); TS ≈ 6.51%\n    - SyncCode baseline: Mean BER ≈ 11.83%; TS ≈ 42.71% (severe failure)\n    - Pattern: BFD remains within ≈ 1–2% of Oracle on most attacks and dramatically outperforms SyncCode, especially under TS\n  - Shift tolerance and detection stride\n    - Reliable decoding when window offset ≤ 10% EUL; BER ≈ 0–1% with low variance\n    - With 5% EUL slide step, ≤ 20 detections per EUL suffice for near-Oracle recovery\n    - BER degrades toward random guessing as shift exceeds ≈ 35% EUL\n  - Efficiency expectations\n    - Encoding/decoding throughput: ≈ 54× real-time (GPU A100), ≈ 7.7× real-time (CPU EPYC 7V13)\n    - BFD scanning at 5% EUL step: ≈ 0.38× real-time on CPU; negligible impact on GPU throughput relative to encoding\n- Comparative expectations versus prior DNN watermarking\n  - Versus RobustDNN (≈ 1.3 bps): at ≤ 2 bps, expect ≥ 10 dB SNR gain and lower BER on MF/RS/TS with similar or slightly higher capacity\n  - Versus DeAR (≈ 8.8–9 bps): at 9 bps, expect SNR +5 dB, PESQ +0.36, and lower BER across almost all attacks (QTZ parity or slight regression possible)\n- Contextual conditions and scale dependencies\n  - Improvements are most pronounced:\n    - On longer utterances (10–20 s) with repeated segments and BFD localization\n    - Under desynchronization attacks (TS), where traditional sync mechanisms fail\n    - On synthetic audio outputs, due to their spectral consistency\n  - Trade-offs:\n    - Higher capacity (32 bps) can sustain high SNR (≈ 36–39 dB) but yields mean BER ≈ 2–3% on single segments; redundancy and error correction across segments bring utterance-level BER to ≈ 0.5%\n    - Adding the shift module slightly degrades SNR (≈ 1.5 dB) and increases segment BER (≈ +1.3%) but enables robust localization and overall better utterance-level robustness\n- Non-applicable NLP tasks (for completeness)\n  - Language modeling (lambada_openai, wikitext, ptb), reading comprehension (squad_completion, squad_v2, narrativeqa), commonsense reasoning (hellaswag, piqa, social_iqa, commonsenseqa), factual QA (arc_easy, arc_challenge, boolq, openbookqa), context resolution (winogrande, winograd), other (swde, fda): not applicable to audio watermarking; expect no measurable changes relative to baselines\n\nArchitectural_Symptoms:\n- Training characteristics\n  - Curriculum learning manifests as:\n    - Stage 1 (no attacks, weak perceptual constraint): rapid reduction in message loss (Lm) with minimal instability; audible noise may be present\n    - Stage 2 (attacks on, weak perceptual constraint): increased variance in Lm per-attack; validation BER reveals hard-attacks (MF, TS) dominating errors\n    - Stage 3 (strong perceptual constraint): La decreases sharply; SNR rises into ≈ 36–39 dB range; audible noise disappears; overall Mean BER stabilizes\n  - Weighted attack sampling behavior:\n    - Sampling probabilities for MF and TS increase after each validation cycle; subsequent epochs show monotonic BER decreases for these attacks until plateau\n    - Validation curves across attacks flatten toward uniform BER distribution (attack difficulty equalization)\n  - Stability indicators:\n    - Smooth convergence without NaNs over ≈ 69k steps total; adversarial terms (Lg/Ld) remain bounded and oscillate within stable ranges\n    - Training remains stable across modest λa, λg adjustments; invertible shared-parameter encoder/decoder prevents encoder–decoder mismatch instabilities common in separate-net designs\n- Runtime/memory behaviors\n  - Model size: ≈ 2.5M parameters; ≈ 43% parameters in message FC layers; low memory footprint permits large batch sizes without OOM on V100/A100\n  - Throughput patterns:\n    - Encoding and decoding times are symmetric (shared INN), with decoding cost ≈ encoding\n    - BFD adds a predictable linear factor proportional to number of sliding windows (e.g., 20 windows per EUL at 5% stride)\n  - Memory scaling:\n    - Constant memory per 1 s EUL segment (2×501×41 STFT feature maps); linear growth with batch size and number of detection windows; no quadratic growth with audio length since processing is per-segment\n  - Hardware utilization:\n    - High GPU utilization during STFT/ISTFT and CNN blocks; I/O-bound effects minimal; CPU-only runs achieve ≈ 7.7× real-time indicating efficient cache/memory bandwidth use\n- Profiling signatures that indicate the method is working\n  - Shift robustness observable during validation: BER remains ≤ 1% for offsets ≤ 10% EUL; sharp BER inflection beyond ≈ 10% offset\n  - Localization profile: BFD similarity scores show a dominant peak at the true segment ± one detection stride; SyncCode fails to peak under TS; BFD peak persists\n  - Repeated encoding trigger:\n    - Segments with initially high SNR (> 38 dB) are re-encoded; after 1–3 repeats, SNR converges to 36–38 dB and BER under MF/TS drops noticeably; absence of repeats correlates with elevated BER on high-amplitude music/event segments\n  - Domain effects:\n    - Speech-dominant batches yield lower BER variance; music/event-heavy batches show more repeats and higher initial BER that diminishes after weighted-attack adaptation\n- Potential neutral/negative effects to watch\n  - Enabling the shift module reduces SNR by ≈ 1.5 dB and increases segment Mean BER by ≈ 1.3% (trade for localization robustness)\n  - On fully muted segments, watermarked outputs exhibit audible noise; mitigation: skip encoding on segments with post-encode SNR < 25 dB\n  - Under extreme desynchronization (shift > 35% EUL) or very aggressive TS, BER approaches random guessing without sufficient redundancy/BFD stride refinement\n  - As capacity grows via larger K, parameter share in FC layers rises (> 43%); if unaddressed, may impact memory efficiency; remedy via parameter-free up/down-sampling for message maps",
        "BACKGROUND": "Title: WavMark: Watermarking for Audio Generation\n\nHistorical Technical Context:\nFor nearly three decades, audio watermarking was dominated by signal-processing approaches that embedded low-capacity, hand-crafted marks into the time or frequency domain. Canonical methods include Least Significant Bit (LSB) substitution in the waveform, echo hiding, spread spectrum embedding, patchwork schemes that manipulate statistical relations of subbands, and Quantization Index Modulation (QIM). These techniques rely on expert-designed heuristics and deterministic transforms, typically achieving limited payloads and being fragile to common audio manipulations like re-sampling, compression, filtering, and time-scale modifications.\n\nWith the rise of deep learning, convolutional neural networks (CNNs) and encoder–decoder architectures began to augment watermarking in vision (deep steganography and watermarking), later extending to audio. Early DNN-based audio watermarking systems adopted a three-block paradigm—Encoder → Attack Simulator → Decoder—trained end-to-end to learn robust embeddings against predefined distortions. However, these systems usually trained separate encoder and decoder networks, lacked principled invertibility, and only supported decoding when the watermark segment’s position was known, thus requiring external synchronization codes for localization. Synchronization codes (e.g., Barker sequences) from classical audio watermarking were commonly prepended, but they are brittle under desynchronization attacks such as time stretch/speed changes, and integrate poorly with end-to-end learned systems.\n\nConcurrently, invertible neural networks (INNs), exemplified by NICE, Real NVP, and Glow, provided exact, learnable bijections between data and latent spaces, enabling rigorous forward/backward transformations with shared parameters. INNs saw success in image translation, super-resolution, steganography, and speech synthesis (WaveGlow, Glow-TTS), motivating their application to watermarking: encoding and decoding are mathematically reciprocal, suggesting that a single invertible model can unify both directions and improve fidelity and robustness. The rapid advances in zero-shot voice cloning and generative audio (e.g., VALL-E, Spear-TTS, MusicGen) heightened the need for proactive, robust watermarking that is imperceptible, high-capacity, easy to localize, and resilient across diverse audio domains, including synthetic content.\n\nTechnical Limitations:\n- Low Capacity and Imperceptibility Trade-offs: Prior DNN-based audio watermarking typically achieved <10 bps capacity (e.g., 1.3 bps and 8.8 bps) with moderate or poor imperceptibility (often SNR < 30 dB). Increasing payload degraded audio quality or robustness, revealing fundamental limitations in non-invertible encoder–decoder designs.\n- Non-invertibility and Approximation Error: Separate encoder f and decoder g with independent parameters are not guaranteed inverses. Under attacks a(·), decoding computes \\(\\hat{m} \\approx g(a(f(x, m)))\\), introducing compounding approximation error. This mismatch reduces decoding accuracy and stability, especially under strong or unmodeled distortions.\n- Fragile Localization via Synchronization Codes: External synchronization codes enable localization but are vulnerable to desynchronization attacks (e.g., time stretch), leading to significant BER spikes (e.g., >40% under TS in tests). They complicate deployment pipelines and break the end-to-end learned paradigm.\n- Training Instability Across Heterogeneous Attacks and Domains: Attack difficulty varies widely (noise vs. lossy compression vs. time stretching). Uniform sampling causes imbalance in learning, and training on single-domain, sub-1k-hour datasets limits generalization to unseen audio (e.g., music, events, synthetic speech), creating domain shift issues.\n- Memory/Parameter Scaling in Message Mapping: Mapping a \\(K\\)-bit message to waveform length \\(L\\) via linear layers \\(\\Gamma_{FC} \\in \\mathbb{R}^{L \\times K}\\) yields \\(O(LK)\\) parameters. For \\(L=16{,}000\\), \\(K=32\\), each linear layer has ≈0.512M parameters, and two such layers contribute ≈43% of the total 2.5M parameters. Capacity scaling increases memory footprint and deployment cost.\n- Real-time and Detection Complexity: Naïve sliding-window localization has \\(O(T/\\Delta)\\) decoding attempts for utterance length \\(T\\) and step \\(\\Delta\\). Without model-level tolerance to positional shifts, practical detectors either run slowly or miss segments after edits. Efficient detection requires reducing attempts per Encoding Unit Length (EUL) and ensuring robustness to offsets.\n\nPaper Concepts:\n- Invertible Neural Network (INN) Block: A bijective transformation with shared parameters for encoding and decoding. For spectrogram-domain inputs \\(x_l\\) (audio branch) and \\(m_l\\) (message branch), the forward (encoding) is\n  \\[\n  x_{l+1} = x_l + \\phi(m_l), \\quad\n  m_{l+1} = m_l \\odot \\exp(\\sigma(\\rho(x_{l+1}))) + \\eta(x_{l+1}),\n  \\]\n  and the backward (decoding) is\n  \\[\n  m_l = (m_{l+1} - \\eta(x_{l+1})) \\odot \\exp(-\\sigma(\\rho(x_{l+1}))), \\quad\n  x_l = x_{l+1} - \\phi(m_l).\n  \\]\n  Here \\(\\phi, \\rho, \\eta\\) are CNN-based dense modules; \\(\\sigma\\) is sigmoid; \\(\\odot\\) is Hadamard product. Intuitively, the message perturbs audio features via \\(\\phi\\), while audio-adaptive scaling/shifting of \\(m\\) via \\(\\rho,\\eta\\) ensures recoverability under attacks.\n- Encoding Unit Length (EUL): The fixed temporal unit for embedding, set to 1 second at 16 kHz. Input waveform \\(x_{\\text{wave}} \\in \\mathbb{R}^L\\) (\\(L=16{,}000\\)) is converted to a complex spectrogram via STFT:\n  \\[\n  x_{\\text{spec}} = \\Gamma_{STFT}(x_{\\text{wave}}), \\quad x_{\\text{spec}} \\in \\mathbb{R}^{B \\times 2 \\times W \\times H},\n  \\]\n  with Hamming window size 1000, hop 400, yielding \\(2 \\times 501 \\times 41\\) feature maps (magnitude/phase channels). The EUL is the stride for iterative embedding over long utterances.\n- Brute Force Detection (BFD) with Pattern Bits: A localization method that slides a decoding window and attempts recovery at each offset. The message \\(m \\in \\{0,1\\}^K\\) is partitioned into pattern bits \\(p\\) and payload \\(q\\). A detection is valid if the decoded \\(\\hat{p}\\) matches \\(p\\). With a decoding step of 5% EUL and a shift-tolerant model (±10% EUL), only ≈20 attempts per EUL are required, dramatically reducing detection complexity. Formally, for offsets \\(\\{s_i\\}\\), choose\n  \\[\n  i^\\star = \\arg\\max_i \\ \\text{sim}(\\hat{p}(s_i), p),\n  \\]\n  then compute BER on \\(\\hat{q}(s_{i^\\star})\\).\n- Shift Module: A training-time augmentation that randomizes the decoding window by a temporal shift \\(s \\in [0, 0.1 \\cdot \\text{EUL})\\), implemented via truncation/concatenation. It teaches the INN to decode under positional misalignment. Let \\(\\Gamma_{\\text{shift}}(x')\\) denote the shifted watermarked audio; decoding enforces \\(f^{-1}(\\Gamma_{\\text{attack}}(\\Gamma_{\\text{shift}}(x'), z)) \\approx m\\). Empirically, BER remains near 0 when shift <10% EUL and degrades beyond that, enabling robust localization with coarse sliding steps.\n- Weighted Attack Simulator: A training component applying one of ten attacks per batch—Random Noise, Sample Suppression, Low-pass, Median Filter, Re-sampling, Amplitude Scaling, Lossy Compression (MP3), Quantization, Echo Addition, Time Stretch. Sampling weights \\(w_a\\) are updated from validation BERs to focus training on harder attacks:\n  \\[\n  w_a \\propto \\text{BER}_a \\quad \\Rightarrow \\quad a \\sim \\text{Categorical}(w).\n  \\]\n  This balances learning across heterogeneous distortions and stabilizes robustness.\n- Curriculum Learning for Watermarking: A three-stage schedule controlling perceptual and robustness constraints. Stage 1: no attacks, weak perceptual loss \\((\\lambda_a=100, \\lambda_g=10^{-4})\\) to learn core embedding; Stage 2: introduce attacks (8k steps) to build resilience; Stage 3: strong perceptual constraints \\((\\lambda_a=10^4, \\lambda_g=10)\\) with reduced LR to ensure inaudibility. The total loss is\n  \\[\n  L_{\\text{total}} = \\lambda_a \\|x - x'\\|_2^2 + \\|m - \\hat{m}\\|_2^2 + \\lambda_g \\log(1 - d(x')),\n  \\]\n  jointly optimized with the discriminator loss \\(L_d\\).\n\nExperimental Context:\nThe evaluation targets a generative watermarking task with emphasis on robustness, capacity, imperceptibility, and practical localization. Two regimes are used: segment-based (single EUL) and utterance-based (10–20 s audio) with repeated segments to provide redundancy. Robustness is stressed through ten common attacks spanning additive noise, filtering, resampling, compression, desynchronization (time stretch), and temporal edits (echo, suppression). Imperceptibility is measured by Signal-to-Noise Ratio (SNR) and PESQ; decoding accuracy by Bit Error Rate (BER). Capacity (bits per second) is tracked to quantify payload efficiency.\n\nThe paper compares against prior DNN systems (1.3 bps and 8.8 bps baselines) and a SOTA open-source industrial tool (Audiowmark, ≈20 bps practical capacity), focusing on accuracy vs. efficiency trade-offs: high capacity (32 bps) without sacrificing inaudibility (SNR ≈ 36–39 dB, PESQ ≈ 4.2–4.3) and with superior robustness (e.g., average BER 0.48% over ten attacks in utterance tests, ~28× lower than Audiowmark at 14.08%). The localization philosophy replaces fragile synchronization codes with BFD plus a shift-tolerant INN, achieving resilience even under desynchronization (e.g., Time Stretch causing BER ≈ 6.51% for BFD vs. ≈42.71% for SyncCode in locating tests). Generalization is tested on synthetic audio (VALL-E, Spear-TTS, MusicGen), demonstrating near-zero BER across attacks, despite no synthetic data in training. Efficiency is reported via real-time factors: GPU encoding/decoding ≈54× faster than real-time; CPU ≈7.7×; BFD detection ≈0.38× on CPU with 5% EUL steps, acceptable for practical deployment.",
        "ALGORITHMIC_INNOVATION": "Core_Algorithm:\n- Replace the conventional dual-network “Encoder–Decoder” with a single parameter-shared invertible neural network (INN) that performs watermark embedding (forward pass) and recovery (inverse pass) in the STFT domain per 1-second Encoding Unit Length (EUL). The host waveform is transformed to a 2-channel spectrogram (magnitude/phase), and a K-bit message is linearly expanded to EUL length, then STFT’d to match the audio feature shape.\n- Per invertible block, apply additive coupling on the audio branch and multiplicative-affine coupling on the message branch, ensuring exact invertibility; the message branch is discarded after forward propagation, while the audio branch is ISTFT’d back to watermarked waveform.\n- Introduce a shift module that randomly misaligns the decode window up to 10% EUL during training to learn position-tolerant decoding, and an attack simulator that applies one of ten attacks per batch with adaptive weighted sampling (weights updated by BER).\n- Watermark localization uses Brute-Force Detection (BFD): slide a 1 s window with 5% EUL stride, decode each, check fixed pattern bits, and select the position with the highest pattern match; repeated encoding is applied segment-wise until an SNR gate is met (e.g., SNR < 38 dB) to trade imperceptibility for robustness.\n\nKey_Mechanism:\n- The key insight is that watermark embedding and recovery are reciprocal operations; using an INN with explicit additive and multiplicative-affine coupling yields a bijective mapping that preserves information, enabling robust decoding even after attacks and small temporal misalignments.\n- Training with randomized attacks (and weighted sampling) shapes the INN’s coupling functions to concentrate watermark energy in frequency-phase structures resilient to common distortions; the shift module induces tolerance to positional errors that makes BFD feasible and stable.\n- Pattern bits create a verifiable signature for localization without fragile external synchronization codes; repeated encoding increases watermark energy in constrained domains while maintaining inaudibility via SNR gating.\n\nMathematical_Formulation:\n- Audio and message representations:\n  - Waveform xwave ∈ ℝ^L (L = 16,000 at 16 kHz per EUL). STFT yields xspec = Γ_STFT(xwave) ∈ ℝ^{B×C×W×H}, C=2 (mag, phase), W=41 frames (hop=400), H=501 bins (win=1000, Hamming).\n  - Message mvec ∈ {0,1}^K (K=32). Linear expansion and STFT:\n    \\[\n    \\textstyle mspec = \\Gamma_{\\mathrm{STFT}}(\\Gamma_{\\mathrm{FC}}(m_{\\mathrm{vec}})), \\quad \\Gamma_{\\mathrm{FC}} \\in \\mathbb{R}^{L\\times K}.\n    \\]\n- INN forward (encoding) through l-th block (ϕ, η, ρ are dense 2D CNNs with dense connections):\n  \\[\n  \\textstyle x_{l+1} = x_l + \\phi(m_l),\n  \\]\n  \\[\n  \\textstyle m_{l+1} = m_l \\odot \\exp(\\sigma(\\rho(x_{l+1}))) + \\eta(x_{l+1}),\n  \\]\n  with σ(·) = sigmoid, ⊙ = element-wise product. After n blocks, discard m_n and ISTFT:\n  \\[\n  \\textstyle x'_{\\mathrm{wave}} = \\Gamma_{\\mathrm{ISTFT}}(x^{n}_{\\mathrm{spec}}).\n  \\]\n- INN inverse (decoding) with random z ∼ 𝒩(0,I) for the message branch input:\n  \\[\n  \\textstyle m_l = (m_{l+1} - \\eta(x_{l+1})) \\odot \\exp(-\\sigma(\\rho(x_{l+1}))),\n  \\]\n  \\[\n  \\textstyle x_l = x_{l+1} - \\phi(m_l).\n  \\]\n  Recover message:\n  \\[\n  \\textstyle \\hat{m}_{\\mathrm{vec}} = \\Gamma_{\\mathrm{FC}}^{-1} \\big( \\Gamma_{\\mathrm{ISTFT}}(\\hat{m}_{\\mathrm{spec}}) \\big).\n  \\]\n- Shift module (training-time misalignment): for shift s ∈ [0,0.1·EUL),\n  \\[\n  \\textstyle \\Gamma_{\\mathrm{shift}}(x'_{\\mathrm{wave}}; s) = \\mathrm{concat}\\left( x'_{\\mathrm{wave}}[sL:L],\\; x_{\\mathrm{wave}}[L:(L+sL)] \\right).\n  \\]\n- Attack simulator: apply a single attack a ∈ 𝒜 per sample; adaptive weights p(a) are updated via validation BER:\n  \\[\n  \\textstyle p_{t+1}(a) = \\frac{\\mathrm{BER}_t(a)}{\\sum_{a'\\in\\mathcal{A}} \\mathrm{BER}_t(a')}.\n  \\]\n- Objectives:\n  \\[\n  \\textstyle L_m = \\| m_{\\mathrm{vec}} - \\hat{m}_{\\mathrm{vec}} \\|_2^2, \\quad\n  L_a = \\| x_{\\mathrm{wave}} - x'_{\\mathrm{wave}} \\|_2^2,\n  \\]\n  \\[\n  \\textstyle L_d = \\log(1 - d(x_{\\mathrm{wave}})) + \\log(d(x'_{\\mathrm{wave}})), \\quad\n  L_g = \\log(1 - d(x'_{\\mathrm{wave}})),\n  \\]\n  \\[\n  \\textstyle L_{\\mathrm{total}} = \\lambda_a L_a + L_m + \\lambda_g L_g.\n  \\]\n- Localization via BFD with pattern P bits: slide windows indexed by u with stride Δ = 0.05·EUL; decode \\(\\hat{m}^{(u)}\\) and compute pattern validity score \\(s(u)\\) (e.g., Hamming similarity):\n  \\[\n  \\textstyle s(u) = \\frac{1}{P} \\sum_{i=1}^{P} \\mathbf{1}\\big( \\hat{m}^{(u)}_i = \\mathrm{pattern}_i \\big), \\quad u^* = \\arg\\max_u s(u).\n  \\]\n  Use payload bits Q = K − P from \\(\\hat{m}^{(u^*)}\\) for final decoding (threshold at 0.5 for binarization).\n- Repeated encoding gate: for each segment, if \\(\\mathrm{SNR}(x_{\\mathrm{wave}}, x'_{\\mathrm{wave}}) > \\tau\\) (τ ≈ 38 dB), re-encode on the current watermarked segment until SNR ≤ τ to improve robustness.\n\nComputational_Properties:\n- Time Complexity:\n  - STFT/ISTFT per EUL: O(W·H·log H) via FFT; practically O(L log M) with window size M and hop generating W frames.\n  - INN per EUL forward/backward: O(n·F·C_int·k^2) where n is number of invertible blocks (n=8), F=W·H, C_int is internal channel width in dense blocks, k kernel size. Decoding cost matches encoding due to invertibility.\n  - Attack simulation: linear-time operations per sample; MP3 compression dominant external cost.\n  - BFD localization for an utterance of T seconds with stride Δ = 0.05·EUL: O((20·T) · INN_decode_time + pattern check), since 20 windows per second are evaluated.\n- Space Complexity:\n  - Activations per EUL: O(B·C·W·H) for spectrograms; plus intermediate feature maps inside dense blocks (depends on growth rate of dense connections).\n  - Parameters: ≈ 2.5M total; FC layers contribute ≈ 2·L·K parameters (with L=16000, K=32 gives ≈1.024M, ~43% of total). CNN blocks account for the remainder.\n- Parallelization:\n  - Fully parallelizable over batch B and spatial W×H in convolutions; FFTs parallelize per frame/bin; attacks and shift operations are element-wise or streamable.\n  - BFD window decodings can be parallelized across candidate windows u to utilize GPU cores; reduction for argmax s(u) is negligible.\n- Hardware Compatibility:\n  - GPU-friendly due to 2D CNNs and FFTs; dense blocks benefit from high memory bandwidth. CPU execution is feasible with FFT libraries and optimized convs; reported speeds: ~54.2× real-time on A100 GPU and ~7.7× on AMD EPYC CPU for encode/decode per EUL. BFD at 5% stride observed ~0.38× real-time on CPU (detection often off-line).\n- Training vs. Inference:\n  - Training includes adversarial discriminator and attack sampling; curriculum learning stages adjust λa, λg and introduce attacks progressively, reducing instability and enabling convergence.\n  - Inference omits discriminator and attack sampling; decoding is a single INN inverse pass per window plus FC inverse and ISTFT; BFD adds sliding-window repeats.\n- Parameter Count:\n  - Scales linearly with L·K for FC layers; to reduce, replace Γ_FC/Γ_FC^{-1} with parameter-free up/down-sampling (e.g., nearest-neighbor + reshape) and project via small convs.\n- Numerical Stability:\n  - Multiplicative scaling uses \\(\\exp(\\sigma(\\cdot)) \\in (1,e)\\), preventing explosion; additive/multiplicative affine coupling ensures stable inverses.\n  - Potential saturation of σ mitigated by curriculum (weak-to-strong perceptual constraints), balanced λa, λg, and attack weighting; STFT magnitude-phase representation aids stability under filtering/resampling attacks.\n- Scaling Behavior:\n  - Larger K increases FC parameter load and decoding variance; redundancy via repeated encoding and error correction can offset BER without enlarging K.\n  - Higher sample rates (e.g., 44.1 kHz) increase L, W, H, thus compute and memory; consider multi-scale STFT and lighter dense blocks to maintain throughput.\n  - Shift tolerance trained to ~10% EUL; sliding stride at 5% EUL yields ~20 candidates per EUL—robust and efficient; increasing EUL improves payload efficiency for fixed pattern length P but raises per-window compute.\n- Implementation Pitfalls:\n  - Ensure consistent STFT/ISTFT window/hop across encode/decode; phase channel alignment is critical.\n  - Attack-weight updates should be smoothed (e.g., EMA on BER) to avoid oscillatory sampling.\n  - In muted segments, encoding produces audible artifacts; skip segments with post-encode SNR < 25 dB.",
        "IMPLEMENTATION_GUIDANCE": "Integration_Strategy:\n- Components to implement or modify\n  - Add an invertible watermarking module consisting of:\n    - InvertibleBlock (shared parameters for encoding/decoding) with ϕ(·), ρ(·), η(·) implemented as 5-layer 2D CNNs with dense connections.\n    - MessageLinearEnc (K→L) and MessageLinearDec (L→K) linear layers.\n    - STFT/ISTFT front/back ends using Hamming window.\n    - ShiftModule for training and sliding-window Brute-Force Detection (BFD) at inference.\n    - AttackSimulator with 10 attacks (RN, SS, LP, MF, RS, AS, LC, QTZ, EA, TS).\n    - Discriminator1D (4-layer 1D CNN) for adversarial perceptual constraint.\n  - Insert the watermarking module after waveform generation (post-processing stage) in TTS/audio generation pipelines; for corpus watermarking, place it in the data pre-processing pipeline before distribution.\n  - For repeated encoding, add a loop around the encoder to re-encode until SNR falls below a target threshold.\n- Code-level changes (PyTorch reference)\n  - Use torch.stft/torch.istft with window_size=1000, hop_length=400, window=torch.hamming_window(1000).\n  - InvertibleBlock forward:\n    - xl1 = xl + phi(ml)\n    - ml1 = ml * torch.exp(torch.sigmoid(rho(xl1))) + eta(xl1)\n    - Backward:\n      - ml = (ml1 - eta(xl1)) * torch.exp(-torch.sigmoid(rho(xl1)))\n      - xl = xl1 - phi(ml)\n  - Message mapping:\n    - MessageLinearEnc: nn.Linear(K, L) where L=16000; MessageLinearDec: nn.Linear(L, K).\n    - Apply STFT to both audio and message-mapped waveform to produce xspec, mspec with shape [B, 2, W=41, H=501].\n  - ShiftModule:\n    - Random s in [0, 0.1*EUL] during training; implement via slicing and concatenation: shifted = torch.cat([xw[:, s:], x_unwatermarked[:, :s]], dim=1).\n  - AttackSimulator: implement each attack as a function; during training, sample exactly one attack per item with weighted probabilities from validation BER.\n  - Losses:\n    - La = MSE(x, x’) with λa weight.\n    - Lm = MSE(m, m_hat) on decoder output after attack+shift.\n    - Lg, Ld are standard logistic GAN losses with Discriminator1D.\n  - BFD decoding:\n    - Slide a 1 s window with step = 0.05*EUL (i.e., 50 ms for 16 kHz, hop 800 samples).\n    - Decode each window; compute pattern similarity (e.g., bitwise match count/length); choose window with max similarity; compute BER on payload bits.\n  - Repeated encoding:\n    - After each 1 s segment, compute SNR(x, x’). If SNR > 38 dB, re-run encoder on x’ with same message until SNR ≤ 38 dB (cap repeats to 3 to avoid artifacts).\n- Compatibility with frameworks\n  - PyTorch/torchaudio: recommended; uses standard ops; no custom kernels required.\n  - TensorFlow/JAX: port 2D CNN blocks and STFT equivalents; ensure inverse consistency (tf.signal.stft/istft). Verify ISTFT reconstruction scale/length alignment.\n  - Audio I/O: use torchaudio or librosa; for MP3 attack use ffmpeg-python or pydub with LAME codec at 64 kbps.\n- Migration path\n  - From classical tools (e.g., Audiowmark): replace patchwork insertion with WavMark encoder; keep your error-correction layer if desired (BCH/CRC) on top of payload.\n  - From DNN encoder-decoder pairs: consolidate into a single INN with shared parameters; remove external sync code; add BFD.\n- Dependencies and hardware features\n  - GPU acceleration: CUDA/cuDNN (Ampere or Volta tested). Mixed precision (torch.cuda.amp) recommended.\n  - CPU-only: acceptable for encoding; BFD detection is ~0.38× real-time on EPYC 7V13; use PyTorch MKL backend.\n  - External: ffmpeg for MP3 attack simulation; ensure deterministic STFT/ISTFT settings across environments.\n- Training pipeline integration\n  - Three-stage curriculum training; weighted attack sampling updated per validation BER.\n  - Log SNR, PESQ, BER per attack after each stage; update attack sampling weights to emphasize harder attacks.\n\nParameter_Settings:\n- Core model\n  - Sampling rate: 16 kHz (target). If 44.1 kHz, prefer downsample to 16 kHz for current model or increase L and adjust linear layers (see hardware-dependent notes).\n  - EUL: 1.0 s (fixed). For higher payload efficiency, EUL can be increased to 2.0 s with K scaled to 64–128 bits.\n  - STFT: window_size=1000, hop_length=400, Hamming window; center=False; onesided=True.\n  - Spectrogram shape: [B, C=2, W=41, H=501].\n  - Invertible blocks: n=8 (robust default); scale n in [6, 10] depending on capacity and compute budget.\n  - ϕ, ρ, η functions: 5-layer 2D CNN with dense connections; kernel sizes 3x3; channels 32–64; growth rate 16–32.\n- Message capacity\n  - K (bits per 1 s): 32 (default), use [9, 32] depending on robustness target.\n  - Pattern bits: 8–12; recommended 10; payload bits = K - pattern_bits.\n- Loss weights and learning rates\n  - Stage 1 (no attacks): lr=1e-4, λa=100, λg=1e-4; steps 3.5k.\n  - Stage 2 (attacks on): lr=1e-4, λa=100–500, λg=1e-3–1e-2; steps 8k.\n  - Stage 3 (strong perceptual): lr=1e-5, λa=1e4, λg=10; steps ~57.85k.\n  - Optimizer: Adam (β1=0.9, β2=0.999, weight_decay=0–1e-5). Gradient clipping: 1.0–5.0 (critical to avoid instability with INN).\n- Initialization\n  - Linear layers: Kaiming uniform with a=√5; bias=0.\n  - 2D CNNs: Kaiming normal (fan_in), bias=0; last layers in ϕ, ρ, η initialized with small weights (std 1e-3) to start near identity mapping for stability.\n  - Discriminator: Kaiming normal; spectral norm optional on conv layers for stability.\n- Shift and detection\n  - Training max shift s_max = 0.1*EUL (critical).\n  - BFD step s_step = 0.05*EUL (20 detections per EUL); for lower latency, s_step up to 0.1*EUL with slight BER increase.\n- Repeated encoding\n  - SNR target after encoding: 36–38 dB; start repeat if SNR > 38 dB; cap repeats at 3; if after 3 repeats SNR still >38 dB, increase λa decay temporarily or lower K.\n- Attack sampling\n  - Initialize uniform weights; after each validation epoch, set sampling probability p_attack ∝ BER_attack (normalize to sum=1). Update per 1–2k steps.\n- Batch and training scale\n  - Batch size: 32–128 segments on 24–40 GB GPUs. For 8×V100 (16 GB), use per-GPU batch=16–32.\n  - Mixed precision: O2 (FP16) for convs; keep STFT/ISTFT in FP32 for numerical stability.\n- Hardware-dependent settings\n  - A100: channels up to 64 and n=10 blocks feasible; expect 2–3× throughput vs V100.\n  - CPU: use smaller n=6, channels=32, batch size ≤8; consider disabling MP3 attack during training to avoid I/O bottleneck (simulate compression noise instead).\n\nApplication_Conditions:\n- Beneficial scenarios\n  - Protecting synthetic speech/music (TTS, voice cloning, generative music) with segment lengths 10–20 s or longer.\n  - Environments subject to common audio transformations: resampling (0.5×–2×), slight time-stretch (0.9–1.1×), lossy compression (MP3 64 kbps), light noise (SNR ~34–35 dB).\n  - When robustness to desynchronization (TS/RS) is needed; avoids fragile external sync codes.\n- Hardware requirements\n  - Minimum: single 16 GB GPU for training; CPU-only inference acceptable.\n  - Disk/network: ffmpeg for LC attack; ensure temp storage for transcodes; consider RAM-backed tmpfs for speed.\n- Scale considerations\n  - Capacity 32 bps per second becomes advantageous for payloads ≥100 bits across 3–4 s; for longer content (10–20 s), redundancy + BFD yields BER ~0.5%.\n  - With very short utterances (<3 s), prefer larger pattern ratio (12–16 bits) or reduce K to improve detection.\n- Task compatibility\n  - Speech watermarking and audio event content: robust; music with high amplitude/less silence: use repeated encoding and stricter SNR gating.\n  - Real-time streaming: current design is chunked 1 s blocks; for sub-second latency, reduce EUL to 0.5 s and K to 16–24 bits (trade capacity for latency).\n- Alternatives\n  - Use Audiowmark if you need CPU-only, no-GPU, low complexity and can tolerate desynchronization failure (TS attacks) and higher BER (~14%).\n  - Traditional sync codes for legacy pipelines; WavMark BFD preferred when TS robustness and end-to-end learnability matter.\n- Resource constraints\n  - Low compute/memory: decrease n, channels; reduce K; disable heavy attacks during early training; adopt stage-wise curriculum to converge.\n\nExpected_Outcomes:\n- Performance expectations (16 kHz, 10–20 s hosts, K=32, pattern=10, payload=22)\n  - SNR: 36–39 dB; PESQ: 4.1–4.3.\n  - Utterance-level BER: 0.4–0.6% across 10 attacks with clipping of first segment by up to 1 s.\n  - Segment-level BER: 2–5% at 32 bps; 0.06–0.24% at 2–9 bps with relaxed λa.\n  - Detection robustness: decoding succeeds if window center within ≤10% EUL of true position; with 5% EUL step, expect correct window in ≤20 tries per EUL.\n  - Speed: encoding ~54× real-time on A100, ~7.7× on EPYC 7V13 CPU; BFD ~0.38× real-time on CPU (step=5% EUL).\n- Timeline\n  - Stage 1: fundamental encoding learned in 1–2 hours on 8×V100.\n  - Stage 2: robustness accrues over 8–12 hours; BER drops unevenly across attacks.\n  - Stage 3: imperceptibility improves over 1–2 days; SNR stabilizes >36 dB.\n- Trade-offs\n  - Higher K increases BER and lowers imperceptibility; increasing λa improves SNR but can hurt BER.\n  - Shift module introduces slight BER/SNR degradation but enables robust localization and eliminates sync-code dependency.\n  - Repeated encoding improves robustness in high-amplitude music but risks audible artifacts if overused; enforce SNR bounds.\n- Benchmark comparisons\n  - Expect ~28× BER reduction vs Audiowmark at comparable capacity (payload ~20–22 bps) and imperceptibility.\n  - TS attacks: Audiowmark often fails locate (BER ~50%); WavMark ~0–2% BER.\n- Failure modes\n  - Silent segments: encoder injects audible noise; skip segments with SNR < 25 dB post-encoding.\n  - Sample-rate mismatch: using 44.1 kHz without model adjustment leads to shape mismatches and poor decoding.\n  - Over-large detection step (>0.1*EUL): BFD misses segments; BER spikes.\n  - Excessive time-stretch (>±20%): desynchronization beyond learned robustness; BER may approach random.\n  - Attack weight imbalance: if not updated, model overfits to easy attacks; hard attacks BER stays high.\n- Debugging indicators\n  - Check STFT/ISTFT round-trip on clean audio (MSE < 1e-6 normalized); if larger, window/hop mismatch or scaling bug.\n  - Monitor per-attack BER histograms; hard attacks (MF, TS) should improve stage 2→3; if not, increase their sampling weights.\n  - Pattern similarity distribution during BFD should be bimodal; flat distributions indicate poor pattern allocation or decoding failure.\n  - SNR drift above 40 dB post-encoding signals under-encoding; trigger repeated encoding or lower λa temporarily.\n- Hardware-specific outcomes\n  - A100: can increase channels and blocks to raise capacity to K=48–64 with similar BER at cost of training time.\n  - V100: stay with K ≤32; mixed precision essential; avoid large batch sizes that cause kernel fallback.\n  - CPU: BFD latency sensitive; reduce s_step to 0.1*EUL or pre-index candidate windows via energy-based heuristics to prune.\n\nValidation procedures:\n- Unit tests\n  - Verify invertibility: run forward then backward on random xspec/mspec; MSE < 1e-6.\n  - STFT/ISTFT reconstruction on sine sweeps and speech; PESQ drop <0.05 vs original.\n- Attack suite\n  - Evaluate BER across the 10 attacks with specified parameters; confirm mean BER ≤0.6% on 10–20 s hosts.\n- Localization\n  - Compare Oracle vs BFD vs SyncCode: expect BFD within +0.5–1.5% BER of Oracle across attacks; SyncCode fails on TS (BER >35%).\n- Imperceptibility\n  - Measure SNR and PESQ across datasets; enforce SNR ≥36 dB; spot-check by listening tests and spectrogram differences (modifications subtle across time/frequency).\n- Throughput\n  - Measure encoding and BFD times on target hardware; confirm GPU encoding ≥50× real-time, CPU BFD ≥0.3× real-time.\n- Deployment checks\n  - Pattern/payload integrity: CRC or BCH on payload to detect decode errors; log pattern hit rates >95% across windows.\n  - Silent segment filter: compute SNR after encoding; skip segments with SNR <25 dB to avoid artifacts."
    }
]