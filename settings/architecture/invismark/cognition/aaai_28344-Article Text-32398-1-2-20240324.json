[
    {
        "DESIGN_INSIGHT": "### DESIGN_INSIGHT_HIGH: [Minimum External Rectangle (MER) Resynchronization – Content-Region Canonicalization for Robust Multi-Source Watermarking]\n- Replaces whole-image embedding/decoding with content-region embedding and canonicalized resynchronization to survive irregular cropping, extreme downsizing, and translation in composite images. Prior deep watermarking encoders (HiDDeN, MBRS, LIM, SSLWM) synchronize the decoder to the full image or a preselected patch; MuST replaces this with an object-centric MER pipeline that preserves synchronization even after severe pasting distortions (IPD).\n- Key mechanism: the encoder embeds a message \\(W_m \\in \\{0,1\\}^L\\) only in the minimum external rectangle of the primary object, defined by the object mask. Let \\(I_{co}\\) be the cover image and \\(M\\) its primary-object mask. The canonicalized content crop is \\(I_{mer} = \\mathrm{MER}(I_{co})\\), i.e., the tight bounding rectangle of \\(M\\). The encoder processes \\(I_{mer}\\) via Conv+SE blocks, concatenates an expanded \\(W_m\\), and produces \\(I_{en}\\). After inverse resizing, the encoded crop is reinserted into the original background to form the watermarked material. The encoder is trained with a perceptual loss over the canonical region and an adversarial discriminator over the same region:\n  \\[\n  L_{\\mathrm{ENC}} = \\mathrm{MSE}\\big(\\mathrm{MER}(I_{co}),\\,\\mathrm{MER}(I_{en})\\big),\\quad\n  L_{\\mathrm{ENC2}} = \\log\\big(1 - \\mathrm{DIS}(\\mathrm{MER}(I_{en}))\\big),\n  \\]\n  \\[\n  L_{\\mathrm{DIS}} = \\log\\big(\\mathrm{DIS}(\\mathrm{MER}(I_{en}))\\big) + \\log\\big(1 - \\mathrm{DIS}(\\mathrm{MER}(I_{co}))\\big).\n  \\]\n- Fundamental difference from prior approaches: instead of attempting to make the watermark globally invariant to geometric/noise transforms, MER establishes a spatial canonical frame tied to the object content. At inference, DET predicts \\(M\\) on the composite \\(I_{com}\\); connected-component labeling yields \\(\\{M^{(k)}\\}\\) and the decoder receives resynchronized crops \\(I^{(k)}_{\\mathrm{dis}} = \\mathrm{MER}(I_{com}; M^{(k)})\\). This explicit content-region canonicalization converts IPD transforms (scale \\(s\\), translation \\(\\tau\\), irregular boundary crops) into near-identity transforms within the MER frame, thereby preserving encoder–decoder synchronization under extreme resizing and cut-paste operations—a failure mode for prior single-image watermarking schemes.\n\n### DESIGN_INSIGHT_HIGH: [Differentiable Multi-Source Image Composing Simulation (MIC) – End-to-End Training Against Realistic Pasting and Fusion Distortions]\n- Replaces generic augmentation/noise layers with a task-specific, differentiable compositor that models the multi-source editing pipeline: object pasting (IPD) plus aesthetic fusion (IFD). Prior works rely on classical distortions (JPEG, blur, affine) or single-source shooting artifacts; MuST builds a compositor that reproduces the distortion operators intrinsic to multi-source collages.\n- Key mechanism: MIC samples a set of encoded objects \\(\\{I^{(k)}_{en}\\}\\) and a background \\(B\\), applies IPD transforms \\(\\Gamma_k\\) (irregular cropping, extreme downscaling \\(s_k\\in[0.3,0.4]\\), positional shift) and IFD transforms \\(\\Omega_k\\) (edge moving via slight crop, masked feathering via Gaussian blur with \\(\\sigma\\in[0.1,1]\\), global smoothing, brightness \\(\\alpha_k\\in[-0.2,0.2]\\), contrast \\(\\beta_k\\in[0.8,1.2]\\)). The composite is generated by masked blending:\n  \\[\n  I_{com} \\;=\\; \\left( \\sum_{k} \\Omega_k\\!\\big(\\Gamma_k(I^{(k)}_{en})\\big)\\odot M_k \\right) \\;\\;+\\;\\; B \\odot \\left(1 - \\sum_{k} \\Omega_k(M_k)\\right),\n  \\]\n  where \\(M_k\\) is the (transformed) object mask; placement is non-overlapping and messages are reordered by a known zigzag index for multi-instance loss computation.\n- Fundamental difference from prior approaches: MIC explicitly models, in a differentiable manner, the two-stage human editing workflow (pasting+fusion) that defeats classical single-image watermark robustness, aligning training to the true deployment domain. This yields robustness to severe downsizing and boundary manipulations that are underrepresented in earlier noise models, enabling end-to-end optimization of ENC/DET/DEC to the composite-generation operator \\( \\Phi_{\\mathrm{MIC}}(\\cdot)\\).\n\n### DESIGN_INSIGHT_MEDIUM: [Multi-Instance Detector–Decoder with MER Cropping and Zigzag Ordering – Scalable Source Attribution from Composites]\n- Modifies single-image decoding into a multi-instance pipeline that first detects watermark-bearing regions and then decodes per-instance messages. Prior methods either decode from a whole image or rely on preselected/salient patches (e.g., SIFT-based), lacking automated multi-source localization and instance-wise resynchronization.\n- Key mechanism: a U-Net detector outputs a probabilistic mask \\(M\\) trained with pixel-wise cross-entropy,\n  \\[\n  L_{\\mathrm{DET}} = - G_t \\log(M) - (1-G_t)\\log(1-M),\n  \\]\n  where \\(G_t\\) is the MIC-generated ground-truth mask. Connected-component labeling yields instance masks \\(\\{M^{(k)}\\}\\) and MER crops \\(I^{(k)}_{\\mathrm{dis}}\\). Instances are ordered by zigzag traversal to provide positional consistency with the embedded message set. The decoder stacks ResBlocks and SE blocks, applies GAP to obtain channel-only features, and maps to the bit vector via a linear head; training minimizes\n  \\[\n  L_{\\mathrm{DEC}} = \\mathrm{MSE}\\big(W_m,\\, W'_m\\big),\n  \\]\n  with joint optimization of encoder and decoder,\n  \\[\n  L = \\lambda_{\\mathrm{ENC}} L_{\\mathrm{ENC}} + \\lambda_{\\mathrm{ENC2}} L_{\\mathrm{ENC2}} + \\lambda_{\\mathrm{DEC}} L_{\\mathrm{DEC}}.\n  \\]\n- Difference from prior approaches: this detector–decoder design converts the multi-source tracing problem into instance-wise decoding with content-region canonicalization, avoiding brute-force search or reliance on handcrafted keypoints. It scales naturally with the number of pasted sources and maintains high PSNR/SSIM by constraining embedding to MER regions, while achieving near-constant accuracy across diverse backgrounds and object categories.",
        "EXPERIMENTAL_TRIGGER_PATTERNS": "Task_Performance_Signatures:\n- Multi-source tracing under MIC (multi-source image composing simulation)\n  - Expect SOTA extraction accuracy across pasting and fusion distortions.\n  - Downsizing (Scale Rate):\n    - 0.3–0.5: ACC ≈ 98.0–98.6% (e.g., 0.3 scale: 98.22/98.17/98.25/98.03% for 1–4 materials).\n    - 0.2 (extreme downsize): ACC ≈ 95.1–96.4% for 1–6 materials.\n  - Irregular cropping (CROP RATIO, removed portion 0.2–0.9): Maintain top-curve accuracy among baselines; expect ≥95% ACC overall and ≥98% for moderate cropping (≤0.5 removal).\n  - Image fusion distortions (single-distortion stress tests):\n    - Brightness ±50%: 98.33–98.41% ACC.\n    - Contrast ±50%: 98.79% (−50%), 98.28% (+50%).\n    - Gaussian blur σ = 0.1/1/2: 98.42%/98.38%/98.13% ACC.\n- Visual quality of watermarked materials\n  - Average PSNR 42.89 dB, SSIM 0.9883; typically ≥42.5 dB on mixed datasets and ≥44 dB on clean exemplars.\n  - Residual energy localized to object regions rather than background when the noise layer is active.\n- Generalization across material types and backgrounds\n  - Datasets: SOIM 98.06%, CASIA V2.0 96.23%, ICCV09 97.46% (avg 97.92% ACC).\n  - Backgrounds: photos 98.39%, pure color 98.03%, composed 97.83% (similar performance across types).\n- Scaling with number of materials on one canvas\n  - With 1–4 materials at scale 0.3–0.5: ACC ≈ 98% sustained.\n  - With 5–6 materials (requiring heavier downsizing): ACC ≈ 95–96% (slight drop due to smaller MERs).\n- Real-world editors (no simulated noise; Photoshop/Canva/Pixso)\n  - Photoshop: 96.67–98.33% ACC.\n  - Canva: 95.00–97.78% ACC.\n  - Pixso: 98.33–100% ACC.\n- Contribution of MER and noise layer (ablation)\n  - No MER, no noise layer: 50.84% ACC.\n  - Noise layer only: 54.43% ACC.\n  - MER only: 86.37% ACC.\n  - MER + noise layer (full MuST): 98.25% ACC.\n- Competitive pattern vs baselines (under MIC)\n  - Expect higher robustness than SSLWM and LGDR across downsizing/cropping; comparable or slightly higher than CIN under fusion distortions; visual quality superior or on par (e.g., PSNR 42.89 vs CIN 42.56, LGDR 42.10, SSLWM 35.12).\n- Degradation bounds and conditions\n  - Severe downsizing (0.2 scale) and many materials (≥5) produce small but consistent reductions: ACC remains ≈95–96%.\n  - Interconnected/overlapping materials can degrade detection/segmentation and thus extraction (not quantified in-paper, observed as a limitation).\n\nArchitectural_Symptoms:\n- Training characteristics\n  - Encoder-decoder joint training:\n    - LENC (MSE on MER regions) decreases steadily; PSNR >42 dB on validation MERs by later epochs.\n    - Adversarial loss stabilizes; DIS and ENC reach a balance without mode collapse (visuals indistinguishable; high SSIM ~0.988).\n  - Decoder:\n    - LDEC (message MSE) converges smoothly; validation bit-ACC approaches ≥98% under MIC.\n    - When the noise layer is disabled, convergence may appear normal on clean data but fails to translate to composite robustness (test ACC ≈50–54%).\n  - Detector (U-Net):\n    - LDET (BCE) drops rapidly; masks converge to near-binary confidence.\n    - Validation IoU ≈99.7% and TPR ≈100% for ≤4 materials; slight TPR decline to ≈99% at 5–6 materials (symptom of heavy downsizing).\n- Robustness/synchronization indicators\n  - Residual localization: watermark residue energy confined to the object’s MER (clear in visualization); dispersed residue into background indicates missing noise-layer effect and predicts poor post-compositing extraction.\n  - Stable resynchronization: successful connected-component extraction from DET masks followed by high-confidence per-ROI decoding; mismatch between detected ROI count and embedded count signals likely failure.\n- Runtime/memory behaviors\n  - MER-centric processing: compute and memory at decode scale with the number and size of detected MERs, not with the full canvas size; large canvases with few small objects maintain efficient inference.\n  - Consistent throughput across varying background types; small slowdowns when many tiny MERs are present due to increased per-ROI decoding.\n- Profiling signatures to watch\n  - High detector confidence maps (near 0 or 1) and IoU >99% on validation composites.\n  - Post-detection zigzag ordering stable and deterministic; reorder-induced mismatches are absent when the system works.\n  - No NaNs or divergence during adversarial training; discriminator accuracy hovers around chance on MER crops late in training, indicating good perceptual quality.\n- Negative/neutral effects\n  - With extreme downsizing or tightly touching/overlapping objects, detector confidence margins shrink; minor under-segmentation can appear, correlating with slight ACC drops.\n  - Removing MER or the noise layer yields apparent training stability but poor MIC-time extraction (strong anti-pattern indicating mis-specified training).",
        "BACKGROUND": "Title: MuST: Robust Image Watermarking for Multi-Source Tracing\n\nHistorical Technical Context:\n- Digital image watermarking began with signal-processing methods that embed bits in transform domains such as the discrete cosine transform (DCT) and discrete wavelet transform (DWT). Robustness to classical global operations (JPEG compression, blurring, additive noise) and certain geometric changes (rotation, scaling) was the primary objective. As image editing grew more sophisticated, local geometric distortion–resilient schemes (e.g., LGDR) introduced symmetric embedding and redundancy to withstand both local and global geometric attacks.\n- Deep learning shifted watermarking to end-to-end encoder–noise–decoder pipelines. HiDDeN pioneered autoencoder-style training with differentiable noise layers; StegaStamp targeted print–capture distortions. MBRS added message processors and Squeeze-and-Excitation (SE) blocks to improve robustness, especially to JPEG. LIM localized hidden codes to sub-images and introduced localization for shooting distortions. SSLWM moved embedding to learned feature spaces using ResNet-50 and key-based extraction; DIPW used patch-level hiding guided by SIFT to defend against deliberate plagiarism. These approaches assume a single host image and predominantly global distortions.\n- The problem space evolved with multi-source composite images common in creative workflows: unauthorized users irregularly crop, downsize, reposition, and blend multiple materials into a new background. Existing single-image watermarking fails when the encoder–decoder synchronization is broken by irregular cropping, extreme downsizing, feathered edges, and brightness/contrast adjustments. Retrieval/matching pipelines also struggle due to severe distortions and ownership ambiguity. MuST introduces a detector-driven, multi-source resynchronization and decoding framework to trace multiple sources reliably.\n\nTechnical Limitations:\n- Fragile synchronization under local operations: Single-image decoders implicitly assume spatial alignment between encoded content and the decoder’s receptive field. Irregular cropping and positional shifts induce an unknown transform \\( \\tau \\) (translation, scale) such that the decoder receives \\( I' = \\tau(I) \\), breaking alignment. Exhaustive search over scales/translations incurs complexity \\( O(|S||T|HW) \\), impractical for composites with multiple materials.\n- Extreme downsizing robustness: Prior methods degrade when scale factors \\( s \\in [0.3, 0.4] \\) are applied (observed in real-world composing to fit materials on backgrounds). Small \\( s \\) reduces informative high-frequency watermark signal and can defeat decoders trained on milder resizing.\n- Multi-source detection and segmentation: Baseline schemes lack mechanisms to locate K reused materials within a composite. Without a detector, extraction requires manual or heuristic cropping, leading to background contamination and false negatives. Pixel-wise segmentation at scale demands architectures that maintain high true positive rate (TPR) and IoU at \\( O(HW) \\) compute and memory.\n- Robustness to fusion distortions (IFD): Feathering, Gaussian smoothing, brightness and contrast adjustments introduce spatially varying, boundary-localized degradations. Approaches optimized for global distortions do not model edge-aware blending, leading to reduced extraction accuracy.\n- Dataset and training mismatch: Existing training pipelines do not simulate realistic multi-source composing pipelines. Without a tailored noise layer, encoders distribute watermark energy across the full image, leaving content-local watermarks underrepresented and vulnerable when only the primary component survives the composing process.\n- Scalability with multiple materials: Serial extraction across K detected components scales as \\( O(KHW) \\). Methods without resynchronization (e.g., MER) experience compounding failure probabilities as K grows, deteriorating multi-source tracing reliability.\n\nPaper Concepts:\n- Minimum External Rectangle (MER): Given a cover image \\( I_{\\mathrm{co}} \\) and a binary mask \\( M \\in \\{0,1\\}^{H \\times W} \\) of the primary component, MER extracts the tight axis-aligned bounding box around the foreground pixels:\n  \\[\n  I_{\\mathrm{mer}} = \\mathrm{MER}(I_{\\mathrm{co}}) = I_{\\mathrm{co}}[y_{\\min}:y_{\\max},\\, x_{\\min}:x_{\\max}],\n  \\]\n  where \\( (x_{\\min}, y_{\\min}), (x_{\\max}, y_{\\max}) \\) bound the set \\( \\{(x,y)\\mid M(x,y)=1\\} \\). Intuitively, MER concentrates watermark embedding within the content-only crop to preserve synchronization after irregular cropping and downsizing in composites.\n- Multi-source Image Composing Simulation Module (MIC): A differentiable noise layer that instantiates realistic composing operations. For each encoded element \\( I_{\\mathrm{en}}^i \\), MIC applies image pasting distortions (IPD) such as irregular cropping and scaling \\( s \\in [0.3,0.4] \\), positional changes, then image fusion distortions (IFD): edge moving, feathering via masked Gaussian blur with \\( \\sigma \\in [0.1,1] \\), global smoothing, brightness shift \\( \\Delta b \\in [-0.2,0.2] \\), and contrast scaling \\( \\gamma \\in [0.8,1.2] \\). Multiple elements are placed non-overlapping on a background to form\n  \\[\n  I_{\\mathrm{com}} = \\mathrm{MIC}\\big(\\{I_{\\mathrm{en}}^i\\}_{i=1}^K,\\, I_{\\mathrm{bg}}\\big).\n  \\]\n  MIC trains robustness to realistic, multi-source editing without expensive non-differentiable operations.\n- Encoder–Discriminator Objectives: The encoder embeds a length-\\( L \\) bit message \\( W_m \\) into \\( I_{\\mathrm{co}} \\) while preserving perceptual quality. Using MER-focused reconstruction and adversarial training:\n  \\[\n  L_{\\mathrm{ENC}} = \\mathrm{MSE}\\big(\\mathrm{MER}(I_{\\mathrm{co}}),\\, \\mathrm{MER}(I_{\\mathrm{en}})\\big),\n  \\]\n  \\[\n  L_{\\mathrm{ENC2}} = \\log\\big(1 - \\mathrm{DIS}(\\mathrm{MER}(I_{\\mathrm{en}}))\\big),\n  \\quad\n  L_{\\mathrm{DIS}} = \\log\\big(\\mathrm{DIS}(\\mathrm{MER}(I_{\\mathrm{en}}))\\big) + \\log\\big(1 - \\mathrm{DIS}(\\mathrm{MER}(I_{\\mathrm{co}}))\\big).\n  \\]\n  The adversarial term pushes MER crops of encoded images to be indistinguishable from covers, encouraging content-local imperceptibility.\n- Detector (U-Net) for Material Localization: A segmentation network \\( \\mathrm{DET} \\) predicts mask \\( \\hat{M} \\) of reused materials in \\( I_{\\mathrm{com}} \\). Trained with pixel-wise binary cross-entropy:\n  \\[\n  L_{\\mathrm{DET}} = - G_t \\log(\\hat{M}) - (1-G_t)\\log(1-\\hat{M}),\n  \\]\n  where \\( G_t \\) is the MIC-generated ground-truth mask. Connected-component labeling extracts \\( \\{\\mathrm{MER}(I_{\\mathrm{com}})_i\\}_{i=1}^K \\) in zigzag order for decoding. Intuitively, DET re-establishes synchronization by isolating each pasted content region prior to decoding.\n- Decoder with SE Blocks and GAP: The decoder \\( \\mathrm{DEC} \\) maps each distorted MER crop to a recovered message \\( W_m' \\) using residual and SE blocks followed by global adaptive average pooling (GAP) to produce channel-only features robust to spatial misalignment:\n  \\[\n  L_{\\mathrm{DEC}} = \\mathrm{MSE}(W_m, W_m'), \\quad\n  L = \\lambda_{\\mathrm{ENC}} L_{\\mathrm{ENC}} + \\lambda_{\\mathrm{ENC2}} L_{\\mathrm{ENC2}} + \\lambda_{\\mathrm{DEC}} L_{\\mathrm{DEC}}.\n  \\]\n  GAP aggregates features irrespective of spatial scale/position, aiding robustness to downsizing and feathering.\n- Image Pasting Distortions (IPD) and Image Fusion Distortions (IFD): IPD encompass operations that alter geometry before compositing—irregular cropping, extreme downsizing \\( s \\ll 1 \\), and positional changes modeled as a spatial transform \\( \\tau \\). IFD model post-paste blending—edge moving and feathering (masked convolution), global smoothing (Gaussian filtering), brightness shift and contrast scaling:\n  \\[\n  I' = \\gamma (I + \\Delta b), \\quad \\gamma \\in [0.8,1.2], \\, \\Delta b \\in [-0.2,0.2].\n  \\]\n  These categories define the editing taxonomy the method is trained to resist.\n\nExperimental Context:\n- The evaluation centers on robustness and traceability in multi-source composite settings. The authors construct the Single-Object Image Material Dataset (SOIM, 6.5k materials with foreground masks) to simulate realistic pasting and fusion operations via MIC. Additional tests on CASIA V2.0 and ICCV09 assess generalization to other material styles. Metrics include perceptual quality of encoded materials (PSNR and SSIM), extraction accuracy (ACC, percentage of correctly recovered bits), and detection metrics (TPR, IoU) for the segmentation stage.\n- Experiments examine independent distortion dimensions (downsizing, irregular cropping, brightness/contrast, Gaussian blur) and the full composing pipeline. Trade-offs are analyzed by comparing MuST to SSLWM, CIN, and LGDR under aligned settings; CIN is assisted with segmented crops because it lacks detection. Reported performance demonstrates that MuST maintains high visual quality (e.g., PSNR ≈ 42.89 dB, SSIM ≈ 0.9883) and consistently achieves ACC ≥ 98% for up to three materials (and strong performance up to six when background space allows), including real-world edits performed in Photoshop, Canva, and Pixso (≈95–100% ACC). Detector performance remains near-perfect (TPR ≈ 99–100%, IoU ≈ 99.7%), supporting end-to-end multi-source tracing with resilience to extreme downsizing and fusion distortions. The evaluation philosophy prioritizes robustness and scalability in realistic composing over synthetic, single-image distortions, with end-to-end training (500 epochs; \\( L=30 \\) bits; AdamW) emphasizing content-local watermarking and detector-driven resynchronization.",
        "ALGORITHMIC_INNOVATION": "**Core_Algorithm:**\nMuST replaces the single-image encoder–noise–decoder pipeline with a multi-stage pipeline that embeds watermarks only in the Minimum External Rectangle (MER) of each material, simulates multi-source composition via a learnable noise layer (MIC), detects reused materials in the composite with a U-Net detector, and resynchronizes them via MER-based extraction before decoding. Concretely: (1) crop the primary component MER from each material using its mask, embed a message with a CNN encoder and adversarial discriminator, and paste back; (2) compose multiple encoded materials on a background using MIC that applies image pasting distortions (irregular crop, extreme downsize, translation) and image fusion distortions (feathering via masked Gaussian blur, global smoothing, brightness/contrast); (3) detect and segment all reused materials in the composite image, extract their MERs with connected-component labeling, and (4) decode each message independently. The fundamental change from standard approaches is the explicit resynchronization path (DET + MER) that inverts composition operations and restores the encoder–decoder alignment under severe cropping/resizing, plus a composition-aware noise layer that matches real-world editing. The scope spans the whole system: per-material encoding on MER regions, global detection on the composite, and per-component decoding.\n\n**Key_Mechanism:**\nThe key insight is that watermark synchronization is destroyed by pasting operations; by embedding within the MER of the foreground object and later detecting and re-cropping the same object, MuST reconstructs the encoder’s spatial reference frame at decode time. Training with a composition-aware MIC exposes the model to the exact class of multi-source distortions (extreme downsize, irregular crop, feathering), aligning the learned invariances with real editing operations. This targeted resynchronization plus task-aligned noise yields robustness without sacrificing imperceptibility, since embedding is confined to the object content rather than the entire image.\n\n**Mathematical_Formulation:**\n- Preprocess and MER embedding:\nLet Ico ∈ R^{C×H×W} be a material and Mco ∈ {0,1}^{H×W} its binary mask. Define MER(·) as cropping the tight bounding box around supp(Mco) and resizing to a fixed size h×w:\nImer = MER(Ico) ∈ R^{C×h×w}.\nThe encoder ENC(·; θe) produces an encoded object patch Îmer:\nÎmer = ENC(Imer, Wm; θe),\nwhere Wm ∈ {0,1}^L is the L-bit message expanded and concatenated channel-wise with features inside ENC. The watermarked material Ien is formed by inverse resize and paste:\nIen = Paste(Ico, InvResize(Îmer), Mco).\n\n- Encoder and discriminator objectives:\n\\[ L_{\\text{ENC}} = \\text{MSE}\\big(MER(I_{co}),\\, MER(I_{en})\\big), \\]\n\\[ L_{\\text{ENC2}} = \\log\\!\\big(1 - DIS(MER(I_{en}))\\big), \\]\n\\[ L_{\\text{DIS}} = \\log(DIS(MER(I_{en}))) + \\log\\!\\big(1 - DIS(MER(I_{co}))\\big). \\]\n\n- Multi-source composition (MIC):\nGiven N encoded materials {Ien^{(j)}, Men^{(j)}}_{j=1}^N, background B ∈ R^{C×H_b×W_b}, and per-material transforms T_j from the IPD/IFD families (resize s_j, crop C_j, translate t_j, brightness γ_j, contrast β_j, masked Gaussian blur Gσ,j), the composite is\n\\[ I_{com} = \\Big( \\sum_{j=1}^{N} \\alpha_j\\, T_j(I_{en}^{(j)}) \\odot T_j(M_{en}^{(j)}) \\Big) + \\Big( 1 - \\sum_{j=1}^{N} \\alpha_j\\, T_j(M_{en}^{(j)}) \\Big) \\odot B, \\]\nwhere 0 ≤ α_j ≤ 1 are blending weights and ⊙ is elementwise product.\n\n- Detection and decoding:\nDET(Icom; θd) = M ∈ [0,1]^{H_b×W_b} is trained with pixelwise BCE using ground-truth Gt from MIC:\n\\[ L_{\\text{DET}} = -\\, G_t \\log M - (1 - G_t)\\log(1 - M). \\]\nConnected-component labeling extracts K components {R_k}. Each component is cropped and normalized by MER to produce I^{(k)}_{dis} = MER(Icom|_{R_k}) ∈ R^{C×h×w}. The decoder DEC(·; θc) predicts messages W'_m^{(k)}:\n\\[ W'_m^{(k)} = DEC\\big(I^{(k)}_{dis}; \\theta_c\\big), \\qquad L_{\\text{DEC}} = \\text{MSE}\\big(W_m^{\\pi(k)},\\, W'_m^{(k)}\\big), \\]\nwhere π is the zigzag order used by MIC to map component order to message order.\n\n- Joint training objective:\n\\[ L = \\lambda_{\\text{ENC}} L_{\\text{ENC}} + \\lambda_{\\text{ENC2}} L_{\\text{ENC2}} + \\lambda_{\\text{DEC}} L_{\\text{DEC}}. \\]\n\n- Complexity:\nLet h×w be MER size, H_b×W_b the composite size, N the number of materials, and F(·) the MACs of a CNN at a given spatial size. Then per-step costs:\nENC: O(N·F(h,w)), MIC: O(H_bW_b + N·hw) for geometric ops and filtering, DET (U-Net): O(F(H_b,W_b)), DEC: O(∑_{k=1}^K F(h,w)), typically K≈N.\n\nSymbols: θe, θd, θc are network parameters; α_j blending weights; T_j composition transforms; Gσ Gaussian blur; γ_j, β_j brightness/contrast; π ordering permutation.\n\n**Example Format:**\n- MER crop: Imer = MER(Ico) ∈ R^{C×h×w}\n- Composite synthesis: Icom = ∑_j α_j T_j(Ien^{(j)}) ⊙ T_j(Men^{(j)}) + (1 − ∑_j α_j T_j(Men^{(j)})) ⊙ B\n- Decoder input: Idis^{(k)} = MER(Icom|_{R_k})\n- Losses: L = λENCLENC + λENC2LENC2 + λDECLDEC\n- Complexity: ENC+DEC O(N·F(h,w)), DET O(F(H_b,W_b)), MIC O(H_bW_b)\n\n**Computational_Properties:**\n- Time Complexity:\n  - Training per batch: O(N·F(h,w)) for ENC + O(F(H_b,W_b)) for DET + O(∑_k F(h,w)) for DEC + O(H_bW_b) for MIC. With K≈N, total O(F(H_b,W_b) + N·F(h,w) + H_bW_b).\n  - Inference: same without backprop; decoding is per detected component: O(F(H_b,W_b) + K·F(h,w) + H_bW_b).\n- Space Complexity:\n  - Feature maps dominate. Peak memory roughly O(Act(DET; H_b,W_b) + N·Act(ENC; h,w) + K·Act(DEC; h,w)), where Act(·) denotes sum of intermediate activations retained for backprop (training) or minimal for inference. MER reduces encoder/decoder spatial footprint from H×W to h×w (typically h,w ≪ H,W), yielding significant memory savings vs whole-image embedding.\n- Parallelization:\n  - Highly parallel on GPU: (i) per-material ENC is embarrassingly parallel across N; (ii) MIC uses batched geometric warps and separable Gaussian blurs; (iii) DET is a single CNN over the composite; (iv) DEC is parallel across K components. Distributed data parallel is straightforward; model parallelism unnecessary for typical sizes.\n- Hardware Compatibility:\n  - Operations are standard CNN convs, bilinear resampling, Gaussian filtering, masking—efficient on CUDA/Tensor Cores. Memory bandwidth sensitive stages are DET at composite resolution and Gaussian blurs; use half-precision (FP16) with dynamic loss scaling to improve throughput. On CPU, MIC is efficient; CNN stages benefit greatly from GPU.\n- Training vs. Inference:\n  - Training adds discriminator and backprop through MIC and DET/DEC, increasing memory and compute; inference drops adversarial branch and uses only forward paths. Thresholding DET masks and CCL add negligible overhead.\n- Parameter Count:\n  - Adds parameters for DET (U-Net), ENC/DEC backbones, and a small DIS. MER strategy keeps ENC/DEC compact because they operate on h×w patches; no parameter blow-up vs single-image encoders. Parameter count scales with channel widths and depth; no dependence on N.\n- Numerical Stability:\n  - Potential instabilities: adversarial loss oscillations (mitigated by small λENC2 and one-sided label smoothing), extreme downsize aliasing (use antialiased bilinear downsampling), mask fragmentation (apply small morphological closing before CCL), class imbalance in DET (use BCE with focal weighting or balanced sampling). MSE targets on MER stabilize encoder/decoder around object regions.\n- Scaling Behavior:\n  - With more materials N, runtime scales linearly in N for ENC/DEC and near-constant for DET (fixed composite size). With larger composites (H_b,W_b), DET and MIC dominate. Message length L affects only the encoder’s message expansion and decoder’s final linear layer; compute impact is minor. Robustness scales favorably because MER resynchronization removes dependence on absolute position/scale; MIC exposure covers wider distortion ranges without architectural changes.",
        "IMPLEMENTATION_GUIDANCE": "Integration_Strategy:\n- Overall pipeline insertion\n  - Add a new preprocessing module MER(·) that crops the primary component to its minimum external rectangle (MER) using the instance mask, resizes it to the encoder input size, and records the inverse transform for reinsertion.\n  - Insert a multi-source image composing simulation (MIC) module after the encoder and before the detector/decoder during training.\n  - Add a U-Net based detector (DET) after the MIC to predict masks for each reused material in the composite image.\n  - Add a connected-components + MER extractor to segment each predicted object and reconstruct its MER patch for the decoder.\n  - Add a decoder (DEC) that processes each MER patch and outputs a message vector per object. Reorder objects in zigzag spatial order to align with training targets.\n  - Add an adversarial discriminator (DIS) that receives MER(Ien) and MER(Ico) to improve perceptual quality.\n\n- Code-level changes (PyTorch reference)\n  - Preprocessing (MER)\n    - Implement mask-to-box and crop:\n      - Use torchvision.ops.masks_to_boxes(mask.bool()) to get [x1,y1,x2,y2].\n      - Clamp and pad box by 1–3% of diagonal to preserve feathered edges.\n      - Crop and resize to fixed H=W=640 with antialias=True (torchvision.transforms.functional.resize).\n    - Keep a structure with crop box, resize scale, and original canvas size for inverse placement.\n  - Encoder\n    - If starting from HiDDeN/MBRS, replace the “cover image” input with MER(Ico) and concatenate a broadcasted message tensor on the channel dimension:\n      - msg ∈ {0,1}^L -> expand to [B,L,H,W] via repeat; if L ≠ H or W, embed msg with 1x1 conv to C’ and concat with image features.\n    - Insert Squeeze-and-Excitation (SE) blocks after early and mid conv stages.\n    - Return Ien by inverse-resizing MER(Ien) into the original crop box and alpha-compositing with the untouched background.\n  - MIC (training only)\n    - Implement as a nn.Module using Kornia: kornia.augmentation and kornia.filters for differentiable ops.\n    - Compose N encoded images by applying IPD (irregular cropping, downsizing, translation) and IFD (feathering, smoothing, brightness/contrast) onto a background canvas. Keep ground-truth masks Gt_i on the composite to supervise DET.\n  - Detector\n    - Use a standard U-Net (3 input channels) with BCE loss on pixel masks. Output a sigmoid mask M ∈ [0,1] with threshold τ=0.5 (tunable).\n  - Extraction for decoding\n    - Postprocess M with morphological opening (kernel 3–5) and remove small components (<0.2% of image area).\n    - Connected components: skimage.measure.label or cv2.connectedComponentsWithStats.\n    - For each component, compute MER box, crop patch from composite, and resize to encoder MER size (H=W=640).\n    - Order components by zigzag: sort by (row-major with alternation every row) to match training order.\n  - Decoder\n    - Stack ResBlocks + SE, then global average pooling (GAP) over H×W to C, then a Linear(C,L) layer to produce message logits. If using MSE target in [0,1], apply sigmoid; for BCE, use logits directly.\n  - Discriminator\n    - A small CNN that receives MER(Ico) and MER(Ien). Use spectral normalization or weight norm for stability. Output a single probability.\n  - Losses and training loop\n    - Total loss L = λENC*MSE(MER(Ico),MER(Ien)) + λENC2*GAN_loss + λDEC*MSE(Wm,W’m). Train ENC/DEC/DIS jointly; DET trains with BCE on Gt masks from MIC.\n    - Alternate updates: 1 step DIS, 1 step ENC+DEC. DET can share optimizer and be updated each step or on a separate schedule (e.g., 1:1).\n\n- Compatibility and migration\n  - From HiDDeN/LIM/MBRS:\n    - Replace their noise layers with MIC; keep JPEG/noise augmentations as optional extra.\n    - Modify Encoder forward to accept MER patches and return full-size composited image plus MER(Ien) for losses.\n    - Add DET and MER extraction block before Decoder; previous decoders that expect full image must be adapted to per-object MER patches.\n  - Frameworks\n    - PyTorch: primary target (paper used PyTorch + Kornia). Ensure Kornia>=0.7.0 for differentiable augmentations and Gaussian kernels.\n    - TensorFlow/JAX: rewrite MIC with tf.image and jax.image; replace Kornia ops with equivalent TF Addons or custom kernels. Connected components with tf.raw_ops.ImageConnectedComponents or custom CPU path.\n\n- Dependencies and hardware features\n  - PyTorch>=1.12, torchvision>=0.13, Kornia>=0.7, scikit-image>=0.19 or OpenCV>=4.5 for connectivity.\n  - Use AMP (torch.cuda.amp) and channels-last memory format for speed on NVIDIA Ampere/ADA (TF32/FP16).\n  - No custom CUDA kernels required; all operations available off-the-shelf.\n\n- Training pipeline integration\n  - DataLoader yields tuples (Ico, mask, background, Wm).\n  - For each batch: apply MER to each (Ico, mask), encode, compose with MIC into a composite and Gt masks, run DET to predict M, extract components, run DEC on MER patches, compute losses, and optimize.\n\nParameter_Settings:\n- Input/output sizes\n  - MER patch size H=W=640 (default); for lower memory use 384–512. Background canvas 1000×1000 (default), scalable to 768–1536 square depending on expected number of materials.\n- Number of materials per composite during training\n  - Uniform random in [1, 6] with non-overlap constraint. For small canvases or memory constraints, cap at 3–4.\n- Message length L\n  - Default 30 bits. Robust range 16–64 bits. For L>64 expect slight PSNR drop and ACC decline; consider error-correcting codes for L≥64.\n- Loss weights\n  - λENC ∈ [0.5, 1.0] (default 0.7); λENC2 ∈ [1e-4, 5e-3] (default 1e-3); λDEC ∈ [1.0, 3.0] (default 2.0).\n  - If adversarial training destabilizes, reduce λENC2 by 2–10×.\n- Optimizer and schedule\n  - AdamW with lr ∈ [1e-4, 3e-4] (default 1e-4), betas=(0.9, 0.999), weight_decay=0–1e-4.\n  - Linear warmup 1–5 epochs; cosine decay over 500 epochs; early stopping on val ACC/BER.\n  - Gradient clipping at 1.0 helps stabilize DIS/ENC updates.\n- Batch size\n  - A6000 48GB: 4–6 composites per batch with AMP, otherwise 3 (paper used 3).\n  - 24GB (RTX 3090/4090): 2–3 with AMP and H=W=512; gradient accumulation to reach effective batch size 6–8.\n- MIC parameters (Kornia)\n  - Downsizing (Scale Rate): sample per object in [0.3, 0.6] for robustness; paper default [0.3, 0.4]. Include [0.2, 0.8] during curriculum to broaden generalization.\n  - Irregular cropping: crop ratio per object ∈ [0.1, 0.5] of object area; random border-specific removal; post-crop slight edge moving by extra 2–5 pixels.\n  - Translation: place without overlap by greedy packing; edge margin ≥ 10–30 pixels.\n  - Feathering: Gaussian blur around mask boundary with σ_edge ∈ [0.5, 2.0], applied via masked convolution; blend width 3–11 pixels.\n  - Smoothing (global): Gaussian σ_global ∈ [0.1, 1.0] (paper) or up to 2.0 for harder training.\n  - Brightness: add ∈ [-0.2, 0.2] (default) or extend to [-0.5, 0.5] for robustness.\n  - Contrast: multiply ∈ [0.8, 1.2] (default) or extend to [0.6, 1.4].\n- Detector\n  - U-Net base filters: 32–64 at first layer, depth 4; dropout 0.1–0.2 if overfitting.\n  - Threshold τ for binarizing M: 0.4–0.6 (default 0.5). If TPR too low, lower τ by 0.05.\n  - Loss: BCE; optional Dice loss weight 0.1–0.5 to improve boundary quality.\n- Decoder\n  - ResBlocks: 4–6 with SE after every 1–2 blocks; channels 64–128.\n  - GAP -> Linear to L outputs. Use sigmoid and MSE or BCEWithLogitsLoss; threshold 0.5 at inference.\n- Discriminator\n  - 3–5 conv blocks with instance norm/spectral norm; lr 1e-4, one update per ENC step.\n- Initialization\n  - He/Kaiming normal for convs; SE gamma initialized to 1.0; discriminator last layer bias=-2 to start with low “real” confidence and reduce early overpowering.\n- Critical vs robust parameters\n  - Critical: MER crop accuracy, MIC scale range, DET threshold τ, λDEC (directly affects ACC).\n  - Robust: exact U-Net depth, DIS architecture details, global smoothing σ within [0.1,1.0].\n\nApplication_Conditions:\n- Beneficial scenarios\n  - Multi-source composite images where each material is a distinct object pasted onto a background (advert creatives, product collages, posters, memes).\n  - Expected resizing mostly downsizing (Scale Rate 0.2–0.8), irregular cropping on object borders, feathered blending, brightness/contrast tweaks, mild global smoothing.\n  - When copyright traceability of multiple sources is required; each provider embeds its own 30-bit ID.\n- Hardware requirements\n  - Minimum: 1× 16GB GPU for training with MER 384–512 and batch size 1–2 (gradient accumulation). Recommended: ≥24GB GPU or 48GB (A6000) for 640^2 and batch size 3–6.\n  - Inference: 8–12GB GPU or CPU acceptable for DET+DEC at modest throughput.\n- Scale considerations\n  - Datasets: ≥5k materials with masks for solid generalization; backgrounds diverse (photos, pure color, graphics).\n  - Number of materials per composite at inference: 1–6 on a 1000×1000 background; performance remains strong up to 4–5 objects; slight recall drop at 6.\n- Task compatibility\n  - Works best with single-object materials with available or derivable masks (white-background product images, cutouts).\n  - Neutral/Harmful when: objects overlap heavily, semi-transparent blending (alpha <0.6), or materials are extremely small after downsizing (scale <0.2).\n- Alternatives comparison\n  - Prefer MuST over traditional single-image watermarking when tracing multiple sources within a single composite is needed.\n  - If only single-source watermark and heavy JPEG/compression robustness is primary, consider MBRS/JPEG-focused methods; you can incorporate JPEG into MIC to cover both.\n- Resource constraints\n  - For low-memory deployments, reduce MER size to 384–512 and use lightweight U-Net (depth 3) with minor ACC loss (<1–2%).\n\nExpected_Outcomes:\n- Performance improvements\n  - Visual quality of watermarked materials: PSNR ≈ 42.5–44.1 dB, SSIM ≈ 0.985–0.995 when comparing MER(Ico) vs MER(Ien) and composite output vs cover in unaltered regions.\n  - Extraction accuracy (ACC) under MIC with 3 materials and default ranges: ≥98% (30-bit messages).\n  - Detector quality under MIC: TPR ≈ 99–100% for ≤4 materials; IoU ≈ 99.7–99.9%. At 6 materials, TPR may drop to ≈98.9–99.2%.\n- Real-world results\n  - With Photoshop/Canva/Pixso compositions involving resizing, feathering, and brightness/contrast edits: ACC ≈ 95–100% across 1–4 materials.\n- Timeline expectations\n  - Visual quality improvements from DIS appear within first 10–20 epochs.\n  - Robust extraction under MIC typically stabilizes by 150–250 epochs; full convergence by 400–500 epochs on 6k training materials.\n- Trade-offs\n  - Stronger MIC aggressiveness (downsize <0.25, blur σ>2) hardens robustness but can reduce PSNR by 0.5–1.5 dB and slow training.\n  - Longer messages (>64 bits) reduce ACC unless compensated with stronger encoder capacity and error-correcting codes.\n- Failure modes\n  - Interconnected or overlapping materials (objects touching after feathering) causing merged components; decoder sees merged MER and fails to map to correct message.\n  - Extreme downsizing (<0.2 scale) or severe blur (σ≥3) leading to ACC drops below 90%.\n  - Complex color manipulations (heavy hue shifts), strong JPEG (q≤30), or alpha transparency <0.5 not seen in training can degrade ACC; include these in MIC if needed.\n  - Detector under-segmentation when τ too high or masks too thin near feathered edges; missed objects drop ACC to 0 for those items.\n- Debugging indicators\n  - Residual visualization: MER(Ien)-MER(Ico) should show low-energy content, concentrated within object area; large background residual suggests inverse placement bug.\n  - DET metrics: TPR and IoU on MIC Gt should exceed 98%; if not, lower τ or improve postprocessing.\n  - Decoder BER should be <2% for most samples; spikes indicate ordering mismatch (zigzag indexing) or incorrect crop-to-resize normalization.\n  - GAN stability: if DIS loss saturates at 0 and ENC loss explodes, reduce λENC2 by 10× and/or use spectral norm.\n- Hardware-specific outcomes\n  - A6000 48GB + AMP: training throughput 2–3 iters/sec with batch 3–4; full training 12–24 hours for 500 epochs on 6k images.\n  - RTX 3090/4090 24GB + AMP: batch 2–3; expect 1.5–2.0 iters/sec; training 18–36 hours.\n  - CPU inference for DET+DEC on 1000×1000 composite: 0.5–1.0 s per image depending on components count.\n\nValidation procedures:\n- Unit tests\n  - Verify MER/inverse-MER round-trip with synthetic masks: L2 error <1e-6 on integer coordinates and <1e-3 after resize.\n  - Confirm zigzag ordering against Gt by placing numbered squares and checking decoded order.\n- MIC sanity checks\n  - Visualize per-step composites with overlaid Gt masks. Assert non-overlap and min margin constraints; log applied parameters for reproducibility.\n- End-to-end evaluation\n  - Report PSNR/SSIM on MER regions and ACC/BER per object on a held-out set (SOIM test + CASIA V2.0 + ICCV09).\n  - Stress tests: sweep scale ∈ {0.2,…,0.9}, crop ratio ∈ {0.2,…,0.9}, blur σ ∈ {0.1,1,2}; verify curves similar to paper: MuST top across downsizing/cropping; ACC ≥97–99% in default ranges.\n- Robustness extensions (optional)\n  - Add JPEG(q∈[30,100]), hue/saturation jitter (±0.1–0.2), and alpha blending with opaque backgrounds to MIC; track effects on ACC and PSNR.\n\nTroubleshooting:\n- Low ACC but high DET IoU\n  - Check component ordering; ensure zigzag sorting matches training; ensure message-target alignment in minibatch.\n- Detector misses small objects\n  - Increase MER training scale lower bound from 0.3 to 0.4 for very small items, or increase canvas resolution to 1280×1280.\n- Artifacts around pasted regions\n  - Reduce λENC2 or discriminator capacity; ensure antialias=True in all resizes; add total variation loss weight 1e-6–1e-5 if needed.\n- Instability with DIS\n  - Use one-sided label smoothing (real=0.9), spectral norm, and reduce λENC2 to 1e-4; optionally switch to hinge loss.\n\nHardware/Software notes:\n- Enable cudnn.benchmark=True for fixed input sizes; use channels_last to improve memory BW.\n- Use mixed precision (torch.cuda.amp.autocast + GradScaler).\n- Profiling with torch.profiler to identify slow CPU-bound connected components; consider batching components or using OpenCV CUDA if available."
    }
]