[
    {
        "DESIGN_INSIGHT": "### DESIGN_INSIGHT_HIGH: [Fourier-Steered Initial Noise Watermarking with DDIM Invert-and-Check – Training-Free, Distributional Fingerprint for Diffusion Models]\nTree-Ring Watermarking replaces post-hoc pixel/frequency-space edits or learned encoder–decoder watermarking heads by minimally steering the initial diffusion noise and verifying it via model inversion. Instead of adding a watermark to the sampled image \\(x_0\\), the method embeds a secret “key” directly in the Fourier domain of the starting noise \\(x_T\\), then generates images with an unmodified sampler; detection inverts a possibly manipulated image back to its approximate \\(x_T\\) and tests for the key.\n\nMechanism: choose a circular low-frequency mask \\(M\\) and a complex key \\(k^\\*\\in\\mathbb{C}^{|M|}\\). Draw the starting noise by enforcing the key in Fourier space\n\\[\n\\mathcal{F}(x_T)_i \\sim \n\\begin{cases}\nk^\\*_i, & i\\in M\\\\\n\\mathcal{N}(0,1), & i\\notin M\n\\end{cases}\n\\]\nthen generate \\(x_0=D_\\theta(x_T)\\) with standard DDIM; for detection, invert any test image \\(x_0'\\) using DDIM inversion \\(x_T' = D_\\theta^\\dagger(x_0')\\) (no prompt required) and compute a key-match score on \\(M\\), e.g.\n\\[\nd = \\frac{1}{|M|}\\sum_{i\\in M}\\left|k^\\*_i - \\mathcal{F}(x_T')_i\\right|.\n\\]\nThe watermark is accepted if \\(d<\\tau\\).\n\nFundamental difference from prior work: the watermark is a distributional bias on the sampler’s input (no visible, additive signal in \\(x_0\\)); there is no extra training, no architectural change to the generator, and robustness is inherited from Fourier invariances. Rotation, translation, dilation, crop, blur, JPEG, and color jitter in pixel space map predictably in Fourier space, so the embedded low-frequency structure is recoverable after inversion. Complexity is unchanged at generation (standard DDIM), and detection is a single inversion pass (same order as one DDIM sample), both \\(O(T)\\) denoising steps.\n\n\n### DESIGN_INSIGHT_MEDIUM: [Noncentral χ² P-Value Detector on Inverted Fourier Coefficients – Calibrated, FPR-Controllable Watermark Testing]\nHeuristic thresholds for watermark detection are replaced by a statistically calibrated test over the inverted noise’s Fourier coefficients inside \\(M\\). Under the null hypothesis that a non-watermarked image, after inversion, yields Fourier coefficients distributed as complex Gaussian noise, the method derives an exact noncentral \\(\\chi^2\\) test to produce interpretable P-values and explicit control of the false-positive rate.\n\nLet \\(y=\\mathcal{F}(x_T')\\) on \\(M\\), and estimate the per-image variance \\(\\sigma^2\\) by \\(\\sigma^2=\\frac{1}{|M|}\\sum_{i\\in M}|y_i|^2\\). Define the score\n\\[\n\\eta=\\frac{1}{\\sigma^2}\\sum_{i\\in M}\\left|k^\\*_i - y_i\\right|^2.\n\\]\nUnder \\(H_0: y\\sim\\mathcal{N}(0,\\sigma^2 I)\\), \\(\\eta\\sim\\chi^2_{|M|}(\\lambda)\\) with noncentrality \\(\\lambda=\\frac{1}{\\sigma^2}\\sum_{i\\in M}|k^\\*_i|^2\\). The P-value is\n\\[\np=\\Pr\\!\\left(\\chi^2_{|M|}(\\lambda)\\le \\eta\\mid H_0\\right),\n\\]\ncomputed via the noncentral \\(\\chi^2\\) CDF. Thresholding \\(p<\\alpha\\) provides TPR/FPR trade-offs with explicit control of FPR at level \\(\\alpha\\).\n\nKey difference from prior art: instead of correlational or bit-accuracy metrics from steganographic decoders, this detector is grounded in the Gaussianization property of diffusion forward processes and the accuracy of DDIM inversion. It remains effective without knowledge of the prompt, classifier-free guidance scale, or the number of sampling steps—empirically robust to step mismatch and missing conditioning—because the test operates on the recovered \\(x_T'\\) statistics.\n\n\n### DESIGN_INSIGHT_MEDIUM: [Radially-Structured Keys in Low-Frequency Fourier Bands (Tree-RingRings) – Transformation-Invariant, Multi-Key Design]\nSingle-frequency or pixel-space watermarking is replaced by concentric, ring-constant keys placed in a circular low-frequency mask, balancing robustness, invisibility, and key capacity. The key assigns a constant complex value per ring index while preserving overall Gaussian-like statistics along rings to avoid shifting the generator’s distribution.\n\nMechanism: partition the low-frequency disk \\(M\\) into radii \\(\\{\\rho=1,\\ldots,R\\}\\). For each ring, sample a complex constant \\(a_\\rho\\sim \\mathcal{N}(0,\\sigma_\\rho^2)\\) and set\n\\[\nk^\\*_i = a_{\\rho(i)} \\quad \\text{for } i\\in M,\n\\]\noptionally drawing \\(\\{a_\\rho\\}\\) per model/user to support multiple keys. Rotations in pixel space rotate \\(\\mathcal{F}(x)\\) but preserve ring membership; translations induce global phase shifts; dilations/crops rescale frequencies. Consequently, the ring-constant structure remains detectable after inversion, while the key’s Gaussian-like marginal statistics minimize perceptual or FID impact.\n\nDifference from prior approaches: compared to “fixed-frequency” Fourier watermarks (e.g., toggling a single DFT/DCT coefficient) or zeroing a band (Tree-RingZeros), the ring design provides strong robustness to rotations and rescalings while enabling multi-key assignment and maintaining image quality. Empirically, larger mask radii improve AUC/TPR but trade off FID; moderate radii yield a favorable balance, and detection remains strong across strong JPEG, blur, noise, crop/scale, and rotation perturbations.",
        "EXPERIMENTAL_TRIGGER_PATTERNS": "Task_Performance_Signatures:\n- Image-generation detection accuracy (primary objective)\n  - Expect AUC ≈ 1.000 and TPR@1%FPR ≈ 1.000 in clean conditions for Tree-RingRand and Tree-RingRings on Stable Diffusion-v2; similar for ImageNet 256×256 diffusion.\n  - Under adversarial transforms (rotation 75°, JPEG q=25%, 75% crop+scale, 8×8 Gaussian blur, Gaussian noise σ=0.1, brightness jitter up to 6):\n    - Tree-RingRings (Stable Diffusion): average AUC ≈ 0.975; TPR@1%FPR ≈ 0.694.\n    - Tree-RingRand (Stable Diffusion): average AUC ≈ 0.918; TPR@1%FPR ≈ 0.702.\n    - Tree-RingZeros (Stable Diffusion): average AUC ≈ 0.963; TPR@1%FPR ≈ 0.715.\n    - Cross-model (ImageNet 256×256): Tree-RingRings average AUC ≈ 0.966; Tree-RingRand ≈ 0.940; Tree-RingZeros ≈ 0.921.\n  - Attack-wise breakdown (Stable Diffusion, AUC):\n    - Rotation: Rings ≈ 0.935; Rand ≈ 0.486; Zeros ≈ 0.994.\n    - JPEG: Rings ≈ 0.999; Rand ≈ 0.999; Zeros ≈ 0.984.\n    - Crop+Scale: Rings ≈ 0.961; Rand ≈ 0.971; Zeros ≈ 0.999.\n    - Gaussian blur: Rings ≈ 0.999; Rand ≈ 0.999; Zeros ≈ 0.977.\n    - Gaussian noise (σ=0.1): Rings ≈ 0.944; Rand ≈ 0.972; Zeros ≈ 0.877.\n    - Color jitter (brightness up to 6): Rings ≈ 0.983; Rand ≈ 0.994; Zeros ≈ 0.907.\n  - P-value test (noncentral χ2, mask M): for watermarked images p typically < 1e-8 to < 1e-50; for unwatermarked images p ~ 0.15–0.91. Expect robust separation even after strong transforms (e.g., blur, rotation).\n\n- Image quality and semantic fidelity (side objective; should not degrade)\n  - Stable Diffusion-v2:\n    - FID baseline ≈ 25.29; Tree-RingRings ≈ 25.93 (negligible +0.64 absolute); Tree-RingRand ≈ 25.47 (≈ +0.18); Tree-RingZeros ≈ 26.56 (largest shift).\n    - CLIP score baseline ≈ 0.363; Tree-RingRings ≈ 0.364; Tree-RingRand ≈ 0.363; Tree-RingZeros ≈ 0.356. Expect within ±0.001 (Rings/Rand) and small decline for Zeros.\n  - ImageNet 256×256:\n    - FID baseline ≈ 17.73; Tree-RingRings ≈ 17.68 (no degradation); Tree-RingRand ≈ 18.68; Tree-RingZeros ≈ 18.78.\n\n- Contextual conditions and scale dependencies\n  - Guidance scale (Stable Diffusion): improvements in detectability remain stable for guidance ∈ [2, 18]; expect minor detection error increase at very high guidance (e.g., 18) but AUC remains ≈ 0.96–0.97 (Rings).\n  - Generation vs. detection steps:\n    - Robust to mismatch: With generation steps ∈ {25, 50, 100, 200, 400, 800} and detection steps as low as 10, expect AUC ≈ 0.96–0.97 averaged over clean+attacks (Rings). Using fewer detection steps can slightly improve detection power.\n  - Watermark radius r (mask size):\n    - Detection strength increases with r; quality slightly degrades. Typical operating point: r ≈ 16 yields strong detection with tolerable FID shift (< +3 absolute).\n  - Transform severity:\n    - Even at extreme blur (kernel size up to 40) expect AUC ≥ 0.90 with Rings; brightness jitter and Gaussian noise are the strongest degraders but remain within high AUC regimes for moderate strengths.\n\n- Efficiency expectations\n  - Training: no additional training or finetuning (0% extra training compute).\n  - Generation-time overhead: negligible; one FFT/IFFT on initial noise plus masked assignment (expect <1–2% latency impact vs. vanilla DDIM sampling).\n  - Detection-time cost: proportional to DDIM inversion steps; with 10–50 steps, expect similar or lower wall-clock than generation. FFT-based key test adds negligible overhead.\n\n- Degradation bounds and competitive baselines\n  - Compared to training-free DwtDct/DwtDctSvd and learned RivaGAN:\n    - Clean detectability competitive with RivaGAN (AUC ≈ 1.0) but with zero training and invisibility (no image-space edits).\n    - Adversarial average AUC (Rings) ≈ 0.975 vs. RivaGAN ≈ 0.854 on Stable Diffusion.\n    - Quality drop bounded: CLIP within ±0.001 (Rings/Rand); FID change within +0.7 absolute (Rings); Zeros shows the largest quality cost.\n\n- Irrelevant LM/NLP/other benchmarks (no expected change; out-of-domain)\n  - Language modeling: lambada_openai, wikitext, ptb – expect no change (technique targets image diffusion; not applicable).\n  - Reading comprehension: squad_completion, squad_v2, narrativeqa – expect no change.\n  - Commonsense reasoning: hellaswag, piqa, social_iqa, commonsenseqa – expect no change.\n  - Factual QA: arc_easy, arc_challenge, boolq, openbookqa – expect no change.\n  - Context resolution: winogrande, winograd – expect no change.\n  - Other tasks: swde (structured extraction), fda (data augmentation) – expect no change.\n\nArchitectural_Symptoms:\n- Training characteristics\n  - Training-free: no optimizer steps, no loss curves; any training loss/validation metrics for the base model remain unchanged.\n  - Stability indicators: no added divergence risk; no NaN occurrences attributable to watermarking (key insertion only modifies initial noise in Fourier domain).\n  - Scaling properties: behavior stable as model depth/width increases (watermark operates outside training and does not scale with model parameters).\n\n- Runtime/memory behaviors\n  - Generation-time memory and throughput:\n    - Memory footprint indistinguishable from baseline DDIM sampling; FFT/IFFT on xT adds O(N log N) compute on a single noise tensor with negligible extra VRAM.\n    - Throughput comparable to baseline; GPU utilization dominated by UNet steps, not FFTs.\n  - Detection-time memory and throughput:\n    - Memory usage comparable to generation with the same number of DDIM steps; supports identical batch sizes as baseline inversion runs (no additional activations stored).\n    - Throughput scales linearly with number of inversion steps; using 10 detection steps often suffices (AUC ≈ 0.96–0.97), enabling faster-than-generation verification.\n  - No OOM regressions vs. baseline at larger batch sizes; key check (masking + FFT + χ2 P-value) adds negligible memory.\n\n- Profiling signatures and statistical checks\n  - Fourier-domain ring signature:\n    - Inverted noise xT′ exhibits concentric ring magnitudes in F(xT′) within mask M; rotation/crop/flip in pixel space preserve detectable ring structure (rotation-invariant radial statistics).\n  - Distance and P-value criteria:\n    - L1/L2 distance over M between F(xT′) and key k* falls below tuned threshold τ at 1% FPR with TPR ≈ 0.69 (adversarial average, Rings).\n    - Noncentral χ2 statistic η yields very small P-values (p < 1e-8 to < 1e-50) for watermarked images; natural/unwatermarked images produce large P-values (≈ 0.15–0.91).\n  - Gaussianity outside mask:\n    - Histogram of F(xT′) outside M remains Gaussian with mean ≈ −0.005 and std ≈ 0.996 (expected for natural images/inverted noise), confirming minimal distribution shift.\n  - Step-mismatch robustness:\n    - Similar η/P-value distributions and ROC metrics when detection steps differ widely from generation steps (e.g., detect with 10 steps vs. generate with 200–800 steps).\n  - Hyperparameter symptoms:\n    - Increasing mask radius r strengthens detection but slightly increases FID; expect monotonic trade-off.\n    - Higher guidance scale slightly weakens inversion fidelity (no text at detection), but ring detection remains strong (AUC ≈ 0.96–0.97).\n  - Failure/edge indicators:\n    - Very strong Gaussian noise and extreme brightness jitter increase η and P-values (weaker detection), with Tree-RingZeros most sensitive to noise/jitter.\n    - For Tree-RingRand, rotation notably weakens detectability (AUC ≈ 0.49 at 75°), signaling the need for Rings when rotation robustness is required.\n\n- Comparison with baselines (operational)\n  - Unlike post-hoc image-space watermarking (e.g., Cox/DwtDct) and learned encoders (RivaGAN), Tree-Ring produces:\n    - No visible image-space artifacts; no increase in image-space frequency spikes.\n    - Stronger adversarial robustness without training, observable as consistently higher AUC under rotation, blur, and crop/scale.\n  - Operators should observe negligible shifts in CLIP score and near-baseline FID (Rings/Rand), confirming invisibility and minimal distribution perturbation.",
        "BACKGROUND": "Title: Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust\n\nHistorical Technical Context:\n- Before this work, image generation advanced from GANs to score-based generative modeling and diffusion models. Denoising Diffusion Probabilistic Models (DDPM) formalized a forward noising process with a learned reverse denoising process, and DDIM provided a deterministic, faster sampler. Latent Diffusion further improved efficiency by operating in the VAE latent space (e.g., Stable Diffusion). These architectures became the dominant paradigm for high-fidelity, text-conditioned image synthesis.\n- Digital image watermarking historically relied on post-hoc, additive perturbations applied in transform domains (DCT, DWT, Fourier-Mellin, Radon, SVD) to gain robustness to translations, rotations, rescaling, and JPEG compression. With deep learning, “deep” watermark encoders/decoders (e.g., RivaGAN, HiDDeN) learned to embed bit strings robustly, sometimes by training the generator or data with embedded marks. However, these methods still modified finished images and required training pipelines. In language models, a different line of work altered output token distributions without post-hoc edits, offering “invisible” watermarks without training.\n- The problem space evolved from embedding bits in fixed images to the need to attribute content from generative models at scale, under arbitrary user-side transformations. Prior image methods often suffered from brittleness to common manipulations (crops, rotations, noise), visible artifacts at higher strengths, and lacked statistically calibrated detection. This paper introduces a distributional watermark for diffusion models by subtly biasing the initial noise distribution in Fourier space, avoiding any post-hoc pixel edits while maintaining robustness and providing calibrated P-values.\n\nTechnical Limitations:\n- Post-hoc visibility and quality trade-offs: Traditional/additive watermarks can introduce visible artifacts or degrade FID/CLIP when strengthened for robustness; training-based encoders add extra networks and tuning. This work addresses the visibility-quality trade-off by embedding in the initial noise distribution (no pixel-space edits), showing negligible FID increase (e.g., Stable Diffusion FID 25.93 vs 25.29 baseline; CLIP 0.364 vs 0.363).\n- Robustness under common transforms: Prior methods degrade under rotations, cropping/rescaling, color jitter, noise, and blur. Reported adversarial AUCs for baselines drop substantially (e.g., DwtDct AUC 0.574; RivaGAN 0.854 under averaged attacks). This work designs ring-structured Fourier keys with invariances to convolutional and geometric transforms, achieving high adversarial AUC (Tree-RingRings 0.975 on Stable Diffusion; 0.966 on ImageNet).\n- Training and deployment overhead: Learned watermark encoders/decoders require end-to-end training, extra parameters, and integration complexity. This paper introduces a training-free plug-in with O(L log L) FFT/IFFT overhead at generation time and no model finetuning, keeping API latency and memory nearly unchanged.\n- Lack of calibrated detection: Many detectors use heuristic thresholds without statistical guarantees, risking elevated false positives. This work formulates a hypothesis test with a noncentral χ² distribution to compute P-values, enabling explicit control of false positive rate α (e.g., TPR@1%FPR up to 1.000 clean; 0.694 adversarial for Tree-RingRings on Stable Diffusion).\n- Sensitivity to unknown generation settings: Prior methods often assume knowledge of prompts, guidance scales, and diffusion steps for detection. This work detects with empty prompts and mismatched step counts, showing minimal AUC loss across a grid of generation/detection steps and robustness up to guidance scale 18.\n- Computational detection bottleneck: Detection that relies on pixel-space correlation may be fast but brittle; training-time schemes incur O(N·E) optimization over datasets. Here, detection cost is a single DDIM inversion trajectory (O(T) model evals) plus O(L log L) FFT, comparable to a generation pass, but performed only when verification is needed.\n\nPaper Concepts:\n- Forward diffusion (DDPM): Adds Gaussian noise in T steps\n  - Definition: q(xt|xt−1) = N(xt; √1−βt xt−1, βt I), with cumulative ᾱt = ∏i=1..t (1−βi), yielding the closed form\n    xt = √ᾱt x0 + √(1−ᾱt) ε, with ε ∼ N(0, I).\n  - Intuition: Progressively corrupts data to nearly Gaussian noise; enables learning a denoiser for generative sampling.\n- DDIM reverse sampling: Deterministic mapping from noise to image using a learned εθ\n  - Estimate x̂0 at step t: x̂t 0 = (xt − √(1−ᾱt) εθ(xt)) / √ᾱt, then\n    xt−1 = √ᾱt−1 x̂t 0 + √(1−ᾱt−1) εθ(xt).\n  - Role: Efficient, high-quality sampler widely used in practice and required by this method for both generation and inversion.\n- DDIM inversion Dθ†: Approximate retrieval of the initial noise xT from a given image x0\n  - Mapping: x′T = Dθ†(x′0), implemented by stepping “forward” in the DDIM sense under the learned εθ and an assumption that local step differences satisfy xt−1 − xt ≈ xt+1 − xt.\n  - Intuition: Enables verifying a watermark by reconstructing the starting noise used to generate the image, even without conditioning text.\n- Fourier-space key embedding with circular mask:\n  - Keyed initialization:\n    For noise xT ∈ R^L with Fourier transform y = F(xT), choose a binary mask M and key k* ∈ C^{|M|}, and set\n    F(xT)_i ∼\n      k*_i, if i ∈ M\n      N(0, 1), otherwise.\n  - Mask design: M is a centered circular region (radius r) in low-frequency modes to inherit invariances to rotation/translation/dilation in pixel space.\n  - Intuition: Embedding in Fourier space avoids localized pixel artifacts and exploits transform-domain stability under common image manipulations.\n- Tree-Ring watermark families:\n  - Tree-RingZeros: k* = 0 on M, maximizing invariance to shifts/crops/dilations but deviating from Gaussianity; robust but limits multi-key capacity.\n  - Tree-RingRand: k* ∼ N(0, I) on M, distribution-matched to preserve quality and support multiple keys, but not rotation-invariant.\n  - Tree-RingRings: Concentric-ring pattern with constant (Gaussian) value on each ring; rotation-invariant and robust to multiple perturbations while remaining close to isotropic Gaussian statistics.\n- Statistical detection with noncentral χ² P-value:\n  - Inversion: Given x′0, compute x′T = Dθ†(x′0), set y = F(x′T).\n  - Null hypothesis (natural image): H0: y ∼ N(0, σ² I), with per-image variance estimate σ² = (1/|M|) ∑_{i∈M} |y_i|².\n  - Test statistic:\n    η = (1/σ²) ∑_{i∈M} |k*_i − y_i|²,\n    which follows χ²_{|M|}(λ) under H0 with noncentrality λ = (1/σ²) ∑_{i∈M} |k*_i|².\n  - P-value: p = Φ_{χ²}(η; |M|, λ). Reject H0 (detect watermark) when p ≤ α. Provides calibrated control of false positive rate and interpretability.\n\nExperimental Context:\n- Evaluation scope and goals: The study evaluates generative watermarking accuracy, robustness under common real-world transforms, and image quality preservation. It emphasizes training-free deployment on state-of-the-art diffusion models (Stable Diffusion v2; ImageNet 256×256) and private detectability (model-owner verification via inversion). The goal is to demonstrate invisibility (no post-hoc edits), robustness across diverse attacks, minimal quality degradation, and statistically calibrated detection.\n- Protocols, metrics, and benchmarks:\n  - Watermark accuracy: ROC AUC and TPR@1%FPR on 1,000 watermarked vs 1,000 unwatermarked images per run (5 runs, different seeds). P-values reported qualitatively; histograms confirm Gaussianity of inverted Fourier coefficients on natural images.\n  - Robustness: Average AUC/TPR across clean and six attacks—rotation (e.g., 75°), JPEG (25% quality), random crop+rescale (75%), Gaussian blur (e.g., 8×8 kernel; ablations up to 40), Gaussian noise (σ up to 0.4; main σ=0.1), and color jitter (brightness factor up to 6).\n  - Quality: FID on MS-COCO-2017 (Stable Diffusion; 5,000 images) and ImageNet-1k (ImageNet model; 10,000 images). CLIP score (OpenCLIP ViT/G) for Stable Diffusion generations.\n  - Key settings: Default 50 DDIM steps for both generation and detection; guidance scale 7.5 for generation; empty prompt for inversion; watermark radius r=10 (ablated).\n- Headline results:\n  - Clean detection: AUC/TPR@1%FPR ≈ 1.000/1.000 for Tree-RingRand and Tree-RingRings on Stable Diffusion and ImageNet.\n  - Robustness (Stable Diffusion, average over clean+attacks): Tree-RingRings AUC 0.975 (TPR@1%FPR 0.694), outperforming DwtDct 0.574 and RivaGAN 0.854. Per-attack AUCs for Tree-RingRings: rotation 0.935, JPEG 0.999, crop+scale 0.961, blur 0.999, noise 0.944, color jitter 0.983.\n  - Image quality: Minimal impact relative to unwatermarked models—Stable Diffusion FID 25.93 vs 25.29; CLIP 0.364 vs 0.363. ImageNet FID 17.68 vs 17.73.\n  - Ablations: Robust detection under large mismatches in generation/detection steps; remains reliable up to guidance scale 18. Larger radius improves robustness with an FID trade-off; r≈16 offers a strong balance. Even under extreme blur (kernel 40) or multiple concurrent attacks, detection remains effective.\n- Efficiency and deployment: Generation-time overhead is a single FFT/IFFT and masked replacement in Fourier space (O(L log L)); no additional training or model finetuning. Detection requires DDIM inversion (O(T) U-Net evaluations) once per verification plus an FFT and a χ² CDF evaluation, suitable for model-owner-side forensics rather than continuous monitoring.",
        "ALGORITHMIC_INNOVATION": "**Core_Algorithm:**\n- Replace post-hoc image-space watermarking with a pre-sampling modification of the initial diffusion noise in Fourier space. Specifically, draw the initial noise x_T ~ N(0, I), compute its 2D FFT y = F(x_T), and overwrite Fourier coefficients on a low-frequency circular mask M with a secret key k* defined either as zeros, iid Gaussian values, or radially-constant ring values.\n- Perform IFFT to obtain the watermarked initial noise x_T^w = F^{-1}(y^w) and run the standard DDIM sampler x_0 = D_θ(x_T^w, c) to generate the image; no further changes to the model, loss, or sampling steps are required.\n- For detection, approximate-invert the diffusion process on a suspect image x_0' via DDIM inversion x_T' = D_θ^†(x_0', c=∅), transform to Fourier space y' = F(x_T'), and compute a key-matching statistic on M to obtain a p-value; declare watermarked if p < α (or distance < τ).\n- Scope: a single pre-step FFT/IFFT per sample at generation time, and a single FFT at detection time; the diffusion model architecture and per-layer computations are unchanged.\n\n**Key_Mechanism:**\n- The key insight is to encode a watermark as a distributional bias in the initial noise’s low-frequency Fourier modes, which propagates through the deterministic DDIM trajectory into x_0 without introducing perceptible image-space artifacts. Because rotations, translations, dilations, convolutions, and flips act predictably on Fourier structure (e.g., rotations rotate the spectrum, translations add global phase), ring-structured keys preserve detectability after common manipulations.\n- Robust detection is enabled by DDIM inversion, which reliably recovers an x_T' whose Fourier coefficients on M remain statistically distinguishable from Gaussian under the keyed pattern. A principled noncentral-χ² test yields calibrated p-values, enabling explicit control of false positives.\n\n**Mathematical_Formulation:**\n- Diffusion forward and reverse (DDIM) summary. Let β_t ∈ (0, 1), ᾱ_t = ∏_{i=0}^t (1 − β_i). The forward process x_t = √ᾱ_t x_0 + √(1 − ᾱ_t) ε, ε ~ N(0, I). DDIM reverse uses a noise predictor ε_θ to deterministically map x_T → x_0:\n  \\[\n  \\hat{x}_0^t = \\frac{x_t - \\sqrt{1-\\bar{\\alpha}_t}\\,\\epsilon_\\theta(x_t, c)}{\\sqrt{\\bar{\\alpha}_t}},\\quad\n  x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}\\,\\hat{x}_0^t + \\sqrt{1-\\bar{\\alpha}_{t-1}}\\,\\epsilon_\\theta(x_t, c).\n  \\]\n  Denote x_0 = D_\\theta(x_T, c) and the approximate inverse (DDIM inversion) x_T = D_\\theta^\\dagger(x_0, c).\n- Key embedding in Fourier space. Let F be the 2D FFT (unitary normalization), M be a circular low-frequency mask (radius r around DC), and k* ∈ C^{|M|} the secret key. Define\n  \\[\n  y = F(x_T),\\quad\n  y_i^{w} = \n  \\begin{cases}\n  k^*_i, & i \\in M\\\\\n  y_i, & i \\notin M\n  \\end{cases},\\quad\n  x_T^{w} = F^{-1}(y^{w}),\\quad\n  x_0 = D_\\theta(x_T^{w}, c).\n  \\]\n  Key variants: Tree-RingZeros (k*_i = 0), Tree-RingRand (k*_i \\sim \\mathcal{N}_\\mathbb{C}(0,1)), Tree-RingRings (k*_i = a_{\\rho(i)}, constant over radial bins ρ with a_{\\rho} ∼ N(0, σ_k^2)).\n- Detection via DDIM inversion and statistical test. Given x_0' (possibly attacked), compute x_T' = D_\\theta^\\dagger(x_0', ∅) and y' = F(x_T'). Estimate variance on the mask σ^2 = \\frac{1}{|M|}\\sum_{i\\in M} |y'_i|^2 and form the test statistic\n  \\[\n  \\eta = \\frac{1}{\\sigma^2}\\sum_{i\\in M} |k^*_i - y'_i|^2.\n  \\]\n  Under H_0: y'|_M ∼ \\mathcal{N}_\\mathbb{C}(0,\\sigma^2 I), η follows a noncentral χ² distribution with |M| degrees of freedom and noncentrality parameter λ = \\frac{1}{\\sigma^2}\\sum_{i\\in M} |k^*_i|^2. The p-value is\n  \\[\n  p = \\Pr\\left[\\chi^2_{|M|,\\lambda} \\le \\eta \\,\\middle|\\, H_0\\right],\n  \\]\n  and watermark is detected if p < α (equivalently, distance d = \\frac{1}{|M|}\\sum_{i\\in M}|k^*_i - y'_i| below threshold τ).\n- Invariance rationale. For image-space manipulation A (e.g., rotation R, convolution C, crop/scale S), the induced transform in Fourier space is structured (rotation of spectrum, magnitude modulation, radial compression). Choosing M to be circular and k* constant on rings (Tree-RingRings) yields approximate invariance: k* aligns with these transforms up to phase and radial re-parameterization, preserving small η.\n- Complexity overview (H × W = L pixels/channels in the latent/noise grid, S DDIM steps, C_θ cost per denoise step):\n  - FFT/IFFT: O(L log L) per sample.\n  - Generation (DDIM): O(S·C_θ), unchanged by watermark.\n  - Detection (DDIM inversion + FFT): O(S·C_θ + L log L).\n  - Mask scoring: O(|M|) (|M| ≪ L).\n\nExample Format:\n- Key overwrite: y_i^w = k^*_i 1[i∈M] + y_i 1[i∉M]\n- Inversion: x_T' = D_\\theta^\\dagger(x_0', ∅), y' = F(x_T')\n- Test statistic: η = (1/σ^2) ∑_{i∈M} |k^*_i − y'_i|^2, σ^2 = (1/|M|)∑_{i∈M} |y'_i|^2\n- Decision: p = Φ_{χ^2_{|M|,λ}}(η), detect if p < α\n- Complexity: O(S·C_θ + L log L) detection vs. O(S·C_θ) standard sampling\n\n**Computational_Properties:**\n- Time Complexity:\n  - Training: none (training-free).\n  - Watermarked generation: O(L log L) for FFT + O(S·C_θ) for DDIM (same S and C_θ as standard inference).\n  - Detection: O(S·C_θ) for DDIM inversion + O(L log L) for FFT + O(|M|) for scoring. In practice, S can be modest (e.g., 25–50) without significant loss in detection AUC.\n- Space Complexity:\n  - Additional memory: O(|M|) to store k* and O(L) temporary buffers for FFT/IFFT; does not change model parameter count.\n  - Runtime activations identical to standard DDIM sampling/inversion; FFT buffers scale linearly with L.\n- Parallelization:\n  - FFT/IFFT and mask operations are fully vectorizable across batch, channels, and spatial dimensions; efficiently handled by cuFFT on GPUs.\n  - DDIM (forward and inversion) parallelizes per step across spatial locations and channels; steps are sequential in t but highly parallel within each step.\n  - Detection across multiple images is embarrassingly parallel; key matching is a simple reduction over M.\n- Hardware Compatibility:\n  - GPU-efficient: FFT kernels and U-Net denoiser dominate runtime; no irregular memory access beyond standard convolutional inference.\n  - CPU-compatible with FFTW/NumPy FFT; the dominant cost remains diffusion steps.\n  - Memory bandwidth demand is unchanged vs. standard diffusion; added FFT buffers are small relative to U-Net activations.\n- Training vs. Inference:\n  - No training or fine-tuning required; watermarking operates as a plug-in to the sampling API.\n  - Detection uses inversion with unknown prompt; using c=∅ (empty prompt) yields strong recovery; the method remains robust across mismatched generation/detection step counts.\n- Parameter Count:\n  - Zero change to model parameters. The secret key k* is a small side datum of size |M|; multiple keys are supported (Tree-RingRand/Rings).\n- Numerical Stability:\n  - Use unitary-normalized FFT to maintain scale. Estimate σ^2 per image on M to normalize η and control sensitivity.\n  - Enforce Hermitian symmetry when overwriting complex Fourier coefficients to keep x_T^w real (or use rFFT/irFFT); avoid DC/very-low-frequency saturation to minimize FID impact.\n  - DDIM inversion error can grow with very high guidance scales; empirically, detection remains reliable up to strong guidance.\n- Scaling Behavior:\n  - With image/latent resolution L increasing, overhead scales O(L log L); choose |M| = O(L^α) with α < 1 to control any quality shift.\n  - Larger masks (radius r) increase robustness (higher AUC under attacks) at the cost of a slight FID increase; rings provide the best robustness vs. quality trade-off.\n  - Robustness degrades gracefully under extreme attacks (e.g., very large blur kernels, severe noise), with Tree-RingRings outperforming Zeros/Rand on average across rotations, crops/rescales, JPEG, blur, noise, and color jitter.\n\nImplementation-critical details and pitfalls:\n- Key design:\n  - Tree-RingRings: define discrete radial bins ρ = {r_1,…,r_K}; set k*_i = a_ρ(i) with a_ρ ∼ N(0, σ_k^2). Use low-frequency bands near DC; exclude the exact DC to reduce sensitivity to brightness shifts.\n  - Tree-RingZeros: maximally invariant to shifts/crops but departs from Gaussian; may slightly increase FID.\n  - Tree-RingRand: minimal distribution shift, supports many keys, but sensitive to rotations.\n- Mask and symmetry:\n  - Apply circular mask M in frequency domain; when using complex FFT, also assign conjugate pairs to preserve real IFFT.\n- Thresholding:\n  - Prefer p-value decision with α controlling FPR; alternatively, calibrate τ on a held-out set of watermarked/unwatermarked pairs to target a desired FPR.",
        "IMPLEMENTATION_GUIDANCE": "Integration_Strategy:\n- Where to integrate (generation/watermarking):\n  - PyTorch/Hugging Face diffusers (Stable Diffusion v1/v2):\n    - Modify StableDiffusionPipeline.prepare_latents(...) to watermark the initial latent noise tensor x_T immediately after sampling it from N(0,1).\n    - If using a custom sampler, insert after the line that initializes latents (commonly torch.randn_like(latents) or scheduler.init_noise_sigma*tensor).\n  - OpenAI guided-diffusion / DDPM codebases:\n    - Insert watermark right after sampling the initial Gaussian x_T in the sampling loop entry point (usually sample_loop or p_sample_loop).\n  - Latent vs pixel-space models:\n    - Latent diffusion (e.g., SD v1/v2): watermark x_T in latent space (shape [B, 4, H/8, W/8] for 512×512).\n    - Pixel-space diffusion (e.g., ImageNet 256×256): watermark x_T in image space (shape [B, 3, H, W]).\n- Watermark injection function (Tree-RingRings, recommended):\n  - Implement function apply_tree_ring(latents, key, mask, config):\n    - For each channel independently (or reuse same key across channels to boost SNR), compute Y = fftshift(fft2(latents[c])) in complex64/complex128.\n    - Replace coefficients at mask indices with key values: Y[mask] = key (complex Gaussian, ring-wise constant magnitudes/phases as specified below).\n    - Enforce Hermitian symmetry to keep the inverse real: for each (u,v) in mask set the conjugate at (-u,-v) to conj(key_uv). If using full complex FFT with ifft2.real(), minor asymmetries are tolerable but enforcing symmetry reduces reconstruction error.\n    - Return latents = ifft2(ifftshift(Y)).real, cast back to float32.\n  - Keep DC (zero frequency) unmodified to avoid sensitivity to color jitter; center mask excludes index (0,0).\n- Ring mask/key construction module:\n  - Precompute and cache on GPU per resolution:\n    - Use a centered radius map R(u,v) computed on the frequency grid with fftshift coordinates.\n    - Build K concentric annuli: r_k ≤ R < r_{k+1}, set mask_k = True within each annulus.\n    - For Tree-RingRings: assign a complex constant per ring: key_k ~ N(0,1) + i N(0,1), broadcast to mask_k (see Parameter_Settings for K and radii).\n    - Concatenate all mask_k into global mask M and corresponding key array k* with piecewise-constant values.\n- Detection path (DDIM inversion + test statistic):\n  - Implement invert_ddim(image, model, scheduler, num_inversion_steps, guidance_scale=0, prompt=\"\"):\n    - Use the same scheduler used in generation but run the forward DDIM “inverse” step xt → xt+1 using the formula in Section 2.1:\n      - Predict noise ϵθ(xt, c) with c empty/unconditional. Compute x̂0 = (xt − sqrt(1 − ᾱt) ϵθ)/sqrt(ᾱt).\n      - Compute xt+1 = sqrt(ᾱt+1) x̂0 + sqrt(1 − ᾱt+1) ϵθ(xt, c).\n    - Initialize x0 from the provided image (for latent diffusion, encode image with VAE to latent). Iterate t = 0..T−1 to get x_T'.\n    - Note: if the exact number of generation steps is unknown, choose 50 detection steps; inversion is robust across mismatched step counts.\n  - Implement compute_detection_score(x_T', mask, key):\n    - y = fftshift(fft2(x_T')), extract y_M = y[mask].\n    - L1 score (threshold-based): d = mean(abs(y_M − key)).\n    - P-value (recommended): estimate σ^2 = mean(abs(y_M)^2); η = (1/σ^2) * sum(abs(key − y_M)^2); compute p = CDF_noncentral_chi2(df=|M|, λ=(1/σ^2)*sum(abs(key)^2), at η). Use scipy.stats.ncx2.cdf.\n- API and key management:\n  - Maintain a KeyRegistry mapping key_id → {resolution, K, radii, key complex vector, mask indices, version}. Version on resolution change.\n  - Embed key_id alongside image metadata in your system database (not in the image) to allow lookups and audits.\n  - Rotate keys periodically; keep old keys for backward detection during grace periods.\n- Compatibility and frameworks:\n  - PyTorch: use torch.fft.fft2/ifft2, torch.fft.fftshift/ifftshift, complex64. Enable autocast off for FFT ops to avoid precision issues in float16.\n  - TensorFlow/JAX: use tf.signal.fft2d/ifft2d or jnp.fft.fft2/ifft2 with equivalent shift ops; ensure complex dtypes and symmetry handling.\n- Migration path:\n  - If currently using post-hoc image Fourier watermarking: remove that post-processing; instead, hook watermarking into noise initialization only.\n  - If using non-DDIM samplers for generation (Euler, PLMS, DPM-Solver): generation can continue unchanged, but for detection you must implement DDIM inversion with the same denoiser ϵθ; if model was not sampled with DDIM, detection performance may degrade (see Expected_Outcomes).\n- Dependencies and hardware features:\n  - FFT uses cuFFT on NVIDIA GPUs automatically via PyTorch; no custom kernels required.\n  - Precompute masks/keys per (H,W) and cache on each device to avoid per-request overhead.\n\nParameter_Settings:\n- Key types:\n  - Tree-RingRings (recommended default): robust under most attacks and minimal FID impact.\n  - Tree-RingRand: use when multiple orthogonal keys and lowest distribution shift are priorities; weaker to rotations.\n  - Tree-RingZeros: only for maximum invariance experiments; expect larger FID (+1–3) and distribution shift.\n- Rings configuration (Tree-RingRings):\n  - Number of rings K: 4–10. Default: 6 for 64×64 latents; 8 for 96–128; 10 for ≥160.\n  - Outer radius r_out (in frequency pixels after fftshift):\n    - Latent 64×64 (SD at 512×512): r_out = 10–16; default 12–14.\n    - Pixel 256×256: r_out = 32–48; default 40.\n  - Inner radius r_in: 2–4 pixels to exclude DC; default 3 (latent 64×64), 6 (pixel 256×256).\n  - Ring widths: equal widths or slightly increasing (1–3 px in 64×64; 4–6 px in 256×256).\n  - Complex ring values: per ring k ~ N(0,1) + i N(0,1); optionally scale by s ∈ [0.9, 1.1] to match ambient variance; keep identical across channels to boost SNR.\n- FFT details:\n  - Dtype: complex64 for Y; input/output real float32 latents.\n  - Enforce Hermitian symmetry: for each (u,v), set Y[-u mod H, -v mod W] = conj(Y[u,v]) when overriding coefficients.\n- DDIM step counts:\n  - Generation: 25–100 steps typical. Detection (inversion): 25–100; default 50. Mismatch between generation/detection steps reduces AUC by ≤0.02 in tested ranges.\n- Guidance scale during detection:\n  - Use unconditional inversion (guidance_scale = 0, empty prompt). Robust up to training-time scales 5–15; still usable at 18 with slight AUC drop.\n- Thresholding and statistical testing:\n  - L1 threshold τ: tune to target FPR. For SD 512×512, latent 64×64, Tree-RingRings, τ typically in [0.18, 0.28] (normalize by |M| and by per-image σ if using L2).\n  - P-value threshold α: choose α = 0.01 (regulatory-friendly), α ∈ [1e-2, 1e-6] depending on your risk tolerance. Report p; declare detection if p ≤ α.\n- Critical vs robust parameters:\n  - Critical: ring radii (r_in, r_out, widths), K, Hermitian symmetry, unconditional inversion usage, correct resolution mask.\n  - Robust: detection steps (25–100), using same key across channels vs per-channel keys, small variations in key scaling s.\n- Scale-dependent settings:\n  - For larger images (e.g., 1024×1024, latent 128×128): increase r_out proportionally (≈H/5 to H/3), K up to 10–12 to maintain annulus SNR.\n- Hardware-dependent settings:\n  - Consumer GPUs (8–12 GB, e.g., RTX 3060/3070): prefer batch watermarking with mask/key cached on device; FFT on 64×64×4 per sample is negligible (<0.2 ms).\n  - Datacenter GPUs (A100, H100): use float32 FFT; if mixed precision everywhere else, keep watermarking ops outside autocast to avoid fp16 complex instability.\n\nApplication_Conditions:\n- Beneficial scenarios:\n  - Any diffusion service needing invisible provenance with robustness to common edits: crops (~75%), JPEG (quality ≥25), blurs (up to 8×8), Gaussian noise (σ≤0.1), rotations (≤90° and even 180°).\n  - Multi-tenant API providers needing per-tenant keys without retraining.\n- Hardware requirements:\n  - Same as standard inference; FFT cost is negligible for latent sizes (≤128×128). No extra VRAM beyond mask/key buffers (tens of KB).\n- Scale considerations:\n  - Gains are consistent from 256×256 pixel models to 512×512 latent models; robustness increases with more frequency bins in M (larger latents/images).\n- Task compatibility:\n  - Text-to-image (Stable Diffusion v1/v2), unconditional diffusion (ImageNet 256). Works with both.\n  - Neutral impact on prompt adherence (CLIP score unchanged within ±0.001 in paper).\n- Alternatives comparison:\n  - Prefer over post-hoc image Fourier watermarks when robustness and invisibility are priorities.\n  - If you must support third-party detection without model access, consider alternative schemes; Tree-Ring requires model for inversion.\n- Resource constraints:\n  - For strict latency SLAs, Tree-Ring adds ~0.5–2 ms per sample end-to-end (FFT + mask ops) and ~tens of ms for detection inversion at 50 steps; acceptable for asynchronous auditing pipelines.\n\nExpected_Outcomes:\n- Performance and quality:\n  - FID impact (Stable Diffusion 512×512): ΔFID ≈ 0.2–0.7 vs. no watermark (Tree-RingRand/Rings). CLIP score change ≈ 0.000–0.001.\n  - Detection (clean): AUC ~ 0.999–1.000, TPR@1%FPR ≈ 0.999–1.000.\n  - Detection (adversarial average over 6 transforms): AUC ≈ 0.95–0.98 (Tree-RingRings), TPR@1%FPR ≈ 0.60–0.70. Rotation robustness significantly better for Rings than Rand.\n- Timeline:\n  - Immediate deployment: no retraining required. Key generation and mask caching are instantaneous.\n  - Detection robustness stable across generation step counts and guidance variations without needing per-run calibration.\n- Trade-offs:\n  - Tree-RingRings vs Tree-RingRand: Rings improves rotation and overall adversarial robustness; Rand yields the lowest distribution shift but weaker to rotation.\n  - Tree-RingZeros: strongest invariance to some edits but noticeable FID increase; not recommended for production unless quality tolerances allow.\n  - DDIM requirement: detection depends on viable DDIM inversion; if generations use non-DDIM samplers and inversion is poor, detection AUC drops.\n- Failure modes:\n  - Using a different model for detection than generation (mismatched ϵθ or VAE): large p-values (false negatives).\n  - Wrong latent resolution or missing fftshift alignment: mask misalignment, d or p not significant.\n  - Excessive manipulations: extreme color jitter (large DC shift), heavy noise (σ>0.2), or very strong blur (kernel > 40 at 512×512) can degrade detection below target TPR.\n  - Very high guidance scales (>18–20) or classifier-free guidance implementation mismatches during inversion reduce accuracy.\n- Debugging indicators:\n  - Sanity check inversion: generate with known x_T; invert and compute mean squared error between x_T and x_T' (expect small, e.g., per-pixel MSE ≪ 1.0 in latent space).\n  - Frequency histogram of y = F(x_T') over M: should be close to complex Gaussian for non-watermarked images; watermarked should align closely with key.\n  - P-values on natural non-watermarked images: uniform-like above 0.1; spurious low p-values indicate mask misalignment or σ^2 estimation bug.\n- Hardware-specific outcomes:\n  - On A100/H100: additional latency per sample for watermarking <0.1 ms (64×64 latents); detection inversion 50 steps typically <50 ms per image with batch size 1–2.\n  - On RTX 3060: watermarking <0.3 ms; detection inversion 50 steps ~80–120 ms per image.\n\nValidation procedures:\n- Unit tests:\n  - Inject a known key into synthetic Gaussian noise; verify that detection recovers p ≤ 1e-6 consistently.\n  - Round-trip: watermark → generate → invert → detect; assert median p ≤ 1e-8 (clean).\n- ROC calibration:\n  - Build datasets of 1,000 watermarked and 1,000 unwatermarked generations across diverse prompts and seeds.\n  - Compute ROC and TPR@1%FPR; verify AUC ≥ 0.99 (clean), ≥ 0.95 (adversarial average).\n- Attack robustness sweep:\n  - Rotations: 0–180° in 45° steps; JPEG quality 10–90; crop+scale 10–90%; Gaussian blur kernel 5–40; noise σ 0–0.4; brightness 1–16. Verify trends similar to Figure 7: graceful degradation and retained detectability at moderate strengths.\n- Operational checks:\n  - Log key_id and configuration version for each generation. During detection, load matching configuration by image size. If unknown, try all known configurations and adjust α for multiple testing (e.g., Bonferroni).\n\nTroubleshooting (common issues and fixes):\n- Detection p-values high on watermarked images:\n  - Check that fftshift/ifftshift order matches your FFT calls.\n  - Ensure mask indices are built for the exact latent resolution used during generation.\n  - Verify unconditional inversion (guidance_scale=0) and correct scheduler (DDIM).\n  - Enforce Hermitian symmetry when writing key into frequency domain.\n- Quality drop (FID increases >1.0):\n  - Reduce r_out or K; ensure key variance ≈ 1.0 per coefficient; avoid Tree-RingZeros in production.\n  - Use the same key across channels rather than independent keys to minimize distribution shift.\n- Rotation robustness weaker than expected:\n  - Confirm ring values are constant within each annulus; avoid angular patterns.\n  - Increase ring width slightly (by 1 pixel in 64×64 latents) to reduce discretization artifacts."
    }
]