[
    {
        "DESIGN_INSIGHT": "### DESIGN_INSIGHT_HIGH: [Temporal Watermark Propagation (TWP) – k-Stride Shared Distortion Field for Efficient Video Watermarking]\nVideo Seal replaces per-frame watermark embedding with a differentiable k-stride propagation of a single distortion field across adjacent frames, eliminating the need to watermark every frame while preserving robustness under temporal edits and compression. Instead of producing a fresh watermark for each frame, the embedder outputs one distortion at stride positions and copies it to the k−1 following frames.\n\nThe core mechanism is a shared distortion sequence defined by\n\\[\nw_i = \n\\begin{cases}\n\\mathrm{Embedder}(x_i, m), & \\text{if } i \\bmod k = 0,\\\\\nw_{i-1}, & \\text{otherwise},\n\\end{cases}\n\\]\nwhere \\(x_i \\in \\mathbb{R}^{3 \\times 256 \\times 256}\\) is the i-th frame, \\(m \\in \\{0,1\\}^{\\text{nbits}}\\) the payload, and \\(w_i\\) the watermark distortion. Extraction aggregates per-frame soft logits via\n\\[\n\\hat{m}_k = \\mathbf{1}\\!\\left( \\frac{1}{T}\\sum_{i=1}^{T} \\tilde{m}_{i,k} > 0 \\right),\n\\]\nto recover the binary message. This design is end-to-end differentiable, allowing the embedder/extractor to co-adapt to propagation artifacts (e.g., motion-induced “shadows”) during training, and reduces embed compute from \\(O(T)\\) to \\(O(T/k)\\) with unchanged activation memory, enabling streaming.\n\nFundamentally different from prior “watermark-skip” heuristics (which leave unwatermarked frames and degrade extraction), TWP maintains a persistent watermark signal in all frames and contrasts with 3D/4D video CNNS (e.g., models requiring full clips in memory) by remaining purely 2D and streamable. It also differs from ItoV-style “time-as-channel” merges by not requiring whole-clip processing, thus offering linear time and memory scaling with clip length while retaining robustness under codecs and geometric edits.\n\n\n### DESIGN_INSIGHT_MEDIUM: [Low-Resolution Distortion Upscaling with Strength Control (\\(\\alpha_w\\)) – Resolution-Agnostic 2D Embedder/Extractor]\nVideo Seal modifies standard image watermarking pipelines to become resolution-agnostic for video by predicting only a low-resolution distortion and upscaling it to the native frame size with explicit strength control. Instead of generating a full-resolution watermarked frame, the embedder outputs a distortion \\(w\\) at \\(256 \\times 256\\) and adds its upscaled version to the original frame.\n\nThe mechanism is\n\\[\nw = \\mathrm{Embedder}(\\mathrm{resize}(x), m),\\quad\nx_w = x + \\alpha_w \\cdot \\mathrm{resize}(w),\n\\]\nwhere \\(\\alpha_w\\) controls the imperceptibility/robustness trade-off at inference without retraining. Extraction resizes frames to the fixed resolution before the ViT-based extractor processes them, yielding soft logits for aggregation. This decouples watermark synthesis from native resolution \\(H \\times W\\), keeping compute and memory constant per frame, while supporting arbitrary image/video sizes and lengths.\n\nThis differs from prior full-resolution or 3D models by operating purely in 2D with distortion addition rather than predicting \\(x_w\\) directly, enabling streamability and mobile feasibility. It also unifies image and video watermarking under the same embedder/extractor, with the ViT patch encoder plus lightweight CNN patch decoder and average pooling mapping features to \\(\\mathbb{R}^{\\text{nbits}}\\) efficiently. Practically, it delivers consistent robustness to geometric and codec transforms across resolutions while offering a tunable \\(\\alpha_w\\) curve for quality versus detection confidence.\n\n\n### DESIGN_INSIGHT_MEDIUM: [Codec-Aware Multistage Training with Straight-Through Estimator (STE) and Extractor Freeze]\nVideo Seal replaces codec simulation networks with a training-time straight-through estimator and adds extractor-only fine-tuning to decouple imperceptibility from robustness. The objective combines message reconstruction, perceptual, and adversarial terms:\n\\[\nL = \\lambda_{\\mathrm{disc}} L_{\\mathrm{disc}} + \\lambda_i L_i + \\lambda_w L_w,\n\\]\nwith \\(L_w=\\frac{-1}{\\text{nbits}}\\sum_k \\mathrm{BCE}(m_k,\\tilde{m}_k)\\), \\(L_i=\\mathrm{MSE}(x,x_w)\\), and a patch discriminator \\(L_{\\mathrm{disc}}\\) trained via dual-hinge loss. A gradient-norm balancer rescales losses:\n\\[\n\\tilde{\\lambda}_k = \\frac{\\lambda_k}{\\sum_{k'} \\lambda_{k'}} \\cdot \\frac{R}{\\|\\nabla_\\theta L_k\\| + \\epsilon}.\n\\]\n\nNon-differentiable codecs are handled via STE:\n\\[\nx_{\\mathrm{aug}} = x_w + \\mathrm{nograd}\\!\\big(T(x_w) - x_w\\big),\n\\]\nwhere \\(T\\) is the actual FFmpeg/PyAV transform (e.g., H.264/H.265). Training is multistage: image pre-training → hybrid image/video post-training with codec augmentations → extractor fine-tuning with the embedder frozen. The freeze stage improves bit accuracy under strong compressions and combined distortions without altering \\(x_w\\) (thus preserving PSNR/SSIM), effectively breaking the typical imperceptibility–robustness trade-off.\n\nCompared to prior works that either avoid codec training, rely on differentiable JPEG only, or train separate compression nets, the STE-based codec inclusion preserves realism of distortions while keeping the pipeline simple and end-to-end. The extractor-only fine-tuning is a key departure from standard joint training, providing targeted robustness gains post convergence and enhancing reliability in high-CRF and combined geometric+codec regimes.",
        "EXPERIMENTAL_TRIGGER_PATTERNS": "**Task_Performance_Signatures:**\n- Benchmarks (domain-relevant)\n  - SA-V (video robustness, 5s clips, original resolution; extraction aggregated across frames):\n    - Expect improved robustness under combined distortions (H.264 CRF=30 + 50% crop + brightness 0.5): bit accuracy ≈0.73 and log10(p) ≈−8.1. This indicates detection at ≤10^-6 FPR in most cases.\n    - Under H.264 alone: CRF=30 bit accuracy ≈0.97, CRF=40 ≈0.89, CRF=50 ≈0.72, CRF=60 ≈0.70 (Video Seal remains competitive vs baselines as compression intensifies).\n    - Geometric alone (e.g., perspective d=0.5): bit accuracy ≈0.91; crop r=0.71 (≈50% area) ≈0.97; rotation ±10° ≈0.94; horizontal flip ≈0.99. Higher scores due to differentiable geometric training and ViT-based extractor.\n  - SA-1B (image robustness, original resolution with downscale/upscale):\n    - Combined distortions (JPEG Q∈{40,60,80} + crop 0.71 + brightness 0.5): bit accuracy ≈0.89–0.92 and log10(p) ≈−17.2 to −19.5; improved vs MBRS/CIN where geometric robustness degrades to chance-level.\n    - Geometric transformations averaged: bit accuracy ≈0.83; valuemetric averaged: ≈0.93; compression averaged: ≈0.99. Expect stable to improved performance versus image-only baselines, especially when geometry + codec are present.\n  - COCO (image generalization):\n    - Combined distortions: bit accuracy ≈0.74 and log10(p) ≈−6.7 to −16.9 (depending on JPEG quality); maintains generalization with similar imperceptibility as SA-1B while achieving higher robustness under geometry + compression.\n- Imperceptibility metrics (quality vs robustness trade-off)\n  - SA-V: PSNR ≈48.0 dB, SSIM ≈0.998, LPIPS ≈0.013, VMAF ≈93.8 at nominal αw=2.0 (expect within ±1–2 VMAF points of MBRS; better SSIM/PSNR than most baselines).\n  - SA-1B: PSNR ≈47.4 dB, SSIM ≈0.999, LPIPS ≈0.009 (negligible quality drops while retaining robustness).\n  - Trade-off curves: At VMAF ≥94 with combined distortions, Video Seal maintains higher −log10(p) than MBRS/TrustMark; expect comparable robustness at high quality for codec-only cases.\n- Efficiency and scaling signatures\n  - Embedding (per 1 second of video): GPU ≈0.42 s; CPU ≈1.14 s; 42.0 GFlops. Extraction: GPU ≈0.11 s; CPU ≈0.69 s; 3.1 GFlops. Expect order-of-magnitude faster extraction than embedder and competitive throughput vs baselines; WAM notably slower at embedding due to heatmap computation.\n  - Temporal watermark propagation with k=4 yields ≈4× embedding speed vs k=1 with negligible robustness drop on SA-V (combined distortions remain ≈0.72–0.74 bit accuracy). Larger k improves speed but may introduce motion “shadow” artifacts; keep k≤4 for fast-motion content.\n  - Extraction aggregation across T frames: bit accuracy increases monotonically with T; expect diminishing returns beyond first 3–5 s; L1-norm weighting gives marginal gains (<+0.01 bit accuracy) over simple averaging.\n- Training regimen effects\n  - Mixed training (image pretraining → video training with differentiable codec augmentation): faster initial convergence on video robustness (bit accuracy gains of +0.10–0.20 at H.264 CRF 50–60 vs image-only), with PSNR unchanged (trend stable around 45–50 dB).\n  - Extractor fine-tuning (embedder frozen): expect +0.03–0.05 absolute bit accuracy under H.264 CRF=30 and under combined distortions without PSNR change; robustness improves particularly on rarely seen augmentations.\n- Comparative robustness expectations\n  - Expect improved/stable performance vs MBRS/TrustMark/WAM on geometric and combined distortions; MBRS/TrustMark may have higher −log10(p) at equal VMAF in codec-only settings due to larger payloads but degrade sharply with geometry. Video Seal remains competitive under codec-only while being superior in geometry+codec scenarios.\n- Cross-domain task placeholders (orthogonal; not applicable)\n  - Language modeling (lambada_openai, wikitext, ptb), reading comprehension (squad_completion, squad_v2, narrativeqa), commonsense reasoning (hellaswag, piqa, social_iqa, commonsenseqa), factual QA (arc_easy, arc_challenge, boolq, openbookqa), context resolution (winogrande, winograd), other tasks (swde, fda):\n    - Expect unchanged or not-applicable performance; watermarking is orthogonal. If measured, accuracy should remain within ±1% of baseline due to post-hoc pixel-level signal with nominal αw, and no model-weight changes.\n\n**Architectural_Symptoms:**\n- Training characteristics\n  - Smoother convergence when switching from image-only to mixed training (image→video) with codec augmentations; BCE (message) + MSE (imperceptibility) + adversarial discriminator loss show reduced variance after ~10 epochs of video training.\n  - Balancer (gradient-norm reweighting) yields stable loss ratios; no NaN occurrences; less likely to diverge when codec augmentation is added via straight-through estimator.\n  - Fine-tuning extractor with embedder frozen: PSNR curve remains flat; bit accuracy steadily improves; indicates successful decoupling of imperceptibility vs robustness.\n  - Video-only training without prior image pretraining is a negative symptom: low bit accuracy and unstable training; mixed schedule restores stability.\n- Runtime/memory behaviors\n  - Constant inference memory w.r.t. source resolution due to downscale to 256×256 and upscale of distortion; memory scales linearly with frames T (O(T·256^2)) rather than with original H×W.\n  - Throughput patterns: near-constant GPU time per processed second across varying resolutions; CPU-bound when resizing large frames; GPU utilization is higher during extractor runs (ViT small + lightweight CNN decoder).\n  - No OOM at batch sizes used (1 clip/GPU of 32 frames) on V100 16 GB; stable as model depth/width increases modestly (16M U-Net embedder, 24M ViT extractor).\n  - Temporal propagation: embedding time decreases ≈linearly with step-size k; extraction time increases ≈linearly with number of frames T. Excessively large k correlates with motion-linked “shadow/glitter” artifacts—visible symptom suggesting to reduce k.\n- Profiling signatures\n  - Embedding: 42.0 GFlops; GPU ≈0.42 s/sec; CPU ≈1.14 s/sec. Extraction: 3.1 GFlops; GPU ≈0.11 s/sec; CPU ≈0.69 s/sec. Expect higher GPU efficiency than WAM; similar or better than MBRS/TrustMark on extraction.\n  - Quality-discriminator behavior: discriminator logits for xw approach those for x (raw hinge loss small) as training progresses; if hinge loss for watermarked frames remains high, expect poor imperceptibility or overstrong αw.\n  - Robustness monitors: log10(p) < −6 under combined distortions on images; < −8 on videos indicates working robustness. If log10(p) trends > −2 under combined distortions, symptom of insufficient codec/geometric augmentation or too-large k/T too small at extraction.\n  - Hardware-specific: CPU-GPU transfer overhead modest; end-to-end latency dominated by resizing and codec I/O; consistent speed regardless of sequence resolution due to 2D per-frame design. If throughput degrades with resolution, symptom indicates bypassing the 256×256 pathway.",
        "BACKGROUND": "Title: Video Seal: Open and Efficient Video Watermarking\n\nHistorical Technical Context:\nNeural watermarking has evolved along two major lines: traditional codec-integrated video watermarking and deep learning-based image watermarking extended to video. Prior to end-to-end neural approaches, video watermarking commonly operated inside compression standards (e.g., H.264/AVC, HEVC), embedding signals via Reversible Variable Length Codes (RVLC) or small perturbations of motion vectors, designed to be imperceptible yet recoverable after decoding. In parallel, image watermarking matured from spatial and frequency-domain methods (DCT, DWT, DFT-based schemes) to CNN-based encoders/decoders with adversarial training and robustness-oriented augmentation (HiDDeN, MBRS, CIN, TrustMark, WAM). These image methods established the embedder–extractor paradigm (often U-Nets for embedding and CNNs/Transformers for extraction), differentiable robustness augmentation (e.g., JPEG), and practical resolution scaling tricks.\n\nDeep learning for video watermarking introduced 3D/4D tensor processing (RivaGAN, DVMark, VHNet), multiscale spatio-temporal designs, and differentiable approximations of codecs via learned CompressionNet or analytical layers. However, these architectures frequently required feeding long clips or entire videos, leading to heavy memory and compute footprints that impeded real-world deployment on high-resolution, long-duration content. ItoV adapted 2D image architectures to video by folding time into channels, enabling faster convergence but still requiring large tensors and complicating inference. The rapid adoption of generative video models (e.g., Sora, MovieGen) and regulatory requirements to watermark both synthetic and real content intensified the need for open, efficient, and robust video watermarking that survives social-media pipelines, editing, and compression.\n\nStandard architectural paradigms prior to this work included: U-Net-based embedders with skip-connections for imperceptibility; extractor CNNs or ViTs for bit recovery; adversarial discriminators for perceptual quality; differentiable augmentation pipelines for robustness to valuemetric and limited geometric transforms; and training conducted at fixed low resolutions (often ≤128×128) or short clips (≤64 frames). End-to-end differentiability across real codecs remained uncommon due to non-differentiability, and reproducible open-source video watermarking models were scarce.\n\nTechnical Limitations:\n- Per-frame embedding is computationally prohibitive for high-resolution, high-FPS videos: if an embedder requires C_embed FLOPs per 256×256 frame, watermarking T frames costs O(T·C_embed). Without propagation, scaling to full resolution adds O(HW) resizing and accumulation per frame; with long videos, CPU/GPU transfer overhead dominates latency.\n- 3D/4D video tensors or channel-folded temporal processing induce memory scaling O(T·H·W·C), often exceeding GPU memory limits for long clips or 4K content, and complicating streamable inference.\n- Robustness gaps: prior image watermarking, trained largely on valuemetric distortions, underperformed on geometric transforms (crop, perspective, rotations) and on combinations with video codecs (e.g., H.264 CRF≥30), leading to degraded bit accuracy and detection confidence.\n- Non-differentiable codecs block gradient flow; learned or analytical approximations introduce approximation error, and straight-through training can be unstable without careful loss balancing, schedules, and augmentations.\n- Synchronization and aggregation challenges: watermarking sparse frames degrades extraction when unwatermarked frames dominate, and naive aggregation can dilute signal under temporal edits and compression.\n- Lack of reproducible baselines and payload-aware comparison: differing nbits made bit accuracy incomparable across methods; absence of open checkpoints hindered fair evaluation, capacity analysis, and deployment studies.\n\nPaper Concepts:\n- Temporal Watermark Propagation: A technique to avoid watermarking every frame by embedding every k frames and copying the distortion to neighbors. Formally, for downscaled frame \\( x_i \\in \\mathbb{R}^{3 \\times 256 \\times 256} \\) and message \\( m \\in \\{0,1\\}^{\\text{nbits}} \\),\n  \\( w_i = \\begin{cases}\n  \\text{Embedder}(x_i, m), & \\text{if } i \\bmod k = 0,\\\\\n  w_{i-1}, & \\text{otherwise}.\n  \\end{cases} \\)\n  This reduces embedding compute to \\( O\\!\\left(\\frac{T}{k} \\cdot C_\\text{embed}\\right) \\) while preserving robustness by maintaining a consistent temporal signal.\n\n- Resolution Scaling and Watermark Strength: Watermarking at fixed 256×256 and upscaling the distortion to original resolution with a tunable strength \\(\\alpha_w\\). For a high-resolution frame \\( x \\in \\mathbb{R}^{3 \\times H \\times W} \\),\n  \\( w = \\text{Embedder}(\\text{resize}(x), m),\\quad x_w = x + \\alpha_w \\cdot \\text{resize}(w). \\)\n  \\(\\alpha_w\\) trades imperceptibility vs. robustness and can be adjusted at inference to meet quality constraints (PSNR/VMAF) or detection requirements.\n\n- Binary Message Lookup Table (LUT) Embedding: The message is injected at the U-Net bottleneck via a learned table \\( T \\in \\mathbb{R}^{\\text{nbits} \\times 2 \\times d_\\text{msg}} \\). For bit index \\( k \\) and value \\( m_k \\in \\{0,1\\} \\),\n  \\( e_k = T[k, m_k, :] \\in \\mathbb{R}^{d_\\text{msg}},\\quad \\bar{e} = \\frac{1}{\\text{nbits}} \\sum_{k=1}^{\\text{nbits}} e_k. \\)\n  The repeated \\( \\bar{e} \\in \\mathbb{R}^{d_\\text{msg} \\times 32 \\times 32} \\) is concatenated with the image latent \\( z \\in \\mathbb{R}^{d_z \\times 32 \\times 32} \\) to yield \\([z; \\bar{e}]\\), enabling compact, flexible message conditioning.\n\n- Straight-Through Estimator (STE) for Non-differentiable Codecs: To train through real codecs \\( T(\\cdot) \\) (e.g., H.264), gradients are approximated by identity:\n  \\( x_\\text{aug} = x_w + \\text{nograd}\\!\\left(T(x_w) - x_w\\right), \\)\n  so \\(\\frac{\\partial x_\\text{aug}}{\\partial x_w} \\approx I\\) while applying the true distortion in the forward pass, improving robustness to real-world pipelines without differentiable surrogates.\n\n- Gradient-Norm Balancer for Multi-loss Training: Loss components are adaptively rescaled using gradient norms at the embedder’s last layer to stabilize training:\n  \\( \\tilde{\\lambda}_k = \\frac{\\lambda_k}{\\sum_{k'} \\lambda_{k'}} \\cdot \\frac{R}{\\|\\nabla_\\theta L_k\\| + \\varepsilon},\\quad \\tilde{L} = \\sum_k \\tilde{\\lambda}_k L_k, \\)\n  where \\( k \\in \\{\\text{disc}, i, w\\} \\) indexes adversarial, perceptual (MSE), and extraction (BCE) losses, \\( R=1 \\), and \\( \\theta \\) denotes the final Conv2D parameters.\n\n- Payload-aware Detection Confidence (p-value) and Capacity: For observed bit accuracy \\( p_a = \\text{bit accuracy}(m, \\hat{m}) \\) over \\(\\text{nbits}\\) independent Bernoulli trials, the detection p-value is\n  \\( \\text{p-value} = \\sum_{k \\ge \\text{nbits}\\cdot p_a} \\binom{\\text{nbits}}{k} 2^{-\\text{nbits}}, \\)\n  often reported as \\( \\log_{10}(\\text{p-value}) \\). Under a Binary Symmetric Channel with error \\( p_e = 1 - p_a \\), Shannon capacity per bit is \\( c(p_e) = 1 - h(p_e) \\) with \\( h(p_e) = -p_e\\log_2 p_e - (1-p_e)\\log_2(1-p_e) \\), and total capacity \\( C = \\text{nbits}\\cdot c(p_e) \\). These metrics normalize comparisons across payload sizes and set thresholds for low false positive rates (e.g., \\(\\log_{10}(\\text{p}) < -6\\)).\n\nExperimental Context:\nThe evaluation prioritizes robustness under realistic video pipelines and imperceptibility at scale, with an explicit efficiency focus. Tasks are discriminative (bit recovery) and systems-level (embed/extract speed). Robustness is measured across transformation families—valuemetric (brightness, contrast, hue, saturation, blur, median), geometric (flip, crop, resize with AR changes, rotation, perspective), codecs (JPEG for images; H.264/H.265 for videos; RGB/H.264 variants), and combined sequences (e.g., H.264 CRF=30 + 50% crop + brightness 0.5). Primary metrics include bit accuracy and its payload-aware confidence \\(\\log_{10}(\\text{p-value})\\); quality is assessed via PSNR, SSIM, LPIPS, and the video-specific VMAF. Efficiency is reported in FLOPs and wall-clock time per second of video on CPU/GPU.\n\nTraining uses SA-1B images (∼500k at 256×256) for pre-training and SA-V videos (51k, 24 fps, 240p–4K) for mixed training with differentiable augmentations and real codecs via STE. The embedder is an efficient U-Net (∼16M params) that injects the 96-bit payload via a LUT at the bottleneck, and the extractor is a ViT-Small (patch size 16, \\(d=d'=384\\), ∼24M params) with a patch decoder and average pooling to \\(\\mathbb{R}^{\\text{nbits}}\\). Inference uses resolution scaling and temporal propagation with \\(k=4\\); extraction aggregates soft logits frame-wise by averaging. Video Seal demonstrates higher robustness than strong image baselines under challenging combined transforms, achieving, for example, \\(\\log_{10}(\\text{p}) \\approx -6.1\\) at high compression plus geometry, while maintaining high imperceptibility (video PSNR ≈ 48.0 dB, SSIM ≈ 0.998, VMAF ≈ 93.8). Efficiency is competitive (embed ≈ 42 GFLOPs, extract ≈ 3.1 GFLOPs; GPU times per second of video ≈ 0.42 s for embedding and ≈ 0.11 s for extraction), and ablations show that image pre-training followed by video training with codec augmentation and extractor fine-tuning yields the most stable and robust optimization without sacrificing PSNR.",
        "ALGORITHMIC_INNOVATION": "**Core_Algorithm:**\n- Replace per-frame 3D/clip-based video watermarking with a 2D embedder–extractor that operates at a fixed spatial resolution (256×256) and introduces temporal watermark propagation to avoid watermarking every frame.  \n- Embed a binary payload via a lookup-table message embedding at the U-Net bottleneck; compute a low-resolution distortion w for one frame every k frames, upscale it to the native frame resolution, and copy this distortion to the next k−1 frames.  \n- Train end-to-end with differentiable geometric/valuemetric transforms and non-differentiable video codecs handled by a straight-through estimator; use a multistage regimen (image pre-train → hybrid image/video post-train → embedder freeze + extractor fine-tune).  \n- Extract per-frame “soft” bits with a ViT-Small + patch-decoder, then aggregate across frames by averaging logits to produce a robust final message.\n\n**Key_Mechanism:**\n- The core insight is that most video codecs and temporal editing attenuate high-frequency, per-frame watermark energy; propagating a consistent distortion across short windows (k frames) preserves a coherent signal that survives temporal compression and motion compensation.  \n- Downscale→embed→upscale decouples training/inference from native resolution, enabling constant-cost watermark synthesis while controlling robustness via a global scaling factor.  \n- STE on codec transforms provides gradients that align the embedder with compression artifacts; multistage training plus loss-balancing stabilizes optimization and extractor fine-tuning improves robustness without sacrificing imperceptibility.\n\n**Mathematical_Formulation:**\nLet a video be frames {x_i} with x_i ∈ ℝ^{3×H×W}, the payload m ∈ {0,1}^{n}, and fixed working resolution h=w=256. Denote resize(·) as bilinear resampling to/from 256×256.\n\n1) High-resolution embedding and scaling:\n\\[\nw = E(\\mathrm{resize}(x),\\, m), \\qquad\nx_w = x + \\alpha_w \\cdot \\mathrm{resize}(w),\n\\]\nwhere \\(E\\) is the U-Net embedder, \\(\\alpha_w>0\\) controls imperceptibility/robustness.\n\n2) Temporal watermark propagation (segment size k):\n\\[\nw_i =\n\\begin{cases}\nE(\\mathrm{resize}(x_i),\\, m), & \\text{if } i \\bmod k = 0,\\\\\nw_{i-1}, & \\text{otherwise},\n\\end{cases}\n\\qquad\nx_{w,i} = x_i + \\alpha_w \\cdot \\mathrm{resize}(w_i).\n\\]\n\n3) Non-differentiable transforms with STE (e.g., H.264/H.265):\n\\[\nx_{\\mathrm{aug}} = x_w + \\mathrm{nograd}\\!\\left( T(x_w) - x_w \\right),\n\\]\nwhere \\(T\\) is the codec or other non-differentiable op, and nograd blocks gradients.\n\n4) Message embedding at the bottleneck via a lookup table \\(T \\in \\mathbb{R}^{n \\times 2 \\times d_{\\mathrm{msg}}}\\):\n\\[\nz_{\\mathrm{msg}} = \\frac{1}{n}\\sum_{k=1}^{n} T[k, m_k, :], \\quad\n\\tilde z = \\mathrm{concat}\\!\\left(z, \\mathrm{repeat}(z_{\\mathrm{msg}})\\right),\n\\]\nwhere \\(z \\in \\mathbb{R}^{d_z\\times 32 \\times 32}\\) is the image latent and repeat matches spatial dimensions.\n\n5) Extraction and aggregation over T frames:\n\\[\n\\tilde m_i = R(\\mathrm{resize}(x'_i)) \\in [0,1]^n,\\quad\n\\bar m = \\frac{1}{T}\\sum_{i=1}^T \\tilde m_i,\\quad\n\\hat m_k = \\mathbb{1}\\{\\bar m_k > 0.5\\},\n\\]\nwhere \\(R\\) is the ViT-Small extractor and \\(x'_i\\) is the possibly edited/compressed frame.\n\n6) Training objective with adaptive loss balancing:\n\\[\nL = \\lambda_{\\mathrm{disc}}L_{\\mathrm{disc}} + \\lambda_i L_i + \\lambda_w L_w,\n\\quad\nL_w = -\\frac{1}{n}\\sum_{k=1}^n \\left[ m_k \\log \\tilde m_k + (1-m_k) \\log(1-\\tilde m_k) \\right],\n\\]\n\\[\nL_i = \\frac{1}{N}\\|x - x_w\\|_2^2,\\quad\nL_{\\mathrm{disc}} = -D_q(x_w),\\quad\nL'_{\\mathrm{disc}}=\\frac{1}{2}\\left(\\max\\{0,1-D_q(x)\\}+\\max\\{0,1+D_q(x_w)\\}\\right),\n\\]\n\\[\n\\tilde\\lambda_k = \\frac{\\lambda_k}{\\sum_{k'} \\lambda_{k'}} \\cdot \\frac{R}{\\|\\nabla_{\\theta} L_k\\| + \\varepsilon},\n\\]\nwhere \\(D_q\\) is a patch discriminator, \\(N=3HW\\), \\(\\theta\\) is the embedder’s last layer, and \\(R=1\\).\n\nComplexity comparison (per video with F frames; working res 256×256; embedded frames F/k; extracted frames T):\n- Standard per-frame watermarking: \\(O(F\\cdot C_E + F\\cdot C_R)\\).\n- Video Seal (propagation): \\(O\\left(\\frac{F}{k}\\cdot C_E + F\\cdot C_{\\mathrm{resize}} + T\\cdot C_R\\right)\\),\nwith \\(C_E\\) and \\(C_R\\) the embedder/extractor compute at 256×256, \\(C_{\\mathrm{resize}}=O(HW)\\) for up/downsampling.\n\n**Computational_Properties:**\n- Time Complexity:\n  - Embedding: \\(O\\!\\left(\\frac{F}{k}\\cdot C_E + F\\cdot HW\\right)\\). \\(C_E\\) scales roughly linearly with the number of convs/features in the 16M-param U-Net at 256×256; up/downscale is linear in pixel count.\n  - Extraction: \\(O(T\\cdot C_R + T\\cdot n)\\). For ViT-Small (patch size 16, \\(P=(256/16)^2=256\\) patches, \\(d=384\\)), multi-head attention per block ≈ \\(O(P d^2)\\), MLPs \\(O(P d^2)\\). With L transformer blocks, \\(C_R \\approx O(L P d^2)\\).\n  - Training adds augmentation costs; codec transforms dominate wall-clock but not backprop due to STE.\n\n- Space Complexity:\n  - 2D per-frame activations only; peak memory for U-Net features and ViT embeddings at 256×256. No 4D spatio-temporal tensors.  \n  - Message table \\(T\\): \\(O(n\\cdot d_{\\mathrm{msg}})\\) with \\(n=96\\), \\(d_{\\mathrm{msg}}=192\\) (negligible vs. model params).\n  - Linear in batch size and number of concurrent frames; temporal propagation avoids buffering long clips.\n\n- Parallelization:\n  - Embedding: compute on every k-th frame; copying distortion is trivial and parallelizable across frames.  \n  - Extraction: fully data-parallel across frames; per-frame ViT forward can be batched on GPU.  \n  - Augmentations (geometric/valuemetric) are per-frame and GPU-friendly; codec calls (FFmpeg/PyAV) typically CPU-bound and pipeline-parallel with GPU compute.\n\n- Hardware Compatibility:\n  - GPU-friendly core ops (Conv2D, bilinear resize, Transformer blocks). No ConvTranspose2D; bilinear upsampling avoids checkerboard artifacts.  \n  - High-resolution handling is CPU-light: watermark computed at 256×256, then resized to \\(H\\times W\\) and added; minimizes GPU memory pressure for 4K videos.  \n  - TorchScript compilation reduces Python overhead; compute/IO overlap recommended when applying codecs.\n\n- Training vs. Inference:\n  - Training uses multistage schedules: image pre-train (stable fast convergence), hybrid video/image post-train with STE codecs, then freeze-embedder + extractor fine-tune (improves robustness at fixed imperceptibility).  \n  - Inference uses streamable frame-by-frame processing; parameter \\(\\alpha_w\\) tunes robustness/quality; k controls runtime vs. motion-induced artifacts; T controls extraction robustness vs. latency.\n\n- Parameter Count:\n  - Embedder U-Net ≈ 16M params (dz=128, Efficient U-Net blocks, RMSNorm + SiLU).  \n  - Extractor ViT-Small + patch decoder ≈ 24M params (d=d′=384, patch size 16, L transformer blocks).  \n  - Discriminator is lightweight; message LUT is small (96×2×192).\n\n- Numerical Stability:\n  - RMSNorm and SiLU promote stable gradients; adaptive loss balancing rescales objectives to similar gradient norms, preventing one loss from dominating.  \n  - STE provides gradient surrogates through non-differentiable codecs; while biased, it empirically stabilizes optimization under compression.  \n  - Avoiding ConvTranspose2D removes checkerboard artifacts; averaging logits across frames reduces variance in extraction.\n\n- Scaling Behavior:\n  - With sequence length F: embedding cost scales as \\(F/k\\); extraction scales linearly with T.  \n  - With resolution \\(H\\times W\\): compute at fixed 256×256; only bilinear resize scales with HW; thus robust to 4K without quadratic GPU cost.  \n  - With payload size n: LUT size and final linear layers scale linearly in n; extractor head maps \\(d'\\to n\\). Larger n increases BCE signal but may require higher \\(\\alpha_w\\) or training schedule adjustments.  \n  - k increase improves throughput but may introduce motion “shadow” artifacts; T increase improves robustness (bit accuracy/log p-value) at higher extraction latency.",
        "IMPLEMENTATION_GUIDANCE": "Integration_Strategy:\n- Core components to implement or modify\n  - Embedder (U-Net, operates at 256×256, returns distortion w rather than a full image):\n    - Implement nn.Module EmbedderUNet(dz=128, dmsg=192, nbits=96) with encoder/decoder per Table 1.\n    - Add binary message lookup table T with shape (nbits, 2, dmsg) as nn.Parameter; initialize and register buffer for bit indices.\n    - Forward: resize input frame to 256×256; compute latent z at 32×32; average selected embeddings T[k, mk, :] over bits; repeat to (dmsg, 32, 32), concatenate with image latent; decoder outputs distortion w ∈ R3×256×256.\n    - Return w; do not add to x in the module to keep training pipeline flexible.\n  - Extractor (ViT-Small + patch decoder):\n    - Implement nn.Module ExtractorViT(nb_patches=16×16, d=384, nbits=96) using timm or torch.hub ViT-Small backbone with patch size=16.\n    - Add residual CNN block mapping ViT features to d′=384 at spatial map 16×16; average pool over spatial; Linear(d′→nbits).\n    - At inference only, apply sigmoid to outputs; during training use BCEWithLogitsLoss (no sigmoid).\n  - Quality Discriminator (PatchGAN):\n    - Implement nn.Module PatchDiscriminator returning raw logits Dq(x) (no sigmoid); use Dual-Hinge loss for D training and -Dq(xw) for G (embedder) step.\n  - Differentiable Editing and Straight-Through Estimator (STE):\n    - Create function: def straight_through(xw, transform_fn): x_t = transform_fn(xw); return xw + (x_t - xw).detach().\n    - Use for non-differentiable ops (JPEG, H.264/H.265 via PyAV/FFmpeg).\n  - Video pipeline glue:\n    - Watermark embedding per frame segment of k frames:\n      - def embed_video(frames, message, k=4, alpha_w=2.0):\n        - Downscale all frames to 256×256 (bilinear, align_corners=False).\n        - For i in range(T): if i % k == 0: w_i = embedder(resized[i], m); else: w_i = w_{i-1}.\n        - Upscale w_i to original H×W (bilinear) and add: xw_i = x_i + alpha_w * w_i_up.\n      - Maintain per-segment copy to mitigate compute and ensure temporal propagation.\n    - Extraction:\n      - def extract_video(frames, T_extract=None):\n        - Resize frames to 256×256; get per-frame logits ˜m_i = extractor(frame_i).\n        - Aggregate: avg_logits = (sum_i ˜m_i)/N; hard = (avg_logits.sigmoid()>0.5).int().\n        - Optionally use L1/L2 weighted average (see “Aggregation at extraction time”), but default “Average” recommended.\n- Code-level changes in existing image watermarking repos\n  - Replace image embedder forward to emit distortion w (xw - x) rather than xw; this is necessary for the downscale/upscale strategy at arbitrary resolutions.\n  - Insert message lookup table T into bottleneck; remove any per-pixel message concatenation done at full resolution.\n  - Add temporal propagation logic in dataloader/inference script; do not change model weights for this.\n  - Add STE wrapper around FFmpeg/PyAV codecs in augmentation pipeline; ensure gradients flow to embedder.\n  - Modify training to compute three losses: BCEWithLogits for bits, MSE for imperceptibility, adversarial loss against PatchGAN; add gradient-norm balancer.\n- Framework compatibility\n  - PyTorch reference implementation (torch>=2.1, torchvision>=0.16, timm>=0.9).\n  - TensorFlow/JAX: mirror U-Net and ViT designs; ensure STE implemented as x + stop_gradient(T(x)-x).\n- Migration path from image watermarking models\n  - If starting from MBRS/TrustMark/WAM:\n    - Keep their 2D embedder/extractor; add temporal propagation (k) and frame averaging at extraction.\n    - Convert their xw output to distortion w by w = xw - x and then scale/upscale as in Eq. (1).\n    - Reduce payload to 96 bits if extractor capacity becomes a bottleneck under geometric transforms.\n    - Add geometric augmentations (crop/rotate/perspective) during training; most image models lack this and break under these transforms.\n- External dependencies and setup\n  - FFmpeg (>=5.1) and PyAV for H.264/H.265 at training/eval; install system ffmpeg with libx264/libx265.\n  - Pillow for JPEG; OpenCV optional for resize/codec wrappers; timm for ViT-Small.\n  - TorchScript/torch.compile for inference speed; pin_memory=True in dataloaders; non_blocking=True on CUDA copies.\n\nParameter_Settings:\n- Architecture\n  - Embedder: dz=128, dmsg=192, nbits=96; RMSNorm in ResNetBlocks; SiLU activations; bilinear down/upsampling; final Conv2D to 3 channels.\n  - Extractor: ViT-Small, patch size=16, d=d′=384; L transformer blocks (default from timm); residual CNN (Conv3×3, GELU, LayerNorm) mapping to d′; Linear→nbits.\n- Training hyperparameters\n  - Loss weights (pre-balancer): λw=1.0 (bits), λi=0.5 (MSE), λdisc=0.1 (GAN).\n  - Gradient balancer: R=1.0; epsilon ϵ=1e-12; compute gradient norms w.r.t. embedder final conv.\n  - Optimizer: AdamW with betas=(0.9, 0.999); weight_decay=1e-4 to 5e-4.\n  - Learning rates:\n    - Image pre-train: LR warmup 1e-6→1e-5 over 50 epochs; cosine to 1e-7 by epoch 800.\n    - Video train: LR warmup 1e-7→1e-6 over 10 epochs; cosine to 1e-8 by end (last 300 epochs).\n  - Batch sizes:\n    - Images: 16 per GPU at 256×256.\n    - Videos: 1 clip (32 frames) per GPU; accumulate grads 2–4 steps if memory allows.\n  - Epoch schedule (robust default):\n    - Stage 1 (images only): 600–800 epochs.\n    - Stage 2 (hybrid): 200–400 epochs alternating 1:1 images:videos with compression augmentation active.\n    - Fine-tune extractor: freeze embedder for 150–250 epochs at end; LR 0.5× of Stage 2; keep augmentations.\n- Augmentations (sample uniformly per mini-batch; use STE for non-diff ops)\n  - Geometric: crop edge ratio r∈[0.7,1.0]; resize r∈[0.7,1.5] (aspect ratio changes allowed); rotation θ∈[-10°,10°]; perspective distortion scale d∈[0.1,0.5]; horizontal flip with p=0.5.\n  - Valuemetric: brightness∈[0.5,2.0], contrast∈[0.5,2.0], saturation∈[0.5,2.0], hue∈[-0.5,0.5].\n  - Blur/filters: Gaussian kernel k∈{3,5,9,13,17}, Median k∈{3,5,9,13,17}.\n  - Compression: JPEG Q∈[40,80] (images); H.264/H.265 CRF∈[9,27] (training); at evaluation test CRF up to 60.\n- Initialization\n  - Conv weights: Kaiming normal (fan_out) for U-Net; ViT from pretrained “deit_small_patch16_224” or similar for stability.\n  - Message table T: normal(0, 0.02); consider orthogonal init across bit indices to reduce embedding collisions.\n  - Discriminator: spectral norm optional; init with small gain (0.02).\n- Critical vs robust parameters\n  - Critical: αw (scaling factor), k (temporal step size), augmentation ranges for geometry/compression, extractor LR during fine-tuning.\n  - Robust: exact MSE weight λi in [0.3,0.7]; discriminator λdisc in [0.05,0.2].\n- Parameter relationships and scale dependencies\n  - αw controls PSNR/robustness: αw∈[1.6,2.4]; nominal 2.0 yields PSNR≈48 dB on SA-V and strong robustness; αw<1.2 increases imperceptibility but reduces combined-transform decoding.\n  - k impacts compute and artifacts: k=1 best quality; k=2–4 recommended; k>6 increases “shadow/blinkering” in high-motion scenes.\n  - T (frames used at extraction) improves bit accuracy sublinearly; use T≥64 for 3–5 s clips when latency permits.\n  - Payload nbits: 64–128 practical; 96 default. Larger payload reduces log10(p) robustness under geometry unless extractor capacity increased.\n- Hardware-dependent settings\n  - Mixed precision (AMP) recommended; set torch.backends.cudnn.benchmark=True.\n  - Use TorchScript or torch.compile for inference; enable channels_last memory format to speed up convs.\n  - GPU VRAM: ≥16 GB for multi-GPU training (1 clip/GPU); on 8–12 GB reduce transformer depth or use grad checkpointing.\n  - CPU-only inference: prefer k=4 and T=32–64 to keep latency reasonable; set num_workers≥8 for PyAV decoding.\n\nApplication_Conditions:\n- Beneficial scenarios\n  - Watermarking high-resolution, long videos where per-frame embedding is impractical; robustness needed under common social media pipeline (H.264/H.265 + mild crops/filters).\n  - AI-generated content attribution and platform moderation; mobile or edge deployment with constrained GPUs.\n- Hardware requirements\n  - Training: 8–16 GPUs (≥16 GB VRAM each) for full schedule; fast storage; FFmpeg installed system-wide.\n  - Inference: CPU-only feasible; GPU V100/A100/RTX 3090 beneficial for extraction throughput.\n- Scale considerations\n  - Becomes advantageous for videos ≥10 s or ≥1080p due to temporal propagation; for short clips (<2 s) and low res (<480p), per-frame embedding may be acceptable with k=1.\n- Task compatibility\n  - Strong under valuemetric changes and compression; robust to mild geometry (≤10° rotation, ≤50% crop area, moderate perspective).\n  - Neutral/weak under extreme geometry (≥30° rotation, ≤33% area crop); consider higher T and lower k, and extend training ranges if this is a requirement.\n- Alternatives comparison\n  - Choose Video Seal over MBRS/TrustMark/WAM when combined geometry+compression robustness is required.\n  - If payload capacity (≥256 bits) is the priority and videos see minimal geometry, MBRS/TrustMark may be preferable.\n- Resource constraints\n  - With limited compute, use pretrained checkpoints and fine-tune extractor only; set k=4 and T=32 to balance latency and robustness.\n\nExpected_Outcomes:\n- Performance metrics (Video Seal at nominal settings: nbits=96, αw=2.0, k=4)\n  - Imperceptibility: PSNR≈48 dB; SSIM≈0.998; LPIPS≈0.013; VMAF≈93.8 on SA-V (5 s, original res).\n  - Robustness (SA-V, first 3–5 s, averaged):\n    - Identity: bit acc≈0.99; log10(p)≈−26.8.\n    - Valuemetric: bit acc≈0.90; log10(p)≈−19.9.\n    - Geometric: bit acc≈0.85; log10(p)≈−17.0.\n    - Compression (H.264/H.265): bit acc≈0.85–0.97 (CRF 30), deteriorates to ≈0.70 at CRF 60.\n    - Combined (H.264 CRF 30 + 50% crop + brightness 0.5): bit acc≈0.73; log10(p)≈−8.1 (detectable at FPR ≤10^-6).\n- Efficiency\n  - Embedding time per 1 s of video: CPU ≈1.14 s; GPU ≈0.42 s.\n  - Extraction time per 1 s: CPU ≈0.69 s; GPU ≈0.11 s.\n  - Temporal propagation k=4 yields ~4× embedding speedup versus k=1 with small robustness impact in most scenes.\n- Timeline expectations\n  - Immediate gains from temporal propagation and downscale/upscale strategy; full robustness benefits require multistage training and extractor fine-tuning.\n  - Extractor fine-tuning (150–250 epochs) typically adds +2–5% bit accuracy under compression/combined transforms without PSNR changes.\n- Trade-offs\n  - Raising αw improves decoding under heavy compression/crops but reduces PSNR/VMAF (expect PSNR drop of 2–5 dB for αw from 2.0→2.4).\n  - Increasing k improves embedding throughput but may introduce motion-tracked “shadow” artifacts on fast-moving content.\n  - Larger payloads reduce log10(p) and combined-transform resilience unless extractor capacity or training data scale increases.\n- Failure modes and mitigation\n  - Shadow/blinkering artifacts with k≥6 or highly non-rigid motion: reduce k to 2–4; add temporal consistency regularization (optional), or increase T at extraction.\n  - Poor geometry robustness if training lacked such augmentations: widen rotation/perspective ranges; add random resize with aspect ratio changes; re-train.\n  - Codec color space mismatch (YUV420 rounding vs RGB path) reducing decode: prefer H.264 in RGB mode when available; include YUV420 encode/decode in training.\n  - Low bit accuracy despite high PSNR: check STE wrapper; confirm gradients reach embedder; validate that extractor uses BCEWithLogitsLoss (no extra sigmoid).\n  - Payload misalignment: ensure bit order and thresholding consistent; verify lookup table indexing (T[k, mk, :]) and aggregation correctness.\n- Debugging indicators\n  - Identity case must decode ≥0.99 bit accuracy; adding frames (T) should monotonically improve bit accuracy; if not, check aggregation and per-frame logits scale.\n  - Histograms of extractor logits: correct bits should have higher logits; severe overlap indicates training instability or missing augmentations.\n  - Log10(p) below −6 under combined transforms indicates practical detectability at FPR 10^-6.\n- Hardware-specific outcomes\n  - Ampere/Ada GPUs: expect ~1.3–1.8× extraction speedup with AMP and channels_last; TorchScript improves CPU extraction by ~15–25%.\n  - CPU-only: rely on k=4 and T=32–64; cap CRF evaluation to ≤50 to keep latency manageable.\n\nQuality Requirements:\n- Validation procedures\n  - Sanity checks: identity decoding (≥0.99), single-transform tests (H.264 CRF 30, brightness 0.5, crop 50%) and combined-transform test; compute PSNR/SSIM/LPIPS/VMAF and bit accuracy/log10(p).\n  - Frame count sweep: evaluate T∈{8,16,32,64}; confirm monotonic decoding improvement.\n  - Step-size sweep: k∈{1,2,4,6}; monitor artifacts visually on high-motion clips; select k≤4.\n- Troubleshooting guide\n  - If adversarial training destabilizes (loss oscillations), reduce λdisc to 0.05–0.1, add gradient penalty or spectral norm; increase batch size or use EMA on embedder weights.\n  - If compression augmentation slows training excessively, cache codec outputs for short schedules, or alternate compression augmentation every N steps.\n  - If extractor overfits images and underperforms on videos, start hybrid schedule earlier (epoch 400–600) and include codec-in-RGB and codec-in-YUV modes.\n- Limitations and risks\n  - Extreme geometric edits (large rotations, strong perspective, small crops) still challenging; expect substantial bit accuracy drops unless training is extended.\n  - Payload security is post-hoc and not cryptographically secure; consider complementary provenance systems for high-stakes use.\n- Actionable deployment advice\n  - Use αw=2.0, k=4, T=64 as default; drop αw to 1.6 for stringent quality needs; raise to 2.2–2.4 for hostile compression platforms.\n  - In mobile/edge contexts, run extractor only on first N=32–64 frames to bound latency; prioritize GPU for extraction in backend services.\n  - Use FFmpeg filters to standardize pixel formats (yuv420p) and bitrate; add platform-specific codec profiles in evaluation to match production pipelines."
    }
]