[
    {
        "DESIGN_INSIGHT": "### DESIGN_INSIGHT_HIGH: [One-Step Generative Prior Watermark Encoder with Condition Adaptor and VAE Skip Fusion – Distributional Embedding Robust to Editing]\n- Replaces conventional residual add-on encoders (small UNets operating in pixel space) with a one-step text-to-image generator (SDXL-Turbo) repurposed as a conditional watermark encoder that maps an original image and a k-bit message into a new image whose distribution is separable for a paired decoder. Instead of adding a secondary ControlNet-style branch (which injects conflicting structural residuals in one-step models), the method fuses conditions before the generative backbone.\n- Key mechanism: a condition adaptor C(·) fuses the image xo and watermark bits w into a single tensor that enters the VAE encoder, maintaining a single structural stream for the one-step UNet U and VAE decoder Dec. Zero-convolution gated skip connections S_i from the VAE encoder to decoder restore fine structures critical for imperceptibility. The encoder is thus E(xo, w) = Dec(U(Enc(C(xo, w))), {S_i}). The decoder D is a ConvNeXt-B with a linear head that outputs the k-bit estimate w′.\n- Fundamental difference from prior work: (1) Multi-step diffusion encoders require O(T) sampling steps and are hard to train end-to-end through editing; here the encoder is one-step, giving O(1) sampling complexity and stable backprop. (2) ControlNet-like late branching degrades one-step models by injecting competing structure; pre-VAE fusion preserves a single latent stream and empirically improves fidelity and robustness. (3) VAE skip fusion (zero-conv gates) shifts the VAE from “compressive-friendly” to “reconstruction-strong,” improving perceptual similarity without sacrificing the generator prior.\n- Objective and training: the joint loss balances image fidelity and decoding reliability,\n  L_ALL = L_IMG(xo, xw) + α·L_BCE(w, w′),\n  with\n  L_IMG = β_MSE·||γ(xo) − γ(xw)||_2^2 + β_LPIPS·LPIPS(xo, xw) + β_GAN·LGAN(xo, xw),\n  where γ is RGB→YUV mapping and LGAN is a standard discriminator loss. A two-stage curriculum (freeze UNet/Dec, then unfreeze) preserves the pretrained prior before full finetuning. This configuration yields higher imperceptibility and robustness than UNet encoders and ControlNet-style conditioning.\n\n\n### DESIGN_INSIGHT_MEDIUM: [Frequency-Aligned Surrogate Noise Layers via Blur Family – Forcing Low-Frequency Embedding to Survive T2I Editing]\n- Modifies the classical “noise layer” between encoder and decoder (traditionally JPEG, brightness, Gaussian noise) by replacing/augmenting it with a family of blurring operators during training, rather than backpropagating through heavy T2I editors. This retools the optimization pressure so that the encoder embeds bits in frequency regions that T2I editing preserves.\n- Key mechanism: spectral analysis shows T2I editing behaves as a low-pass operator that suppresses mid/high-frequency watermark patterns. Let F(·) be the Fourier transform. For a synthetic ring-like spectral watermark w placed at low/mid/high bands, the measured |F(ϵ(xw)) − F(ϵ(xo))| (ϵ: editor) is smallest for low frequencies and largest for mid/high. Several cheap, differentiable blurs T ∈ S (pixelate, defocus, zoom, Gaussian, motion) produce similar spectral attenuation, i.e., act as practical surrogates for ϵ.\n- Training formulation: optimize\n  min_{E,D} E_{xo,w} E_{T∼S} [ L_IMG(xo, E(xo,w)) + α·L_BCE(w, D(T(E(xo,w)))) ],\n  which biases E to concentrate watermark energy into low-frequency bands that survive both T and ϵ. This avoids the O(T) sampling and memory cost of differentiating through editing while producing robustness to a broad set of editors.\n- Difference from prior work: prior robustness layers emphasize JPEG/noise or attempt truncated gradients/STE directly through specific editors (limited generalization and unstable from scratch). The frequency-aligned blur family is editor-agnostic, computationally efficient, and empirically matches the spectral effect of multiple global/local editing models, yielding higher TPR at fixed ultra-low FPR.\n\n\n### DESIGN_INSIGHT_MEDIUM: [Edit-Aware Late-Stage STE Finetuning with One-Step Editor – Stable Curriculum for Robustness Gains]\n- Instead of naively backpropagating through multi-step editors (memory-heavy and unstable), the model introduces a third training stage that integrates a representative instruction-based editor (Instruct-Pix2Pix) using a straight-through estimator (STE), but only after the encoder has learned a robust, low-frequency embedding and high-fidelity synthesis.\n- Key mechanism: define the edited sample as y = ϵ(xw). During finetuning, gradients are passed with ∂y/∂xw ≈ I via STE, allowing L_BCE(w, D(y)) to influence E without differentiating through ϵ. This is done after the two-stage curriculum that first (i) freezes UNet/VAE decoder to stabilize the generative prior and maximize bit accuracy, then (ii) unfreezes all modules to optimize perceptual fidelity. The STE stage converges only with this curriculum; applied from scratch it fails to train.\n- Difference from prior work: gradient truncation or editor-in-the-loop training is either editor-specific or unstable; the staged approach (blur surrogates first, STE last) produces editor-robust solutions while maintaining perceptual quality. Combined with the one-step encoder, this preserves O(1) sampling complexity and avoids large-memory multi-step backprop.",
        "EXPERIMENTAL_TRIGGER_PATTERNS": "Task_Performance_Signatures:\n- W-Bench (image watermark robustness vs. editing; 512×512, TPR@0.1%FPR primary)\n  - Overall robustness (averaged across 8 editing settings: Sto/Det regeneration; 3 global; 2 local; I2V):\n    - VINE-R: ≈88.5% average TPR@0.1%FPR [(99.66, 99.98, 97.46, 86.86, 94.58, 99.96, 93.04, 36.33)%]\n    - VINE-B: ≈83.9% average TPR@0.1%FPR [(91.03, 99.25, 96.30, 80.90, 89.29, 99.60, 89.68, 25.44)%]\n    - StegaStamp (strong baseline but lower quality): ≈81.0% average TPR@0.1%FPR\n    - Expect +3–8% absolute TPR@0.1%FPR gain over StegaStamp averaged across editing tasks, with much lower FID (≤0.15).\n  - Image regeneration (VP stochastic; DPM-Solver deterministic)\n    - Expect TPR@0.1%FPR ≥99.5% (Det) and ≥99% (Sto) for VINE-R across wide difficulty (Det 15–45 steps; Sto ts 60–240).\n    - VINE-B remains ≥99% (Det) and ≥91% (Sto). Expect stability until extreme perturbations where image semantics drift.\n  - Global editing (Instruct-Pix2Pix; UltraEdit; MagicBrush; 50 steps, text guidance 5–9)\n    - Expect VINE-R TPR@0.1%FPR ≈97.5% (Pix2Pix), ≈86.9% (UltraEdit), ≈94.6% (MagicBrush) averaged over guidance scales; VINE-B ≈96.3/80.9/89.3%.\n    - Robustness declines with rising guidance; improvements most pronounced at mid guidance (≈7–8) and when edits preserve layout vs. style rewrites.\n  - Local editing (UltraEdit; ControlNet-Inpainting; mask 10–60% area)\n    - Expect near-ceiling TPR@0.1%FPR for VINE-R: ≈100% (UltraEdit), ≈93% (ControlNet). VINE-B ≈99.6% and ≈89.7%.\n    - TPR remains high even at 50–60% mask; drop is modest (<7% absolute) from small to large masks.\n  - Image-to-Video (SVD; frames 5–19 sampled)\n    - Expect lower TPR@0.1%FPR due to strong frequency suppression: VINE-R ≈36.3%, VINE-B ≈25.4%. This is a known limitation when layout/temporal synthesis alters content across frames.\n    - With TI2V (MAGE+; 10 frames; frames 2–10), expect high TPR@0.1%FPR: VINE-R ≈91.4% average, VINE-B ≈90.3% (indicates generator dependence).\n  - Imperceptibility (quality)\n    - VINE-B achieves state-of-the-art perceptual quality among robust methods: PSNR ≈40.5 dB, SSIM ≈0.9954, LPIPS ≈0.0029, FID ≈0.08 at 512×512.\n    - VINE-R trades ≈3 dB PSNR and slight LPIPS/SSIM changes for robustness: PSNR ≈37.3 dB, SSIM ≈0.9934, LPIPS ≈0.0063, FID ≈0.15.\n    - Compared to MBRS/StegaStamp, expect clearly fewer artifacts and better FID while matching/exceeding robustness on most editing tasks.\n  - Capacity and false positive control\n    - VINE variants support 100-bit payload with reliable detection at 0.1% FPR; expect TPR@0.1%FPR ≥95% on all tasks except the hardest global edits and SVD I2V (where TPR floor ≈25–36%).\n    - Expect detectors to retain po≈0.5 under H0; unlike SSL, no elevated FPR on non-watermarked images.\n  - Classical distortions (sanity/transfer)\n    - With resolution scaling to 512×512, inherent robustness is preserved or improved vs. original training resolution (notably for Gaussian blur and JPEG).\n    - Expect especially strong correlation between robustness to blurs (Gaussian/defocus/pixelate) and robustness to generative edits; enabling blur surrogates in training raises TPR on edits by ≈+20–30% absolute vs. UNet baselines (see ablations Config A→B).\n- Efficiency and resource use\n  - Inference time per image ≈0.08 s; similar to StegaStamp/TrustMark (≈0.07 s) and ≈28× faster than SSL (≈2.19 s).\n  - GPU memory ≈5.0 GB; expect ≈1.7–5× more memory than light baselines due to SDXL-Turbo backbone and skip connections. No throughput degradation with edit difficulty; watermark encoding cost is constant.\n- Contextual conditions amplifying improvements\n  - Edits that suppress mid/high frequencies (style changes, global recolor, mild layout preservation) show largest gains from blur-surrogate training.\n  - Long edit chains (multiple edits) and stronger guidance scales benefit more from VINE-R; VINE-B best when perceptual quality is priority and edits are moderate.\n  - Larger masks (local edits) cause small TPR drop (<7% absolute) thanks to low-frequency embedding bias.\n- Training vs. inference differences\n  - Gains appear after stage-2 unfreeze; stage-3 fine-tune with straight-through on Instruct-Pix2Pix typically adds ≈+1–6% absolute TPR on global/local edits with ≈3 dB PSNR loss.\n  - Resolution scaling allows arbitrary input sizes with near-constant robustness and similar quality; expect similar or slightly better robustness at higher resolutions.\n\n- Cross-domain NLP benchmarks (to satisfy standardized reporting; technique is vision-only)\n  - Language modeling (lambada_openai, wikitext, ptb): Expect unchanged performance (not applicable); within ±0.1% of baseline due to no interaction.\n  - Reading comprehension (squad_completion, squad_v2, narrativeqa): Expect unchanged (N/A).\n  - Commonsense reasoning (hellaswag, piqa, social_iqa, commonsenseqa): Expect unchanged (N/A).\n  - Factual QA (arc_easy, arc_challenge, boolq, openbookqa): Expect unchanged (N/A).\n  - Context resolution (winogrande, winograd): Expect unchanged (N/A).\n  - Other tasks (swde, fda): Expect unchanged (N/A).\n\nArchitectural_Symptoms:\n- Training characteristics\n  - Two-phase stability signature\n    - Stage 1 (UNet/decoder frozen): rapid BCE decrease; bit accuracy surpasses 0.85 threshold before transition. Loss curves smooth with low variance; LPIPS/FID decrease modestly.\n    - Stage 2 (all params trainable): continued BCE reduction and sharp LPIPS/FID improvements; PSNR rises by ≈3–4 dB (Config E→F).\n  - Stage-3 fine-tuning with straight-through estimator\n    - Only converges when applied after base training; applying from scratch typically stalls (bit accuracy plateaus <0.6) or diverges. Healthy runs show +1–6% TPR gain on global/local edits.\n  - Surrogate-blur enablement (Config A→B)\n    - Immediate robustness lift on edits (+20–30% absolute TPR) accompanied by PSNR drop (≈2–3 dB); indicates successful low-frequency embedding shift.\n  - Generative prior and condition path\n    - Condition Adaptor vs. ControlNet branch: swapping to adaptor raises PSNR by ≈+4 dB and lowers FID from ≈2.87 to ≈0.53 with similar TPR; a hallmark of single-stream conditioning working better for one-step models.\n    - Skip connections + zero-conv in VAE: visible LPIPS drop (≈0.0102→0.0029) and FID collapse to ≈0.08; perceptual similarity leaps without robustness loss.\n  - Pretraining effect\n    - Using SDXL-Turbo initialization reduces FID by ≈7× vs. random init at same robustness (Config G vs. H); if pretrained weights are missing, expect noticeably higher FID (~1.0) with similar TPR.\n  - Frequency-domain diagnostic\n    - |F(xw)−F(xo)| exhibits dense low-frequency energy with cross-shaped structure along axes; after editing, |F(ϵ(xw))−F(ϵ(xo))| preserves central/axis energy while high-frequency rings diminish. Absence of this low-frequency signature correlates with poor edit-robustness.\n    - SVD I2V reduces energy across all bands; if spectral difference becomes near-zero, expect low TPR.\n\n- Runtime/memory behaviors\n  - Memory profile\n    - ~5 GB GPU at 512×512; linear growth with batch size; no OOM at typical batches used in training (80–112). Memory overhead dominated by SDXL UNet/VAEs; enabling 50-step backprop through editors is infeasible—hence ST at 25 steps is used during fine-tuning.\n  - Throughput patterns\n    - ≈0.08 s/image encoding; largely constant across edit difficulty since editing is external to encoder at inference. Higher GPU utilization than light baselines due to larger backbone; consistent speed regardless of watermark payload (fixed 100 bits).\n  - Resolution scaling\n    - Residual-based scaling yields near-constant robustness and perceptual metrics across resolutions; commonly improves robustness to Gaussian blur/JPEG at larger resolutions due to effective kernel normalization.\n\n- Profiling signatures and sanity checks\n  - Detection operating point\n    - At 0.1% FPR, TPR should align with Table-1/5 curves; excessive AUROC with low TPR@0.1%FPR indicates inflated FPR (as observed in SSL). Healthy detectors yield po≈0.5 on non-watermarked images and pw≥0.8 on watermarked images.\n  - Edit-quality neutrality\n    - CLIPdir/CLIPimg/CLIPout for edited watermarked images remain within ±0.02 of unwatermarked baselines; larger deltas suggest the encoder is harming semantic or content fidelity.\n  - Ablation checkpoints\n    - Config A (simple UNet) shows PSNR ≈38.2 dB and weak TPR; enabling blurs (Config B) boosts TPR but drops PSNR; adding SDXL-Turbo + adaptor + skips (Config F) recovers PSNR to ≈40.5 dB with strong TPR. Deviations from this trajectory signal misconfigured conditioning or skip pathways.\n  - Negative/neutral effects to watch\n    - Slight quality regression when switching to VINE-R (≈3 dB PSNR drop); expected trade-off for edit robustness.\n    - Persistently low TPR for SVD I2V despite good performance elsewhere is expected; lifting this requires stronger watermark energy or per-frame temporal strategies.\n    - ControlNet-style dual-branch conditioning in one-step models tends to reduce perceptual quality and may slightly depress robustness.",
        "BACKGROUND": "Title: ROBUST WATERMARKING USING GENERATIVE PRIORS AGAINST IMAGE EDITING: FROM BENCHMARKING TO ADVANCES\n\nHistorical Technical Context:\nIn digital image watermarking, classical robust approaches relied on signal processing in transform domains (e.g., DWT-DCT-SVD) to hide bits in perceptually tolerant, low-frequency bands while resisting transmission degradations such as compression and noise. With deep learning, CNN-based encoder–decoder architectures and GAN-regularized models (e.g., RivaGAN, StegaStamp) learned to embed and extract messages end-to-end, improving imperceptibility and robustness to classical distortions (JPEG, blur, resize, crop). Invertible designs (e.g., PIMoG) and training schemes that simulate real degradations (e.g., MBRS mini-batch JPEG) further improved resilience.\n\nIn parallel, large-scale text-to-image (T2I) diffusion models transformed image generation and editing. Multi-step DDPMs and score-based models enabled high-fidelity synthesis and powerful editing (image regeneration, inversion, instruction-guided editing, inpainting), while one-step distillations (e.g., SDXL-Turbo) traded sampling steps for speed. Editing methods such as Instruct-Pix2Pix, MagicBrush, ControlNet-Inpainting, and UltraEdit exposed new attack surfaces: realistic semantic edits that subtly alter textures and mid/high-frequency details, often erasing watermark signals without obvious perceptual changes. Benchmarks lagged behind this threat model; the recent WAVES benchmark targeted regeneration but omitted broader T2I-based global/local editing and image-to-video (I2V) generation.\n\nThis paper positions itself at the intersection: it (i) formalizes a comprehensive, editing-centric benchmark (W-Bench) covering regeneration, global/local editing, and I2V; (ii) diagnoses, in the Fourier domain, why T2I edits preferentially remove mid/high-frequency watermark energy; and (iii) advances a generative-prior watermark encoder (VINE) leveraging a large one-step diffusion model (SDXL-Turbo) with tailored conditioning and training, yielding both higher imperceptibility and significantly improved robustness to state-of-the-art edits.\n\nTechnical Limitations:\n- Editing-model backpropagation bottleneck: Multi-step diffusion editing requires O(T) forward passes; naively backpropagating end-to-end through T sampling steps incurs O(T) memory with activation checkpointing and high compute, often prohibitive for T∈[20,50] and 512×512 latents. Straight-through estimators (STE) or gradient truncation can destabilize training from scratch.\n- Frequency mismatch between training noise layers and modern edits: Prior robustness training emphasized classical distortions (JPEG, additive noise), whose frequency responses do not match T2I edits that selectively attenuate mid/high-frequency components; this leads to generalization gaps when facing modern editing pipelines.\n- Imperceptibility–robustness trade-off in existing encoders: Strong robustness (e.g., MBRS, StegaStamp) often degrades image quality (lower PSNR/SSIM, higher LPIPS/FID), while higher-quality methods (e.g., TrustMark) underperform under edits; encoders lack a powerful generative prior to hide information more invisibly in low-frequency structures.\n- Detection reliability at low FPR: Many evaluations report bit accuracy or AUROC, which do not guarantee high TPR at stringent operating points (e.g., 0.1% FPR). Prior methods can exhibit elevated po under H0, inflating false positives and degrading practical deployability.\n- Conditioning conflicts in one-step generators: Naively adding extra control branches (e.g., ControlNet-like) to a one-step UNet can inject competing structural signals (from image and watermark), harming perceptual fidelity because layout is determined directly by the single denoising step.\n- Resolution rigidity and scalability: Models trained at fixed low resolutions can lose robustness or quality when naively upscaled at inference; efficient, resolution-consistent residual injection is needed to preserve both quality and inherent robustness.\n\nPaper Concepts:\n- W-Bench (Watermarking Benchmark for Editing Robustness): A comprehensive benchmark evaluating watermark detection robustness across four editing families: image regeneration (stochastic VP scheduler ts∈{60,…,240}; deterministic DPM-Solver nd∈{15,25,35,45}), global editing (Instruct-Pix2Pix, MagicBrush, UltraEdit; classifier-free guidance scales 5–9), local editing (UltraEdit, ControlNet-Inpainting; mask areas 10–60%), and image-to-video (SVD; frames 5–19). Imperceptibility metrics include PSNR/SSIM/LPIPS/FID; detection uses TPR at fixed FPR (0.1%, 1%).\n- Frequency-domain editing analysis: Let F(·) denote the Fourier transform. The paper injects ring patterns w in specific bands and measures Δ(f)=|F(ϵ(xw))−F(ϵ(xo))|, where ϵ is an editor. Empirically, editors suppress mid/high-frequency watermark energy while preserving low-frequency components. This motivates training with surrogate low-pass distortions T that share spectra with edits. Mathematically, for blur kernel h, the observed spectrum is F(y)(ω)=H(ω)F(x)(ω); |H(ω)| decays with |ω|, suppressing high frequencies akin to edits.\n- Surrogate blurring distortions: A computationally efficient proxy set T combining pixelation, defocus/zoom/Gaussian/motion blur with classical distortions. Training with T encourages embedding in low-frequency bands that are resilient to T2I edits. Formally, pixelation approximates low-pass filtering plus subsampling; defocus blur is convolution with a pillbox kernel whose transfer function is H(ω)=2J1(r|ω|)/(r|ω|).\n- VINE (generative-prior watermark encoder): A conditional generative watermark encoder E using SDXL-Turbo as backbone. Given original image xo and k-bit watermark w, the encoder produces xw=E(xo,w) such that the decoder D recovers w′≈w under edits ϵ. Conditioning is fused by a learned condition adaptor feeding the VAE encoder; skip connections with zero-conv layers bridge VAE encoder and decoder to improve reconstruction. The loss is:\n  LALL = LIMG(xo,xw) + α LBCE(w,w′),\n  where\n  LIMG = βMSE ∥γ(xo)−γ(xw)∥² + βLPIPS LLPIPS(xo,xw) + βGAN LGAN(xo,xw),\n  LGAN = E[log Ddisc(xo)] + E[log(1−Ddisc(xw))],\n  and γ maps RGB→YUV for perceptual uniformity.\n- Statistical detection at fixed FPR: With k-bit messages and match count M(w,w′), the decision rule M≥τ yields FPR and TPR under H0/H1 as binomial tails:\n  FPR(τ)=P[M>τ|H0]=I_{p_o}(τ+1,k−τ),  TPR(τ)=P[M>τ|H1]=I_{p_w}(τ+1,k−τ),\n  where I is the regularized incomplete beta function, p_o≈0.5 under H0, and p_w≫0.5 under H1. This formalism enforces operating points like TPR@0.1%FPR.\n- Resolution scaling via residual upsampling: For a model trained at u×v, compute residual r′=E(interp(xo,u,v))−interp(xo,u,v), upsample r to image size h×w, then output xw=clamp(xo+r,−1,1). This preserves inherent robustness while enabling high-resolution watermarking compatible with editing models.\n\nExperimental Context:\nThe evaluation centers on robustness to powerful generative edits while maintaining imperceptibility and strict false-positive control. Eleven representative watermarking systems (MBRS, CIN, PIMoG, RivaGAN, SepMark, DWTDCT, DWTDCTSVD, SSL, StegaStamp, TrustMark, EditGuard) are standardized to 512×512 using resolution scaling and evaluated on 10,000 UltraEdit-derived samples with matched prompts and masks. The study prioritizes TPR@0.1%FPR and TPR@1%FPR as deployment-relevant metrics; PSNR, SSIM, LPIPS, and FID quantify visual quality. Regeneration difficulty is swept via noise timesteps or sampling steps; global editing via classifier-free guidance scales; local editing via mask area; I2V via frame indices.\n\nThe experimental goal is twofold: (i) benchmark the fragility of existing watermarking under modern editing (revealing that most methods fail post-edit), and (ii) demonstrate that VINE attains a superior quality–robustness Pareto. VINE-Base (VINE-B) achieves top imperceptibility (e.g., PSNR 40.51 dB, SSIM 0.9954, LPIPS 0.0029, FID 0.08) with strong robustness, while VINE-Robust (VINE-R) trades some quality (PSNR 37.34 dB) for peak robustness (e.g., TPR@0.1%FPR: stochastic regen 99.66%, deterministic regen 99.98%, Instruct-Pix2Pix 97.46%, UltraEdit-global 86.86%, MagicBrush 94.58%, UltraEdit-local 99.96%, ControlNet-Inpaint 93.04%). Ablations confirm: (a) adding blurs to noise layers significantly lifts robustness; (b) adapting SDXL-Turbo with a condition adaptor outperforms ControlNet-style branches in one-step regimes; (c) VAE skip connections with zero-conv improve perceptual fidelity; and (d) late-stage STE fine-tuning against an editor further strengthens resilience. The evaluation philosophy emphasizes realistic editing distributions, strict error control, and scalability across editing types, establishing W-Bench as a robust assessment protocol for future work.",
        "ALGORITHMIC_INNOVATION": "Core_Algorithm:\n- Replace the conventional small CNN watermark encoder with a one-step pretrained text-to-image generator (SDXL-Turbo) and re-purpose it as a conditional watermark encoder. The encoder ingests both the original image and a K-bit watermark via a dedicated condition adaptor that fuses them before the VAE encoder.\n- Introduce frequency-informed surrogate attacks in the training noise layer: a curated set of blurring distortions (pixelation, defocus, Gaussian, motion, zoom blurs) that statistically mimic text-to-image editing in the Fourier domain, augmented with standard transmission distortions (JPEG, saturation, contrast, brightness, Gaussian/shot/impulse/speckle noise).\n- Modify the SDXL-Turbo VAE with skip connections gated by zero-convolution layers to increase reconstructive fidelity and keep residuals initially zero-valued, improving perceptual similarity while allowing robust low-frequency watermark embedding.\n- Train in three stages: (1) freeze UNet+decoder and emphasize watermark extraction loss; (2) unfreeze all modules and re-balance image quality and watermark extraction losses; (3) fine-tune robustness by inserting an instruction-driven editor (Instruct-Pix2Pix) in the noise layer, using a straight-through estimator for backpropagation through editing. Use a ConvNeXt-B watermark decoder to extract 100 bits. Add resolution scaling at inference by encoding at a fixed base resolution and upsampling the learned residual to arbitrary sizes.\n\nKey_Mechanism:\n- The core insight is that modern image editing (diffusion-based regeneration, global/local edits) systematically attenuates mid/high-frequency components while preserving low-frequency content. Training with blurring distortions forces the encoder-decoder to concentrate watermark energy in low-frequency bands, increasing survivability through edits.\n- A strong generative prior (one-step SDXL-Turbo) enables conditional synthesis of watermarked images close to the input manifold; the encoder learns an imperceptible, robust distributional shift recognizable by the decoder. Zero-conv-gated skip connections retain reconstruction fidelity, ensuring the watermark’s invisibility while improving robustness.\n\nMathematical_Formulation:\n- Inputs and operators:\n  - Original image \\( x_o \\in \\mathbb{R}^{H \\times W \\times 3} \\), watermark bits \\( w \\in \\{0,1\\}^K \\) (K=100).\n  - Condition adaptor \\( A_\\phi \\) (parameters \\( \\phi \\)) fuses image and bits: \\( y = A_\\phi(x_o, w) \\in \\mathbb{R}^{H \\times W \\times 6} \\).\n  - SDXL-Turbo VAE encoder \\( f_e \\) and decoder \\( f_d \\), UNet \\( U \\) (null text prompt).\n  - Skip features \\( s_\\ell = g_\\ell(h_\\ell) \\) where \\( h_\\ell \\) are intermediate encoder activations and \\( g_\\ell \\) are zero-conv layers initialized at zero.\n  - Noise layer \\( \\mathcal{T} = t_m \\circ \\cdots \\circ t_1 \\), where \\( t_i \\) are random distortions (blurs + standard).\n  - Editing operator \\( \\varepsilon(\\cdot) \\) (e.g., Instruct-Pix2Pix) used only in stage-3 fine-tuning.\n\n- Encoding (one-step generative prior):\n  \\[\n  z = f_e(y), \\quad \\tilde{z} = U(z;\\,\\text{null}), \\quad x_w = f_d(\\tilde{z}, \\{s_\\ell\\}_{\\ell=1}^L).\n  \\]\n  The zero-conv gates ensure \\( g_\\ell(\\cdot) = 0 \\) at initialization, so skip contributions start from zero and grow smoothly.\n\n- Decoding with surrogate/editing perturbations:\n  \\[\n  \\tilde{x} = \\mathcal{T}(x_w), \\quad \\hat{w} = D_\\psi(\\tilde{x}),\n  \\]\n  where \\( D_\\psi \\) is ConvNeXt-B with a final FC layer outputting K logits.\n\n- Optimization objective:\n  \\[\n  \\mathcal{L}_{\\text{ALL}} = \\mathcal{L}_{\\text{IMG}}(x_o, x_w) + \\alpha \\, \\mathcal{L}_{\\text{BCE}}(w,\\hat{w}),\n  \\]\n  \\[\n  \\mathcal{L}_{\\text{IMG}} = \\beta_{\\text{MSE}} \\, \\| \\gamma(x_o) - \\gamma(x_w) \\|_2^2 + \\beta_{\\text{LPIPS}} \\, \\mathrm{LPIPS}(x_o,x_w) + \\beta_{\\text{GAN}} \\, \\Big( \\mathbb{E}_{x_o}[\\log D_{\\text{disc}}(x_o)] + \\mathbb{E}_{x_o,w}[\\log (1 - D_{\\text{disc}}(x_w))] \\Big),\n  \\]\n  where \\( \\gamma(\\cdot) \\) maps RGB to YUV and \\( D_{\\text{disc}} \\) is a standard GAN discriminator. Stage-wise weights: stage-1 \\( \\alpha=10 \\), \\( \\beta_{\\text{MSE}}=\\beta_{\\text{LPIPS}}=\\beta_{\\text{GAN}}=0.01 \\); stage-2 \\( \\alpha=1.5 \\), \\( \\beta_{\\text{MSE}}=2.0 \\), \\( \\beta_{\\text{LPIPS}}=1.5 \\), \\( \\beta_{\\text{GAN}}=0.5 \\).\n\n- Straight-through estimator in stage-3:\n  \\[\n  \\tilde{x} = \\varepsilon(x_w), \\quad \\text{forward as usual}, \\qquad \\frac{\\partial \\mathcal{L}}{\\partial x_w} \\approx \\frac{\\partial \\mathcal{L}}{\\partial \\tilde{x}},\n  \\]\n  i.e., treat \\( \\varepsilon(\\cdot) \\) as identity in the backward pass to enable gradient flow.\n\n- Frequency analysis (designing surrogate attacks):\n  \\[\n  \\Delta(\\omega) = \\big| \\mathcal{F}(\\varepsilon(x_w)) - \\mathcal{F}(\\varepsilon(x_o)) \\big|,\n  \\]\n  where \\( \\mathcal{F} \\) is the Fourier transform. Empirically, \\( \\Delta(\\omega) \\) is large in mid/high-frequencies for diffusion-based editing; blurs produce a similar attenuation pattern, justifying the surrogate set \\( \\{t_i\\} \\).\n\n- Detection thresholds (evaluation-time statistic):\n  With match probability \\( p_w \\) (watermarked) and \\( p_o \\) (original), for threshold \\( \\tau \\),\n  \\[\n  \\mathrm{TPR}(\\tau) = I_{p_w}(\\tau+1, K-\\tau), \\qquad \\mathrm{FPR}(\\tau) = I_{p_o}(\\tau+1, K-\\tau),\n  \\]\n  where \\( I_p(a,b) \\) is the regularized incomplete beta function, ensuring control at e.g. 0.1% FPR.\n\n- Complexity:\n  - Standard multi-step diffusion encoders: \\( \\mathcal{O}(R \\cdot HW \\cdot c \\cdot k^2) \\) per image (R sampling steps).\n  - Proposed one-step encoder: \\( \\mathcal{O}(HW \\cdot c \\cdot k^2) \\) for VAE+UNet; distortions add \\( \\mathcal{O}(HW) \\).\n  - Resolution scaling: compute residual at base \\( (u,v) \\) then upsample: \\( \\mathcal{O}(uv + HW) \\).\n\nComputational_Properties:\n- Time Complexity:\n  - Training (stage-1/2): \\( \\mathcal{O}(HW \\cdot c \\cdot k^2) \\) for encoder forward/backward + \\( \\mathcal{O}(HW) \\) for distortions + decoder \\( \\mathcal{O}(HW \\cdot C_d) \\). GAN discriminator adds \\( \\mathcal{O}(HW \\cdot C_{\\text{disc}}) \\).\n  - Fine-tuning (stage-3): same, plus forward through \\( \\varepsilon(\\cdot) \\) (editor), typically \\( \\mathcal{O}(S_\\varepsilon \\cdot HW \\cdot c \\cdot k^2) \\) with \\( S_\\varepsilon \\) denoising steps; backward uses straight-through (no extra editor gradients).\n  - Inference (encoding): one forward pass \\( \\mathcal{O}(HW \\cdot c \\cdot k^2) \\); resolution scaling reduces compute by doing \\( \\mathcal{O}(uv) \\) then upsample residual \\( \\mathcal{O}(HW) \\).\n  - Inference (decoding): \\( \\mathcal{O}(HW \\cdot C_d) \\) for ConvNeXt-B.\n\n- Space Complexity:\n  - Activations for VAE encoder/decoder and UNet dominate: \\( \\mathcal{O}(HW \\cdot c) \\) per stage; skip connections store \\( L \\) intermediate tensors; zero-conv adds negligible parameters but stores \\( L \\) gated feature maps.\n  - Noise layer adds no significant parameters; distortions operate in-place.\n  - Decoder memory: \\( \\mathcal{O}(HW \\cdot C_d) \\).\n  - Empirical peak GPU memory ~5 GB for 512×512 (reported), larger than baselines due to UNet+VAE and skip buffers.\n\n- Parallelization:\n  - All convolutions (VAE/UNet/ConvNeXt) are data-parallel and tensor-parallel friendly; batch parallelization scales linearly.\n  - Distortion operators (blurs, JPEG, noise) are per-image kernels; embarrassingly parallel across the batch.\n  - Stage-3 fine-tuning: editor forward is parallelizable; straight-through keeps backward simple (no editor gradients).\n\n- Hardware Compatibility:\n  - GPU-optimized due to heavy convolution/activation usage; benefits from high memory bandwidth and cuDNN kernels.\n  - CPU inference feasible for decoder; encoder recommended on GPU for throughput.\n  - Resolution scaling reduces encoder compute at high target resolutions by computing residual at base size and interpolating.\n\n- Training vs. Inference:\n  - Training uses distortions and adversarial loss; stage-1 freezes UNet+decoder to retain generative prior stability, reducing gradient noise.\n  - Inference omits noise layer; encoder produces watermarked image in a single pass; decoder runs for detection.\n\n- Parameter Count:\n  - Added parameters: condition adaptor (small FC+conv stack), zero-conv gates (per skip), discriminator, and ConvNeXt-B decoder (tens of millions). Overall increase is minor relative to SDXL-Turbo backbone size.\n  - ControlNet-style branch is avoided to prevent conflicting residual signals; condition adaptor is leaner.\n\n- Numerical Stability:\n  - Zero-conv initialization at zero prevents skip-induced artifacts at start; gradual learning improves stability.\n  - Straight-through estimator only in late training (stage-3) to avoid divergence observed when applied from scratch.\n  - YUV-space MSE stabilizes color fidelity; LPIPS balances perceptual quality against low-level losses.\n\n- Scaling Behavior:\n  - One-step generation scales linearly in pixel count; no dependence on sampling steps \\( R \\).\n  - Resolution scaling decouples robustness and quality from target resolution, enabling arbitrary-size watermarking while preserving base-model properties.\n  - As model size (channels, layers) increases, robustness improves due to stronger priors, with proportional increases in compute/memory.\n\nImplementation-critical details and pitfalls:\n- Use null-text prompt for UNet; attempting to encode watermark via text encoder led to non-convergence.\n- Avoid ControlNet-like extra branch in one-step models; providing two structural streams degraded convergence and quality.\n- Apply blurs with varying severity probabilistically per batch to cover a spectrum of editing-like frequency attenuations.\n- Stage scheduling is crucial: freeze UNet+decoder + zero-conv gates at start; unfreeze only after decoder achieves ≥0.85 bit accuracy.\n- For detection thresholding, set \\( \\tau \\) to guarantee desired FPR (e.g., 0.1%); do not rely solely on bit accuracy or AUROC.",
        "IMPLEMENTATION_GUIDANCE": "Integration_Strategy:\n- Modules to modify or add\n  - Watermark encoder: replace your existing encoder with an SDXL-Turbo–based conditional generator. Keep your current decoder if desired, but we recommend ConvNeXt-B.\n  - SDXL-Turbo integration (Diffusers-style codebase):\n    - Modify AutoencoderKL so its encoder conv_in accepts 6 channels instead of 3.\n      - In HuggingFace diffusers, patch vae.encoder.conv_in = nn.Conv2d(6, base_channels, kernel_size=3, stride=1, padding=1).\n    - Insert skip connections from VAE encoder down blocks to decoder up blocks:\n      - Tap features after each VAE encoder downsample block.\n      - Route through ZeroConv (1x1 conv layers with zero initialization) into corresponding VAE decoder upsample blocks and add element-wise.\n  - Condition adaptor (new module) before VAE encoder:\n    - Fuse the watermark bits and input image into a 6-channel tensor:\n      - Watermark branch: w ∈ {0,1}^100 → FC1(100→4096) → ReLU → FC2(4096→12288) → reshape to [B,3,64,64] → Conv2d(3,3,3,1,1) → ReLU → Upsample(scale_factor=4, mode='bilinear') → [B,3,256,256].\n      - Image branch: Conv2d(3,3,3,1,1) → ReLU → Downsample to [B,3,64,64] if you want symmetry with the watermark path, then upsample back to [B,3,256,256].\n      - Concatenate along channel dim → [B,6,256,256] and feed to VAE encoder.\n    - Alternatively, keep the image branch identity (no conv) and only upsample the watermark branch.\n  - Noise layer (train-time): insert a stochastic augmentation layer between encoder output and decoder input; include both communication distortions and surrogate edits (blurs). Implement as a composable nn.Module applied with probability per batch.\n  - Discriminator: add a small PatchGAN discriminator for adversarial loss; wire into the image quality loss.\n  - Watermark decoder: ConvNeXt-B backbone followed by a linear head to 100 logits. Use BCEWithLogitsLoss.\n- Code-level changes (PyTorch)\n  - Create ZeroConv class:\n    - nn.Conv2d(Cin, Cout, kernel_size=1, bias=True); initialize weight and bias to zeros.\n  - Modify VAE forward:\n    - Return intermediate down block features from encoder.\n    - Accept a list of ZeroConv hooks in decoder; at each up block, add zero_conv_i(enc_feats_i).\n  - Thresholding for detection:\n    - Compute TPR@FPR via binomial calibration. Given po≈0.5 on unwatermarked images, choose τ such that I_po(τ+1, k−τ)≤desired FPR (e.g., 0.1%). Store τ per model/version.\n- Framework compatibility\n  - PyTorch 2.1+ recommended; Diffusers ≥0.24 for SDXL-Turbo; timm for ConvNeXt-B.\n  - Xformers/Flash-Attn optional; SDXL-Turbo UNet benefits from flash attention if available.\n- Migration path\n  - If you currently use a UNet encoder: keep your decoder; swap encoder for SDXL-Turbo+VAE with condition adaptor; port your noise layer into the new pipeline.\n  - Start with VINE-Base stage-1 training (freeze UNet + VAE decoder) to stabilize; then unfreeze (stage-2); optionally add edit model STE fine-tune (stage-3).\n- External dependencies and licenses\n  - Editing models for W-Bench: Instruct-Pix2Pix (Diffusers), ControlNet-Inpainting (Diffusers), UltraEdit and MagicBrush (ensure availability/licensing).\n  - SVD for I2V (Stable Video Diffusion); MAGE+ optional for extended TI2V.\n- Training pipeline integration\n  - DDP across 8 GPUs with GradScaler (AMP/bfloat16).\n  - Data: OpenImages for pretraining; Instruct-Pix2Pix dataset for stage-3 fine-tune.\n  - Logging: track bit accuracy, AUROC, TPR@{0.1%,1%}FPR, PSNR/SSIM/LPIPS/FID.\n- Resolution scaling integration (encode at arbitrary H×W)\n  - Compute residual at training resolution (e.g., 256×256), bilinear-upsample residual to target H×W, add to normalized input, clamp to [-1,1], then denormalize. This preserves visual quality and robustness for edits at 512×512.\n\nParameter_Settings:\n- Watermark configuration\n  - Bit length: 100 bits (k=100).\n  - Optional ECC: BCH(127, 79) or BCH(127, 64) with t=10–15; increases robustness at the cost of payload.\n- Loss weights\n  - Stage-1: α=10.0, βMSE=0.01, βLPIPS=0.01, βGAN=0.01.\n  - Stage-2: α=1.5, βMSE=2.0, βLPIPS=1.5, βGAN=0.5.\n  - LPIPS: use vgg backbone.\n  - RGB→YUV mapping γ(·): fixed non-parametric differentiable transform for MSE term.\n- Optimizer and schedules\n  - Adam or AdamW; LR=1e-4 (stages 1–2), LR=5e-6 (stage-3 fine-tune with editing).\n  - Weight decay 1e-4 for adaptor/decoder; 0 or 1e-5 for VAE/UNet when unfrozen.\n  - Gradient clipping: 1.0.\n- Training lengths and batch sizes\n  - Stage-1+2 total ~111k steps at 256×256, batch size 112 on 8×A100-80GB (per paper).\n  - Stage-3 fine-tune ~80k steps, batch size 80.\n  - For 24GB GPUs: batch 8–12 per GPU, gradient accumulation to reach effective 64–96.\n- Initialization\n  - Condition adaptor: Kaiming normal for convs; FC weights Xavier normal; ZeroConv weights/bias initialized to 0.\n  - Decoder head (linear 100): Xavier uniform, bias=0.\n- Noise layer (per-sample random composition; p indicates apply probability)\n  - Classical (transmission):\n    - Brightness: factor ∈ [0.7, 1.3], p=0.3.\n    - Contrast: factor ∈ [0.7, 1.3], p=0.3.\n    - JPEG: quality ∈ [30, 90], p=0.5.\n    - Gaussian noise: σ ∈ [2/255, 10/255], p=0.5.\n    - Shot noise: rate ∈ [0.01, 0.08], p=0.2.\n    - Impulse noise (salt-pepper): prob ∈ [0.005, 0.03], p=0.2.\n    - Speckle noise: σ ∈ [0.01, 0.05], p=0.2.\n  - Surrogate edits (blurs; critical):\n    - Gaussian blur: kernel ∈ {3,5,7,9}, σ ∈ [0.5,3.0], p=0.6.\n    - Defocus blur: radius ∈ [2, 6] px, p=0.5.\n    - Motion blur: length ∈ [5, 25] px, angle ∈ [0°, 180°], p=0.4.\n    - Zoom blur: strength ∈ [0.5, 2.0], p=0.3.\n    - Pixelation: block size ∈ [4, 16] px (downsample/upsample), p=0.5.\n  - Compose at most 2–3 distortions per sample to avoid over-degradation; anneal severity up during training (e.g., linearly 0.5× → 1× range over first 50% steps).\n- Editing fine-tune (stage-3)\n  - Instruct-Pix2Pix: 25 steps (for speed), CFG text scale=7, image guidance=1.5.\n  - Straight-through estimator: treat the edit pipeline output as identity in backward; clip gradients to stabilize.\n- VAE and UNet\n  - Stage-1: freeze UNet + VAE decoder + ZeroConv layers.\n  - Stage-2: unfreeze all; consider Low-Rank Adaptation (LoRA rank 8–16) on UNet as memory-friendly alternative; full finetune yields best quality.\n- Discriminator\n  - PatchGAN 70×70; channels progression 64–128–256–512; use BCE loss (as in paper) or Hinge (robust); LR=1e-4; β1=0.5, β2=0.999.\n- Detection thresholding and metrics\n  - Estimate po on a held-out set of unwatermarked images; typically ≈0.5. Compute τ for FPR=0.1% using binomial tail I_po(τ+1, k−τ)≤0.001. For k=100 and po=0.5, τ≈63–65.\n  - Report TPR@{0.1%,1%}FPR; also track raw bit accuracy and AUROC (not sufficient alone).\n- Scale-dependent settings\n  - Train at 256×256; edit/eval at 512×512 using resolution scaling.\n  - For 1024×1024 deployment, compute residual at 256, upsample to 1024; maintain robustness and quality.\n- Hardware-dependent settings\n  - A100-80GB: batch 112 (fp16 or bf16), flash attention on UNet if available.\n  - RTX 3090/4090 (24GB): batch 8–12, grad accumulation 8–10, gradient checkpointing on VAE/UNet blocks; disable some surrogate distortions in same step to reduce CPU bottlenecks.\n  - Inference memory ~5 GB per image on RTX6000; runtime ~0.08 s/image reported.\n\nApplication_Conditions:\n- Beneficial scenarios\n  - Robust watermarking for real photographs that may undergo T2I-based edits: image regeneration (stochastic VP ts=60–240; deterministic DPM-Solver nd=15–45), global edits (Instruct-Pix2Pix, MagicBrush, UltraEdit), local edits (ControlNet-Inpainting, UltraEdit), and light I2V (SVD).\n  - When you must keep only the edited image for detection (no access to original reference).\n- Hardware requirements\n  - Training: ≥8×A100-80GB for reported batch sizes; feasible on 1–4 GPUs with smaller batches and accumulation.\n  - Inference: single 8–12GB GPU is sufficient for 512×512 with batch size 1–2.\n- Scale considerations\n  - Technique shows best trade-off at 256→512 pipeline; pretraining larger than 256 (e.g., 512) increases compute substantially with marginal robustness gains.\n- Task compatibility\n  - Strong for regeneration, global, and local edits; limited for I2V where scene/layout changes across frames suppress frequency patterns.\n  - Neutral for classic transmission distortions (JPEG, noise) with good resilience from noise layer.\n- Alternative comparisons\n  - Choose VINE over StegaStamp/MBRS when image quality matters (LPIPS ~0.003–0.006) and you need high TPR at 0.1% FPR after edits.\n  - If you can keep a reference watermarked original at detection time, EditGuard’s tamper localization is an alternative, but it is not suited to the “edited-only” detection constraint.\n- Resource constraints\n  - If training budget is tight: skip stage-3 fine-tune; VINE-Base already provides strong robustness with better quality.\n  - If memory-bound: use LoRA for UNet, keep VAE decoder frozen, and reduce edit steps (e.g., 15–25) in fine-tune.\n\nExpected_Outcomes:\n- Performance improvements (vs baselines at 512×512)\n  - VINE-Base: PSNR ≈40.5 dB, SSIM ≈0.995, LPIPS ≈0.0029, FID ≈0.08; TPR@0.1%FPR high across edits (e.g., 99.25% deterministic regeneration, 96.3% Instruct-Pix2Pix, 99.6% UltraEdit-local).\n  - VINE-Robust (after stage-3): PSNR ≈37.3 dB, SSIM ≈0.993, LPIPS ≈0.0063; TPR@0.1%FPR further improves (e.g., 99.98% deterministic regeneration, 97.46% Instruct-Pix2Pix, 99.96% UltraEdit-local).\n  - Expect +5–20% absolute TPR@0.1%FPR over strong baselines under global/local edits while maintaining top-2 image quality.\n- Timeline\n  - Stage-1 yields fast convergence on bit accuracy (>0.85 in days on 8×A100).\n  - Stage-2 refines quality; stage-3 adds 5–10% robustness under edits within another several days.\n- Trade-offs\n  - Adding surrogate blurs increases robustness but can reduce PSNR/SSIM if overused.\n  - Stage-3 (STE through edit model) increases robustness with small quality loss (e.g., LPIPS +0.003–0.004).\n- Benchmark comparisons (W-Bench)\n  - Regeneration: near-perfect TPR@0.1%FPR across VP and DPM-Solver settings.\n  - Global editing (CFG=5–9): robust at higher guidance; VINE-R > VINE-B by ~1–2.5% absolute on average.\n  - Local editing (mask 10–60%): >93–99% TPR@0.1%FPR across sizes; competitive with or better than StegaStamp while preserving quality.\n  - I2V (SVD): moderate TPR; frequency analysis shows patterns attenuated broadly.\n- Failure modes\n  - Image-to-video with large motion or scene/layout changes suppresses watermark energy across bands; TPR drops; consider stronger low-frequency imprint or ECC.\n  - Early use of STE fine-tune causes non-convergence; apply only after stage-2 with stable decoder.\n  - Overly aggressive GAN or too low βMSE/βLPIPS can introduce artifacts or reduce edit compatibility.\n  - High po on unwatermarked images (e.g., due to decoder bias) inflates FPR; recalibrate τ and regularize decoder.\n- Debugging indicators\n  - Frequency maps |F(xw)−F(xo)| should show dense energy near low-frequency center with cross-shaped axes intensity but not overwhelming high-frequency patterns.\n  - Under edits, |F(ε(xw))−F(ε(xo))| retains visible low-frequency energy for robust models.\n  - Monitor po on a clean validation set; keep ≈0.5±0.02; if higher, add stronger regularization on decoder and hard negatives (unwatermarked images) in training.\n  - If LPIPS stalls >0.01 in stage-2, verify skip connections and ZeroConv are wired correctly (non-zero grads).\n- Hardware-specific outcomes\n  - Inference on RTX6000: ~0.08 s/image, ~5 GB peak (reported).\n  - On A100 with flash attention: ~10–20% speedup in UNet; memory headroom enables larger batches.\n\nTroubleshooting and Validation:\n- Confirm stage-1 freeze mask: UNet and VAE decoder parameters remain untouched; only condition adaptor, VAE encoder, discriminator, and watermark decoder update.\n- Validate resolution scaling:\n  - Watermark at 512×512 should preserve PSNR/SSIM relative to 256×256 training; robustness to Gaussian blur and edits maintained or improved.\n- Evaluate thresholds:\n  - Compute τ on ≥5k unwatermarked images; verify empirical FPR≤spec (0.1%, 1.0%).\n- Stress tests:\n  - Regeneration VP ts=240; DPM-Solver nd=15; Global edits CFG=9; Local edits mask 50–60%.\n  - Expect graceful degradation; if sharp TPR drop, increase blur surrogate severity or add zoom/motion blur diversity.\n- Edit quality checks:\n  - CLIPdir/CLIPimg/CLIPout should remain close to unwatermarked baselines; if degraded, increase βMSE/βLPIPS and reduce GAN strength.\n- Data pipeline:\n  - Ensure diverse distortions per batch but cap to 2–3 applied transforms; random seeds per sample; avoid CPU bottlenecks by using CUDA-accelerated ops where possible.\n\nValidation procedures to ensure correctness:\n- Unit-test condition adaptor output shape [B,6,256,256]; verify VAE encoder conv_in accepts 6 channels.\n- ZeroConv outputs initially zero; forward parity of VAE with and without skip connections at step 0 must match bitwise.\n- Fourier diagnostics on 1k samples to match low-frequency dominance patterns similar to figures (dense center + axis-aligned energy).\n- Reproduce baseline metrics on a 1k subset before full-scale runs to confirm pipeline parity."
    }
]