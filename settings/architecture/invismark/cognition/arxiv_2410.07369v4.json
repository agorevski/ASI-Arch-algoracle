[
    {
        "DESIGN_INSIGHT": "### DESIGN_INSIGHT_HIGH: [PRC-Gaussian Sign Coding – Undetectable In-Processing Watermark via Latent Sign Control]\n- Replaces heuristic latent perturbations (e.g., Fourier rings, quadrant constraints) with a cryptographic pseudorandom code embedded directly in the signs of the initial diffusion latents. Instead of modifying the denoising network or adding post-process noise, the sampler draws a Gaussian latent and flips each coordinate’s sign to match a PRC codeword.\n- Key mechanism: sample y ∼ N(0, I_n), sample a PRC codeword c ∈ {−1, 1}^n, and set the initial latent by sign-imposing\n  \\[\n  \\tilde z^{(T)}_i \\;=\\; c_i\\cdot |y_i|\\quad \\text{for } i\\in [n].\n  \\]\n  Since the signs of a standard Gaussian are i.i.d. uniform and a PRC generates bitstrings that are computationally indistinguishable from uniform (even jointly across polynomially many samples), the distribution of watermarked latents is indistinguishable from the original Gaussian. By efficiency of Generate, this indistinguishability propagates to the output images (Theorem 1).\n- Fundamental difference from prior in-processing methods: Tree-Ring imposes structured Fourier patterns; Gaussian Shading restricts latents to fixed quadrants. Both introduce detectable correlations across samples and measurable quality shifts. Here, all coordinates are used (semantic-level dispersion), but only through sign constraints that preserve the exact per-sample Gaussian marginal and eliminate cross-sample artifacts under any efficient test.\n- Mathematical guarantee: for any efficient adversary A,\n  \\[\n  \\big|\\Pr[A(\\text{PRCWat.Sample})=1]-\\Pr[A(\\text{Sample})=1]\\big|\\le O(n^{-c}),\n  \\]\n  leveraging PRC pseudorandomness (from LPN-type hardness). Complexity is O(n) to impose signs; no model retraining or architecture change. Robustness stems from the fact that removing the watermark requires flipping many latent signs dispersed across all dimensions, which is hard without visibly degrading the image after decoding.\n\n\n### DESIGN_INSIGHT_HIGH: [Soft-Decision PRC Detection with Exact FPR Control – LLR Aggregation over Sparse Parity Checks]\n- Replaces heuristic detectors (e.g., magnitude thresholds on Fourier rings) with a principled, soft-decision likelihood-ratio test that aggregates evidence over sparse parity checks and admits a closed-form, provable false-positive bound. The detector operates on the recovered initial latent obtained via diffusion inversion.\n- Key mechanism: invert x to z′ ≈ z via Recover. Model z ∼ N(0, I_n), z′ ∼ N(z, \\sigma^2 I_n). For each coordinate, compute the posterior soft sign\n  \\[\n  s_i \\;=\\; (1-2\\eta)\\,\\mathrm{erf}\\!\\left(\\frac{z'_i}{\\sqrt{2\\sigma^2(1+\\sigma^2)}}\\right).\n  \\]\n  For each sparse parity check w, form \\(\\hat s_w=\\prod_{i\\in w} s_i\\). With a one-time pad a_w∈{±1}, compute the log-likelihood ratio sum\n  \\[\n  S \\;=\\; \\sum_{w\\in\\mathcal{P}} \\log\\!\\left(\\frac{1+a_w \\hat s_w}{2}\\right).\n  \\]\n  Decide “watermarked” iff\n  \\[\n  S \\;\\ge\\; \\sqrt{C\\,\\log(1/F)} \\;+\\; \\tfrac{1}{2}\\sum_{w\\in\\mathcal{P}}\\log\\!\\left(\\frac{1-\\hat s_w^{\\,2}}{4}\\right),\n  \\]\n  where \\(C=\\tfrac{1}{2}\\sum_{w\\in\\mathcal{P}}\\log^2\\!\\left(\\frac{1+\\hat s_w}{1-\\hat s_w}\\right)\\).\n- Fundamental difference from prior detectors: this is a soft-information, code-theoretic test matched to the generative/inversion noise model and the PRC structure. It controls the false positive rate independently of empirical calibration, yielding\n  \\[\n  \\Pr_{k\\sim \\text{KeyGen}}[\\text{Detect}_k(x)=\\text{True}]\\;\\le\\;F\n  \\]\n  for any fixed x (Theorem 2), using a bounded-difference martingale inequality to set the threshold. Prior detectors lack tight, distribution-free FPR guarantees and often rely on per-dataset tuning.\n- Complexity and robustness: Each check aggregates O(t) terms (t-sparse parity), total O(|\\mathcal{P}|\\,t) per detection. Because evidence is multiplied and then log-aggregated across many checks, the detector exploits reliability variations across coordinates (via s_i) and remains robust to moderate inversion/attack noise without increasing the FPR.\n\n\n### DESIGN_INSIGHT_MEDIUM: [LDPC-Style PRC with Message Embedding and BP-OSD Decoding in Latent Sign Space]\n- Extends zero-bit watermarking to multi-bit messaging by replacing fixed patterns with an LDPC-style pseudorandom code whose codeword populates all latent signs. This upgrades in-processing watermarks from “present/absent” to authenticated, high-capacity carriers without changing the image generator.\n- Key mechanism: KeyGen samples a sparse-parity generator G and check matrix P, a one-time pad otp, and trapdoor testbits. A message-bearing codeword is built as\n  \\[\n  c \\;=\\; G\\,y \\;\\oplus\\; e \\;\\oplus\\; \\text{otp}\\quad\\text{with}\\quad y=(\\text{testbits},\\,r,\\,m),\n  \\]\n  where r is random seed, m is the payload, and e ∼ Ber(η)^n injects controlled noise for pseudorandomness. Encoding is mapped to latents via the sign-imposition \\(\\tilde z^{(T)}_i=c_i |y_i|\\). Decoding uses belief propagation with ordered statistics decoding (BP-OSD) on soft inputs s_i from the detector to recover y and hence m, validating with testbits. Detection remains faster and more robust; decoding trades some robustness for capacity.\n- Difference from prior work: Tree-Ring is zero-bit and spatially localized (Fourier rings); Gaussian Shading has per-sample distributional parity but introduces cross-sample correlations (fixed quadrant) and cannot naturally carry long messages with cryptographic pseudorandomness. Here, message capacity scales with n and code rate, while undetectability follows from the joint pseudorandomness of PRC codewords over many generations.\n- Tunable trade-offs and complexity: sparsity t controls undetectability hardness vs. robustness; F sets a rigorous FPR upper bound; message length adjusts code rate and BP complexity (≈ O(|\\mathcal{P}|\\,t·iters)). Because watermark bits span all coordinates, attempts to remove or spoof the message must globally disrupt latent signs, typically degrading perceptual quality after decoding, unlike localized post-processing schemes.",
        "EXPERIMENTAL_TRIGGER_PATTERNS": "**Task_Performance_Signatures**:\n- Image quality parity (vs. un-watermarked Stable Diffusion 2.1 base, 512×512, 50 DPM-Solver steps)\n  - Expect preserved FID/CLIP/IS within statistical noise:\n    - COCO: FID ≈ 76.60 ± 0.27 vs. 76.40 ± 0.31 (≤0.5% relative gap), CLIP ≈ 0.477 ± 0.0039 vs. 0.479 ± 0.0025, IS ≈ 17.47 ± 0.17 vs. 17.54 ± 0.12.\n    - SDP prompts: FID ≈ 63.74 ± 0.35 vs. 63.46 ± 0.25, CLIP ≈ 0.615 ± 0.0014 vs. 0.612 ± 0.0018, IS ≈ 7.50 ± 0.08 vs. 7.50 ± 0.09.\n  - Variability (LPIPS across generations for same prompt) remains indistinguishable from baseline: 0.7589 ± 0.0019 vs. 0.7570 ± 0.0018. Expect no mode-collapsing artifacts (contrast: Gaussian Shading ≈ 0.6503 ± 0.0021).\n\n- Detectability and robustness (Detector TPR at fixed FPR)\n  - Baseline (no attack): TPR@FPR=0.01 ≈ 1.00.\n  - Under common distortions and regeneration attacks, expect:\n    - No attack can push TPR below 0.99 while keeping FID < 70 (quality-preserving regime).\n    - JPEG Q=20: TPR typically ≈ 0.94 with strong quality degradation (FID > 75; PSNR ≈ 28.4), indicating attacker trade-off (quality vs. removal).\n    - Adversarial surrogate-classifier (PGD) attacks: ineffective in practice; surrogate cannot be trained (validation at chance).\n    - Embedding-space attacks: PRC is the only in-processing scheme that can maintain detection at acceptable FID under low-strength settings (e.g., “strength-2” retains >95% detection but introduces visible artifacts).\n  - Multi-key detectability surrogate task: binary classifier distinguishing PRC keys remains ≈ 50% validation accuracy across 10 epochs; contrast: other schemes reach >90–100% validation accuracy in few epochs.\n\n- Configurable false positives\n  - With F configured to 0.01, observed FPR stays ≤ 1% by construction. Expect similarly bounded FPR for Decode (due to test bits).\n\n- Message capacity and decoding\n  - 512-bit payloads: detector remains more robust than decoder; decoder maintains high success under moderate perturbations (expect a modest drop vs. detector).\n  - Clean images (no removal attacks): reliable decoding up to ≈ 2500 bits with t=4, F=1e-9, σ=0.\n  - Expect “detector > decoder” robustness ordering under the same conditions.\n\n- Parameter scaling and sensitivity\n  - Latent dimensionality n = 4×64×64 = 16384 (Stable Diffusion 2.1 base). Expect robustness to improve with larger n.\n  - Parity sparsity t:\n    - t=3 (default): best empirical robustness/quality trade-off demonstrated.\n    - t=2: slightly higher robustness against distortions (curves shift upward), but weaker undetectability guarantees.\n    - t≥7: stronger asymptotic undetectability; expect reduced robustness to attacks and stricter clean-image requirements for reliable decoding.\n  - σ (noise model for soft-decision detection): default σ≈√(3/2) works broadly; σ=0 for clean decoding.\n\n- Qualitative signatures on outputs\n  - No visible artifacts or prompt-level biases; scene composition diversity preserved across seeds. Expect absence of consistent quadrant/FFT ring patterns (contrast: Gaussian Shading shows fixed latent quadrant bias; Tree-Ring shows learnable Fourier ring behavior).\n\n- Cross-domain benchmarks (language and QA tasks listed for completeness)\n  - lambada_openai, wikitext, ptb, squad_completion, squad_v2, narrativeqa, hellaswag, piqa, social_iqa, commonsenseqa, arc_easy, arc_challenge, boolq, openbookqa, winogrande, winograd, swde, fda: not applicable to this vision watermarking method; expect no measurable change relative to task baselines (i.e., similar accuracy/performance).\n\n**Architectural_Symptoms**:\n- Training characteristics (no model retraining required)\n  - Generation path unchanged; sampling from identical diffusion schedule with O(n) sign assignment to the initial latent only.\n  - Surrogate-detector training symptom: for PRC images, training accuracy climbs but validation remains ≈ 50% (memorization without generalization). Other schemes show smooth convergence to high validation accuracy quickly—this contrast is a strong positive indicator for PRC undetectability in practice.\n  - Stability: no divergence/NaNs observed; FPR remains at or below configured bound F across runs.\n\n- Runtime and memory behavior\n  - Generation throughput: near-identical to un-watermarked sampling (no extra diffusion steps; constant-time sign mapping on z(T)); GPU utilization unchanged; memory footprint same as baseline inference.\n  - Detection:\n    - Dominated by inversion (e.g., 50 exact inversion steps, inverse order 0). Expect runtime roughly comparable to a single sampling pass; linear in inversion steps T.\n    - Memory: similar to one diffusion pass; no OOM at standard batch sizes on H100; linear in latent dimension n.\n  - Decoding (message extraction):\n    - Adds belief propagation + OSD compute; slower than detection; modest extra memory overhead; throughput lower than detection but still practical for batch verification.\n\n- Profiling signatures and thresholds\n  - Soft-decision pipeline present: per-latent coordinate soft sign s_i = (1−2η)·erf(z′_i / √(2σ²(1+σ²))).\n  - Test statistic S = Σ_w log((1 + a_w·∏_{i∈w} s_i)/2) exceeds a theoretically derived threshold based on C = (1/2)Σ_w log²((1+ŝ_w)/(1−ŝ_w)) and log(1/F). True positives: S consistently above threshold; benign (non-watermarked) images: S below threshold with probability ≤ F.\n  - Detector > decoder speed and robustness: if decode fails on attacked images but detect succeeds, this asymmetry is expected.\n\n- Scaling properties\n  - Robustness improves with higher latent dimension n (more parity checks contribute signal).\n  - Increasing t raises undetectability cost but reduces noise tolerance; practitioners should observe a monotonic robustness drop vs. t under fixed attacks.\n  - Consistent quality scaling: FID/CLIP/IS remain statistically indistinguishable from baseline across dataset scale (COCO vs. SDP prompts).\n\n- Hardware utilization patterns\n  - Generation: identical GPU kernel mix and memory bandwidth to baseline diffusion inference.\n  - Detection/decoding: inversion adds diffusion-like kernels; expect similar GPU occupancy; no quadratic memory growth; stable performance as batch size increases until standard diffusion-inference limits.\n\n- Negative/neutral indicators\n  - Severe photometric/compression distortions (e.g., JPEG Q≈20 or stronger regeneration) lead to TPR falling toward 0.90–0.95; decoder failure rates increase faster than detector.\n  - Embedding-space adversarial attacks can force low TPR only at the expense of noticeable visual degradation (FID > ~80, visible blur/artifacts).\n  - With very large t (e.g., ≥7), detection sensitivity to noise increases; expect more frequent decode failures on slightly perturbed images unless inversion accuracy is improved or σ tuned.",
        "BACKGROUND": "Title: AN UNDETECTABLE WATERMARK FOR GENERATIVE IMAGE MODELS\n\nHistorical Technical Context:\nWatermarking in images historically relied on post-processing techniques from classical signal processing and steganography, such as DWT/DCT/SVD embedding and learned encoders/decoders (e.g., RivaGAN, StegaStamp). With the advent of deep generative models—first VAEs and GANs, then diffusion and flow-based models—the field saw in-processing watermarking that modifies the generative process itself. Latent diffusion models (e.g., Stable Diffusion) became the dominant architecture for text-to-image generation, where images are produced by iteratively denoising a Gaussian latent z(T) ∼ N(0, I) with a neural denoiser ϵ and decoding via an autoencoder D.\n\nTwo notable in-processing image watermarks preceded this work. Tree-Ring watermarking fixes a structured pattern (e.g., concentric Fourier rings) in the initial latent and detects by inversion (DDIM inversion). Gaussian Shading samples latents from a restricted region (e.g., a fixed quadrant) and tests proximity of inverted latents to that region. In parallel, “undetectability” was formally defined for language-model watermarking and cryptographic pseudorandom codes (PRCs) were proposed as a tool to make watermarks computationally indistinguishable from non-watermarked outputs. These developments motivated bringing cryptographic indistinguishability and rigorous false-positive guarantees to image watermarking without sacrificing quality or variability across generations.\n\nPrior approaches revealed key shortcomings. Post-processing watermarks are fundamentally detectable under minimal assumptions (since any deterministic change to a fixed image can be tested). Tree-Ring imposes a large shift from the Gaussian latent prior, degrading image quality and being learnable by surrogate detectors. Gaussian Shading’s “lossless” per-sample guarantee does not prevent correlations across many generations, which hurts quality metrics that depend on sample sets (e.g., FID, Inception Score) and degrades perceptual diversity. Meanwhile, pixel-level removal and model-based regeneration attacks can weaken robustness. This work proposes the first image watermark with cryptographic-style undetectability, by choosing latent signs via a PRC so that the joint latent distribution remains computationally indistinguishable from Gaussian while enabling robust semantic-level detection after inversion.\n\nTechnical Limitations:\n- Detectability and distribution shift in prior in-processing schemes: Fixed latent patterns (Tree-Ring) and region restrictions (Gaussian Shading) induce measurable deviations from z(T) ∼ N(0, I), making watermarks learnable by classifiers and degrading set-level quality metrics (e.g., significant FID/IS drops). Correlations across outputs violate the i.i.d. Gaussian latent assumption at scale.\n- Variability collapse across generations: Sampling from a fixed latent quadrant reduces diversity; LPIPS-based variability per prompt drops substantially (e.g., 0.6503 vs 0.7570 for originals), limiting user-facing exploration of outputs.\n- Lack of rigorous false-positive control: Many methods set thresholds empirically; they lack a provable bound F on false positives for any image independent of the key. This complicates deployment where missed detections and false attributions have policy and legal implications.\n- Limited payload capacity and zero-bit detection: Tree-Ring is inherently zero-bit; several methods cannot encode long messages robustly, limiting provenance metadata (e.g., timestamps, user IDs, signatures).\n- Fragility to removal and spoofing attacks: Pixel-level and regeneration (VAE/diffusion) attacks can erase or induce watermarks with minimal quality loss in prior schemes; surrogate classifier PGD can both remove and spoof detectable patterns.\n- Practical compute-robustness trade-offs: Diffusion inversion is approximate and computationally heavy (Recover costs O(Tn) denoising steps; here T=50, n≈4×64×64=16384). Decoding long messages with belief propagation is slower and less robust than detection; parameter t controlling PRC sparsity trades undetectability for robustness, with known brute-force distinguishers scaling as O(n^{t−1}).\n\nPaper Concepts:\n- Undetectable Watermark (cryptographic definition): A watermarking scheme with keyed sampling oracle Wk and detection oracle Dk is undetectable if for any probabilistic polynomial-time adversary A with oracle access, the advantage in distinguishing Wk from the unwatermarked sampler U is negligible in n: for any c>0,\n  Pr[A^{Wk}(1^n)=1] − Pr[A^{U}(1^n)=1] ≤ O(n^{-c}).\n  Intuition: Without the key, watermarked outputs are computationally indistinguishable from unwatermarked ones—no efficiently-computable quality metric or classifier can differentiate them, even across many adaptive queries.\n- Pseudorandom Code (PRC): A keyed error-correcting code family with encoder Encodek and detector Detectk such that any polynomial number of codewords are jointly indistinguishable from uniform bits. In the LDPC-style construction used here, the codeword c ∈ {0,1}^n (later mapped to signs) is sampled as\n  c = G y ⊕ e ⊕ otp,\n  where G is a generator-like matrix consistent with t-sparse parity checks P, y concatenates randomness and (optionally) message bits, e ∼ Ber(η)^n is a noise vector, and otp is a one-time pad. Security reduces to hardness of LPN-like problems; robustness derives from error-correcting structure spread across coordinates.\n- Latent Diffusion Generate/Recover pipeline: Image generation with a latent diffusion model proceeds from z(T) ∼ N(0, I) via updates z(i−1) = f_ϵ(π, z(i), i), then x = D(z(0)). Watermarking replaces z(T) with sign-constrained Gaussian samples ẑ(T) defined by ci ∈ {−1,1} from PRC: ẑ(T)_i = c_i |y_i|, y ∼ N(0, I). Detection relies on inversion Recover(x) ≈ z(T) using an exact DPM-solver inversion method to produce soft evidence on latent signs. Complexity is dominated by T denoising/inversion steps; detection adds lightweight algebra on parity checks.\n- Soft-decision PRC Detection Statistic: Given recovered latent z′ and estimated noise σ, compute per-coordinate soft sign expectations\n  s_i = (1 − 2η) · erf( z′_i / √(2σ^2(1+σ^2)) ).\n  For each parity check w, define ˆs_w = ∏_{i∈w} s_i and a random one-time pad factor a_w = ∏_{i∈w} (−1)^{otp_i}. The log-likelihood ratio across checks is\n  S = ∑_{w∈P} log( (1 + a_w ˆs_w)/2 ).\n  Detection accepts if\n  S ≥ √(C log(1/F)) + (1/2) ∑_{w∈P} log( (1 − ˆs_w^2)/4 ),\n  where C = (1/2) ∑_{w∈P} [log((1+ˆs_w)/(1−ˆs_w))]^2. Intuition: Products of soft signs accumulate evidence a la LDPC parity checks; the threshold ensures a provable FPR bound.\n- False Positive Rate (FPR) Guarantee: For any image x generated independently of the watermark key, with key k ← KeyGen(n, F, t), the detector and decoder satisfy\n  Pr[Detect_k(x)=True] ≤ F,  Pr[Decode_k(x)≠None] ≤ F.\n  The bound follows from the random one-time pad yielding independent ±1 signs per check and a martingale tail bound (bounded-variance Hoeffding-type inequality) over the sum S.\n- Practical Undetectability and Complexity Trade-off: The watermark inherits asymptotic undetectability from PRC security, but with known brute-force distinguishers over t-sparse checks that scale as O(n^{t−1}). Increasing t improves asymptotic security but reduces robustness (parity checks span more coordinates and are harder to satisfy under noise/inversion error). Detection time is O(|P|) after inversion; message decoding via belief propagation with ordered statistics (BP-OSD) is more expensive and less robust than detection.\n\nExperimental Context:\nThe paper evaluates in the in-processing text-to-image setting using Stable Diffusion 2.1 (512×512 resolution; latent 4×64×64; 50 DPMSolver steps; CFG=3.0). Watermarks compared include post-processing (DwtDct, DwtDctSvd, RivaGAN, StegaStamp, SSL) and in-processing (Stable Signature, Tree-Ring, Gaussian Shading, PRC). Evaluation prioritizes (i) quality preservation and undetectability, (ii) robustness to removal/spoofing attacks, and (iii) payload capacity.\n\nMetrics emphasize set-level quality and diversity: FID to COCO and to unwatermarked generations, CLIP score, Inception Score, and LPIPS variability per prompt. Detectability is stress-tested by training ResNet classifiers without keys to distinguish watermarked from non-watermarked images or across different keys; undetectability implies near-chance validation accuracy. Robustness is benchmarked under 10 attacks (photometric—brightness/contrast; degradations—Gaussian blur/noise, JPEG; regeneration—VAE-based and diffusion-based; adversarial—PGD surrogate classifier and VAE-embedding), reporting TPR@FPR=0.01 versus PSNR/SSIM/FID. Key findings: PRC uniquely preserves FID/IS/CLIP and LPIPS variability (e.g., COCO FID ≈ 76.60 vs 76.40 original; SDP FID ≈ 63.74 vs 63.46; LPIPS variability ≈ 0.7589 vs 0.7570 original), classifiers fail to generalize beyond chance on PRC, and removal attacks cannot reduce TPR below 0.99 without incurring large FID increases; strong JPEG (quality 20) lowers TPR to ~0.94 with visible blurring. Message capacity experiments show robust 512-bit decoding under attacks and reliable decoding up to ~2500 bits without attacks.",
        "ALGORITHMIC_INNOVATION": "**Core_Algorithm:**\n- Replace the standard Gaussian latent sampling in diffusion models with sign-constrained Gaussian sampling whose sign pattern is a pseudorandom codeword from a keyed Pseudorandom Code (PRC). Specifically, sample y ∼ N(0, I_n) and set the initial latent as ẑ_T = c ⊙ |y| where c ∈ {−1, 1}^n is a PRC codeword.\n- Keep the generative model unchanged; only the initial latent sampling is modified. Detection recovers an approximation z′_T of the initial latent via diffusion inversion and performs soft-decision PRC detection (and optional message decoding via belief propagation).\n- Fundamental change from prior in-processing watermarks (e.g., Tree-Ring, Gaussian Shading): the watermark is a cryptographically pseudorandom sign pattern distributed across all latent dimensions rather than a structured, easily learnable Fourier/sector pattern; this preserves distributional properties and inter-sample variability.\n- Scope: one-time change at sampling time per generated image; detection/decoding apply post hoc without any model retraining or architectural change.\n\n**Key_Mechanism:**\n- The key insight is that for z ∼ N(0, I_n), the sign bits are i.i.d. uniform; replacing these with a PRC codeword yields latents that are computationally indistinguishable from true Gaussians for any efficient adversary without the key. Hence, the watermarked image distribution is undetectable under any efficiently computable metric.\n- Robustness comes from (i) semantic-level embedding (all latent coordinates), (ii) soft-decision detection that weights coordinates by posterior reliability derived from an inversion noise model, and (iii) error-correcting redundancy (sparse parity checks) enabling reliable detection/decoding under perturbations.\n- A rigorous FPR control uses a log-likelihood threshold derived from concentration bounds; undetectability leverages PRC pseudorandomness guarantees.\n\n**Mathematical_Formulation:**\n- Generation and recovery (unchanged denoising; added inversion):\n  - Forward sampling: for i = T,…,1, z^{(i−1)} = f_ε(π, z^{(i)}, i), x = D(z^{(0)}).\n  - Watermarked latent: sample y ∼ N(0, I_n), codeword c ← PRC.Encode_k(m), then\n    \\[\n    \\tilde{z}_T = c \\odot |y|,\\quad x = \\text{Generate}(\\pi, \\tilde{z}_T).\n    \\]\n  - Inversion: z′_T = Recover(x) by exact DPM-solver inversion with decoder inversion to refine z′_0: z′_0 ← z′_0 − α ∇_{z_0}\\|D(z_0) − x\\|_2^2, then integrate inversion ODE: z′_{i+1} = g_δ(z′_i, i).\n\n- PRC (LDPC-style, soft decisions). Let the key contain:\n  - Parity-check matrix P ∈ {0,1}^{r×n} with each row w t-sparse; generator G; one-time pad otp ∈ {0,1}^n; testbits ∈ {0,1}^{B} with B = ⌈log_2(1/F)⌉; noise flip rate η = 1 − 2^{-1/λ}.\n  - Encoding (zero- or multi-bit): sample r ∈ F_2^λ, y = (testbits, r, m), e ∼ Ber(η)^n, codeword over F_2:\n    \\[\n    c_{\\text{bin}} = Gy \\oplus e \\oplus \\text{otp},\\quad c = (-1)^{c_{\\text{bin}}} \\in \\{-1,1\\}^n.\n    \\]\n\n- Soft posterior for sign bits under inversion noise model z′ = z + ϵ, ϵ ∼ N(0, \\sigma^2 I):\n  \\[\n  s_i = (1 - 2\\eta)\\,\\mathrm{erf}\\!\\left(\\frac{z'_i}{\\sqrt{2\\sigma^2(1+\\sigma^2)}}\\right),\\quad\n  \\tilde{s}_i = (-1)^{\\mathrm{otp}_i}\\, s_i.\n  \\]\n  For a parity check w (index set), define\n  \\[\n  \\hat{s}_w = \\prod_{i \\in w} \\tilde{s}_i,\\quad C = \\frac{1}{2}\\sum_{w} \\log^2\\!\\left(\\frac{1+\\hat{s}_w}{1-\\hat{s}_w}\\right).\n  \\]\n\n- Detection statistic and threshold with FPR ≤ F:\n  \\[\n  S = \\sum_{w} \\log\\!\\left(\\frac{1 + \\hat{s}_w}{2}\\right),\\qquad\n  \\tau_F = \\sqrt{C \\log(1/F)} + \\frac{1}{2}\\sum_{w} \\log\\!\\left(\\frac{1 - \\hat{s}_w^{2}}{4}\\right),\n  \\]\n  Detect “watermarked” iff S ≥ τ_F. The bound follows from a one-sided sub-Gaussian martingale inequality applied to random parity signs induced by the unknown one-time pad.\n\n- Undetectability (informal statement following PRC security):\n  \\[\n  \\big|\\Pr\\big[A^{\\text{PRCWat.Sample}_k}(1^n)=1\\big] - \\Pr\\big[A^{\\text{Sample}}(1^n)=1\\big]\\big| \\le O(n^{-c})\n  \\]\n  for any poly-time A, assuming PRC pseudorandomness; Generate is efficient so indistinguishability transfers to images.\n\n- Complexity summary:\n  - PRC encode/detect over t-sparse checks costs O(rt) multiplications; recovering z′_T costs O(T·C_step) (same order as one generation run); belief propagation decoding costs O(rt + I·deg·n) where I is iterations.\n\n**Example Format:**\n- Watermarked latent: \\( \\tilde{z}_T = c \\odot |y|,\\; y \\sim \\mathcal{N}(0,I_n),\\; c \\in \\{-1,1\\}^n \\).\n- Soft reliability: \\( s_i = (1-2\\eta)\\,\\mathrm{erf}\\!\\big(z'_i/\\sqrt{2\\sigma^2(1+\\sigma^2)}\\big) \\).\n- Check aggregation: \\( \\hat{s}_w = \\prod_{i \\in w} (-1)^{\\mathrm{otp}_i} s_i \\).\n- Decision rule: \\( S = \\sum_w \\log\\big((1+\\hat{s}_w)/2\\big) \\ge \\tau_F \\).\n- Complexity: PRC operations O(rt) vs. unchanged diffusion sampling O(T·C_step); detection dominated by inversion O(T·C_step).\n\n**Computational_Properties:**\n- Time Complexity:\n  - Watermarked sampling: O(n) for codeword and elementwise ops + O(T·C_step) generation; negligible overhead vs. standard sampling.\n  - Detection: O(T·C_step) for inversion + O(rt) for soft-check aggregation; typically r ≈ n − k − λ with t ≪ n (e.g., t ∈ {3,…,7}).\n  - Decoding (optional): belief propagation + OSD adds O(rt + I·deg·n); I is small (e.g., ≤ log_t n).\n- Space Complexity:\n  - Store z, z′, and PRC metadata. P can be stored in CSR/COO sparse format: O(rt) memory; codeword/otp O(n) bits. Working buffers O(n).\n- Parallelization:\n  - Generation/inversion fully GPU-parallel across diffusion steps and batch prompts.\n  - PRC ops are embarrassingly parallel: per-coordinate s_i, per-check reductions (length t) parallelizable; batched images exploit thread blocks over checks and coordinates.\n  - Decoding BP runs on GPU with sparse gather/scatter; small t enables efficient warp-level reductions.\n- Hardware Compatibility:\n  - GPU: main cost is inversion (same kernels as sampling). PRC reductions are memory-bandwidth light and compute-light; fit in shared memory for per-block check reductions.\n  - CPU: PRC operations vectorize with SIMD; inversion is slower but feasible for offline detection.\n  - Use float32 for inversion; compute log-likelihoods in float64 or stabilized log-domain with clamping |s_i| ≤ 1 − ε (ε ≈ 1e−6) to avoid log singularities.\n- Training vs. Inference:\n  - No training or finetuning required. Inference-time only changes (sampling and detection). Detector hyperparameter σ can be set per deployment scenario.\n- Parameter Count:\n  - No change to model parameters. Key size dominated by (otp, P, testbits, G) ≈ O(n + rt + B + (n−r)·λ) bits; typical t small.\n- Numerical Stability:\n  - Stability of s_i via erf prevents saturation when |z′_i| large; clamp s_i to [−1+ε, 1−ε].\n  - Compute ∑ log terms in double precision; avoid direct products by using log-domain accumulations.\n  - Decoder uses soft inputs; cap LLR magnitudes to prevent BP oscillations; early stopping after max_bp_iter.\n- Scaling Behavior:\n  - Increasing t strengthens cryptographic undetectability (harder parity recovery) but reduces robustness (more bits per check multiply).\n  - Larger n (latent dimensionality) increases redundancy and detection power at fixed FPR.\n  - Longer message length k reduces r (for fixed n), slightly weakening robustness; detector remains strong as it ignores message bits and aggregates over all checks.\n  - Smaller target FPR F increases B = ⌈log_2(1/F)⌉ and detection threshold τ_F mildly; computational cost unchanged.\n- Implementation-critical details / pitfalls:\n  - Accurate Recover is pivotal; use exact DPM-solver inversion plus decoder inversion refinement.\n  - Set σ to approximate inversion error std; modest mis-specification degrades TPR gracefully due to soft weighting.\n  - For JPEG-heavy or regeneration attacks, prefer detection (more robust) over decoding; use t in {3,4} for robustness; use larger t (e.g., 7) when undetectability is paramount.\n  - For message decoding, use BP-OSD with small max_bp_iter ≈ ⌊log_t n⌋; interleave syndrome checks; verify testbits to bound FPR ≤ F.",
        "IMPLEMENTATION_GUIDANCE": "Integration_Strategy:\n- Where to integrate in latent diffusion (Stable Diffusion 2.1 or similar):\n  - Sampling: replace the initial latent sampling z_T ~ N(0, I) with PRC-conditioned sampling that sets signs from a PRC codeword and magnitudes from |N(0,1)|. Insert at the point where latents are prepared before the denoising loop.\n    - HuggingFace diffusers: override or wrap pipeline.prepare_latents(...) or the call that assigns latents = torch.randn_like(latents). Replace with z = torch.randn_like(latents); sgn = prc_encode(key, message_bits) mapped to {-1,+1}; latents = sgn * z.abs().\n  - Detection: add an image-to-latent inversion stage and a PRC soft-decision detector:\n    - Encode with the VAE encoder to get z_0 ≈ E(x); run decoder inversion refinement (gradient descent on z_0 to minimize ||D(z_0) − x||) and then exact DPM-solver inversion to obtain z_T_rev using Hong et al. (2023).\n    - Convert z_T_rev to soft signs s_i via s_i = erf(z_i / sqrt(2σ^2(1+σ^2))). Run PRC.Detect with soft inputs.\n- Code-level steps:\n  - Key generation module (Python):\n    - Implement PRC.KeyGen(n, message_len, F, t) returning a dict with:\n      - Sparse parity-check set P (r rows, t non-zeros each), generator G, one-time-pad otp ∈ {0,1}^n, testbits ∈ {0,1}^ceil(log2(1/F)), λ, η.\n    - Use bit-packed and sparse representations (CSR/COO) for P. Store otp as uint8 bitset.\n  - Sampling function:\n    - prcwat_sample(key, prompt, message=None):\n      - c = PRC.Encode(key, message) in {-1,+1}^n\n      - y ~ N(0, I_n) (torch.randn on GPU)\n      - z_T = c * |y|\n      - Run Generate(prompt, z_T) with your existing sampler (e.g., DPM-Solver, 50 steps, CFG=3.0).\n  - Detector:\n    - recover_latent(x):\n      - z0 = E(x)\n      - decoder inversion: z0 ← z0 − α ∇_{z0} ||D(z0) − x||^2 for K steps (e.g., α=0.05, K=50)\n      - exact inversion: apply Hong et al. exact inversion with same schedule used in sampling to get z_T_rev\n    - soft scores: s_i = erf(z_T_rev[i] / sqrt(2σ^2(1+σ^2)))\n    - PRC.Detect(key, s) computes log-likelihood over parity checks with otp and thresholding\n    - Optional PRC.Decode(key, s) runs belief propagation + OSD to recover message bits\n- Framework compatibility:\n  - PyTorch: primary target; works with diffusers, custom UNet/autoencoder. Use torch.sparse for P or keep on CPU and compute products in batches.\n  - TensorFlow/JAX: port PRC encode/detect to numpy/jax.numpy; the integration point is still the latent sampling and the inversion routine. Exact inversion reference is PyTorch; TensorFlow/JAX require re-implementation of Hong et al. steps or using DDIM inversion.\n- Migration path:\n  - Start zero-bit mode (message_len=0) to validate quality and undetectability, then enable message embedding (e.g., 512 bits).\n  - Keep your existing schedulers, UNet, and VAE unchanged. Only swap latent sampling and add a detector service.\n- Dependencies:\n  - galois (finite field ops) or a minimal bitwise F2 linear algebra helper; ldpc for BP/OSD is convenient for decoding.\n  - Exact inversion implementation (Hong et al. 2023) or equivalent; DPM-Solver schedule parity with generation is required.\n- Training pipeline:\n  - No retraining required. This is an in-processing watermark.\n  - For robustness validation, add a test harness running standard attacks (JPEG, blur, noise, regen, PGD) and compute TPR@FPR.\n\nParameter_Settings:\n- Key generation and code parameters (for SD 2.1 at 512×512, latent shape 4×64×64 → n=16384):\n  - t (parity sparsity):\n    - Robust default: t=3\n    - Higher undetectability (lower robustness): t=4–7 (e.g., t=7 aligns with log2(n)/2 for n≈16384)\n    - Avoid t=2 in undetectability-critical deployments.\n  - F (target false positive rate bound): 1e−2 to 1e−6 common\n    - Platforms with public detection keys: 1e−4 to 1e−6\n    - Internal moderation filters: 1e−2 to 1e−3\n  - message_len:\n    - 0 (zero-bit detect-only) for maximal robustness\n    - 64–512 bits for robust decoding in the presence of attacks\n    - Up to ≈2500 bits if images are not attacked\n  - Derived parameters per paper:\n    - λ = floor(log2(n/t)); for n=16384, t=3 → λ=12\n    - η = 1 − 2^(−1/λ); for λ=12 → η≈0.056\n    - num_test_bits = ceil(log2(1/F)); e.g., F=0.01 → 7\n    - r = n − k − λ where k = message_len + λ + num_test_bits\n- Inversion and detector parameters:\n  - Sampler: DPM-Solver (order 2/3) with 50 steps; CFG scale 3.0 (as used in experiments)\n  - Exact inversion (Hong et al. 2023):\n    - inversion_steps: 50\n    - inverse_order: 0 (fast and accurate per paper)\n    - decoder inversion GD: steps K=30–100; step size α=0.02–0.1\n  - σ (error stdev for soft decisions):\n    - Default robust value: σ = sqrt(3/2) ≈ 1.225\n    - If your inversion is very accurate (no attacks), σ ∈ [0.2, 0.6] improves detect/decoding confidence\n    - Tune σ by maximizing separation between watermarked and unwatermarked log-likelihood on a validation set\n- Belief propagation and OSD (for decoding messages):\n  - max_bp_iter = floor(log_t n); for n=16384, t=3 → ≈8–9\n  - OSD order: 2–3 for speed; increase to 4–5 for tougher attacks at higher runtime\n- Hardware-dependent guidance:\n  - H100/A100: 50 inversion steps in FP16/BF16; batch=1–4 per GPU for 512×512\n  - L4/T4/A10: reduce inversion_steps to 30–40 for throughput; expect lower TPR—compensate with slightly higher σ\n- Storage and computation:\n  - Keep P sparse: memory O(n t); for n=16384, t=3 → ~49k nonzeros; fits comfortably in CPU RAM; computing detector score is O(n t)\n- Critical vs robust parameters:\n  - Critical: inversion schedule parity with sampler, σ, t, F, message_len\n  - Robust: BP iterations, OSD order, decoder inversion step size within given ranges\n- Initialization:\n  - otp and testbits are sampled uniformly at keygen; persist securely\n  - Generator/parity matrices initialized as in Algorithm 1 with random permutation Π; store as sparse (CSR/COO)\n\nApplication_Conditions:\n- Beneficial scenarios:\n  - Latent diffusion models where you control sampling (server-side or SDK) and can add inversion for detection\n  - Quality-sensitive deployments: need to preserve FID/CLIP/IS and within-prompt variability (LPIPS) across batches\n  - Robustness to commodity removal attacks without heavy image degradation\n  - Optional large-capacity attribution (64–512 bits robust; 2500 bits if images are not attacked)\n- Hardware requirements:\n  - Generation: unchanged from baseline model\n  - Detection: roughly 0.8–1.5× the compute of a single generation pass per image, depending on inversion steps and hardware; 16 GB VRAM class GPU recommended for real-time detection; CPU feasible but slower\n- Scale considerations:\n  - Works well for n≈1e4–1e5 latent dims; robustness improves with larger n for fixed t\n  - For small n (<4k), consider increasing t modestly or reducing message_len to maintain robustness\n- Task compatibility:\n  - Text-to-image latent diffusion models (e.g., SD 2.1); extensions possible to VAEs and flow models if a reliable Recover exists\n  - Neutral to harmful for post-processing-only pipelines where you cannot modify sampling or cannot run inversion\n- Alternative comparisons:\n  - Prefer PRC over Tree-Ring/Gaussian Shading when quality and undetectability across many samples are required\n  - If only post-processing is possible and no inversion is available, PRC is not applicable; consider robust post-processing with known trade-offs\n- Resource constraints:\n  - If detection latency is critical and GPUs are scarce, lower inversion_steps to 30–40 and raise σ slightly; monitor TPR@FPR drift\n  - For on-device generators without detection capability, embed only; central platforms can perform detection\n\nExpected_Outcomes:\n- Quality:\n  - No measurable degradation in FID, CLIP Score, Inception Score compared to unwatermarked baseline under the same prompts and seeds; results in the paper show scores within 3 standard errors of baseline across datasets\n  - Intra-prompt variability preserved: LPIPS comparable to baseline; no mode-collapse-like visual patterns\n- Detectability:\n  - Neural classifiers without key fail to generalize beyond training (≈50% validation accuracy), even with multiple keys; other schemes are learnable\n- Robustness:\n  - Detector TPR@FPR=0.01 ≈ 1.0 on clean images with t=3\n  - Under JPEG compression Q≈20, TPR remains ≈0.9–0.95 but images show noticeable blur; most attacks cannot reduce TPR below 0.99 without FID exceeding ≈70 (significant quality loss)\n  - Message decoding:\n    - 512-bit messages: decoder slightly less robust than detector; still high TPR in common degradations\n    - Up to ≈2500 bits decodable reliably when no attacks are applied\n- Trade-offs:\n  - Increasing t increases undetectability margin but reduces robustness to attacks and inversion errors\n  - Increasing message_len decreases robustness; use shorter messages for attack-resilient attribution\n  - Lowering inversion steps reduces compute but harms detection; compensate with tuned σ\n- Failure modes and diagnostics:\n  - Detector FPR higher than target F:\n    - Check testbits handling; ensure num_test_bits=ceil(log2(1/F)) and consistent otp usage\n    - Verify threshold computation and parity-check independence; ensure random permutation Π is applied consistently\n  - Low TPR on clean images:\n    - Mismatch between sampling scheduler and inversion scheduler; ensure exact parity of timesteps and noise schedule\n    - σ set too large/small; grid search σ ∈ [0.5, 1.6]\n    - Insufficient inversion steps or missing decoder inversion refinement\n  - Decoding fails while detection succeeds:\n    - Increase BP iterations or OSD order; reduce message_len; ensure soft inputs are properly scaled\n  - Adversarial embedding attack unusually effective:\n    - Confirm your VAE weights are not public; consider adversarially robust VAE encoder or keep the exact VAE private\n- Hardware-specific expectations:\n  - H100/A100: near-baseline generation throughput; detection adds a small overhead per image; run inversion in BF16/FP16\n  - L4/T4/A10: expect proportionally higher detection latency; batch detection if memory allows; prefer CPU for PRC detect step to free GPU\n- Validation procedures:\n  - Quality: compute FID/CLIP/IS over 5-folds; scores should match baseline within 3×SE\n  - Undetectability: train ResNet18 on 7.5k watermarked vs 7.5k unwatermarked; validation accuracy should remain near 50%\n  - Robustness: run standardized attacks (JPEG 10–60, Gaussian blur radius 2–12, noise σ 5–30, regen with 10–200 steps, VAE recompression factors 1–6, PGD ε 4–12); plot TPR@FPR vs FID/PSNR/SSIM\n  - False positives: evaluate random unwatermarked images; empirical FPR should be ≤ configured F (e.g., 0.01)\n\nTroubleshooting and Operational Guidance:\n- Key management:\n  - Keep detection key private; rotate keys periodically; store P, G, otp, testbits in a KMS; version keys for auditability\n  - Distribute detection only to trusted partners if minimizing competitive disadvantages is a goal\n- Scheduler parity:\n  - Record schedule metadata with each image (non-sensitive hints): model version, solver, steps; ensures correct inversion selection\n- Data pipeline details:\n  - Use consistent latent layout (NCHW) and flatten to n = C×H×W for PRC ops; restore shape before denoising\n  - Compute PRC encode/detect on CPU if GPU is saturated; it is memory-light and vectorizable\n- Security notes:\n  - Do not deploy t=2 for undetectability-critical contexts; prefer t≥4, ideally ≥7 if you can tolerate lower robustness\n  - Consider public-verifiable attribution schemes built atop PRCs for anti-spoofing in public-key settings if required later"
    }
]