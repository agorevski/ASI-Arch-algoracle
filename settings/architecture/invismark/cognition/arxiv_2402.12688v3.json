[
    {
        "DESIGN_INSIGHT": "### DESIGN_INSIGHT_HIGH: [Partial Instruction-driven Denoising Sampling Guidance (PIDSG) – Making Diffusion Editing Partially Differentiable for Semantic-Robust Watermarking]\n- Replaces the standard “noise layer” used in deep watermarking (e.g., differentiable JPEG) with a diffusion-based instruction-driven editing layer and modifies backpropagation so gradients flow through only the last k denoising steps. This turns an otherwise non-differentiable sampling pipeline into a trainable component that targets semantic distortions rather than pixel noise.\n- Mechanism: Given a watermarked image latent \\(Z_{\\mathrm{wm}}=E(I_{\\mathrm{wm}})\\), a pure-noise latent \\(Z_T\\), and instruction embedding \\(\\phi(\\mathrm{Ins})\\) from a frozen CLIP text encoder, the model runs a T-step denoising schedule with U-Net (frozen). Let \\(S_{a\\rightarrow b}(\\cdot)\\) denote the denoising map from step a to b. PIDSG computes\n  \\[\n  Z_k = \\mathrm{stopgrad}\\left(S_{T\\rightarrow k}(Z_T, Z_{\\mathrm{wm}}, \\phi(\\mathrm{Ins}))\\right), \\quad \n  Z_0 = S_{k\\rightarrow 0}(Z_k, Z_{\\mathrm{wm}}, \\phi(\\mathrm{Ins})),\n  \\]\n  where gradients are truncated in the first \\(T-k\\) steps and allowed only through the last \\(k\\) “gradient-backward” steps. The edited image is \\(I^{\\mathrm{edit}}_{\\mathrm{wm}}=D(Z_0)\\).\n- Fundamental difference from prior work: Prior robust watermarking simulates pixel-level corruptions (JPEG, blur, noise) with differentiable operators. PIDSG injects diverse text instructions and leverages the generative denoising dynamics of diffusion models to expose semantic-level shifts during training. This forces the encoder/decoder to place and read the watermark from concept-aware regions that survive semantic edits.\n- Complexity/efficiency: Memory cost scales with the unrolled steps \\(k\\) instead of the total sampling steps \\(T\\) (i.e., \\(O(k)\\) backprop graph vs. \\(O(T)\\)), enabling practical end-to-end training with diffusion editing in the loop while preserving robustness across samplers and guidance scales.\n\n### DESIGN_INSIGHT_MEDIUM: [Dual-Stage Watermark Supervision Across Pre- and Post-Editing (Lex1/Lex2) – Stabilizing Semantic-Robust Extraction]\n- Modifies the typical encoder–noise–decoder training by adding a second extraction path that supervises the decoder on both the unedited watermarked image and the post-edited image, replacing single-path supervision used in prior robust watermarking.\n- Mechanism: With embedding network \\(E_m\\) and extractor \\(E_x\\), messages are embedded as \\(I_{\\mathrm{wm}}=E_m(I_{\\mathrm{ori}}, m)\\). The extractor is trained with two objectives:\n  \\[\n  L_{\\mathrm{ex1}} = \\mathrm{MSE}\\!\\left(m, E_x(I^{\\mathrm{edit}}_{\\mathrm{wm}})\\right), \\quad\n  L_{\\mathrm{ex2}} = \\mathrm{MSE}\\!\\left(m, E_x(I_{\\mathrm{wm}})\\right),\n  \\]\n  where \\(I^{\\mathrm{edit}}_{\\mathrm{wm}}\\) is produced via PIDSG. The total objective\n  \\[\n  L_{\\mathrm{total}} = L_{\\mathrm{em1}} + \\lambda_1 L_{\\mathrm{em2}} + \\lambda_2 L_{\\mathrm{ex1}} + \\lambda_3 L_{\\mathrm{ex2}}\n  \\]\n  combines fidelity and extraction (see next insight for \\(L_{\\mathrm{em1}}, L_{\\mathrm{em2}}\\)).\n- Core difference: Prior systems typically supervise only post-distortion extraction, which can fail when distortions are semantic and highly non-local. The added pre-edit supervision \\(L_{\\mathrm{ex2}}\\) anchors the extractor to consistent, learnable watermark supports before exposure to semantic edits, acting as a curriculum that prevents collapse and materially improves convergence under aggressive instruction-driven changes.\n\n### DESIGN_INSIGHT_LOW: [Latent-Manifold-Constrained Embedding (Pixel + VAE-Latent Fidelity) – Preserving Editability While Embedding]\n- Refines the embedding stage by adding a VAE-latent consistency loss aligned with the downstream editor’s latent space, replacing pixel-only fidelity used in many watermarking models. This preserves both visual quality and editability.\n- Mechanism: The embedding network concatenates a learned message tensor with the image and outputs \\(I_{\\mathrm{wm}}=E_m(I_{\\mathrm{ori}}, m)\\). Two fidelity losses are applied:\n  \\[\n  L_{\\mathrm{em1}} = \\|I_{\\mathrm{ori}} - I_{\\mathrm{wm}}\\|_2^2, \\quad \n  L_{\\mathrm{em2}} = \\|E(I_{\\mathrm{ori}}) - E(I_{\\mathrm{wm}})\\|_2^2,\n  \\]\n  where \\(E\\) is the (frozen) VAE encoder used by the instruction-driven editor. By constraining both pixel and latent distances, the watermarked image remains close to the editor’s manifold of the original image.\n- Difference from prior work: While pixel-domain fidelity maintains imperceptibility, aligning in the editor’s latent space ensures the watermarked image retains the same editing affordances under diffusion guidance, a prerequisite for learning semantic-robust watermark placements via PIDSG.",
        "EXPERIMENTAL_TRIGGER_PATTERNS": "Task_Performance_Signatures:\n- Core watermarking/editing robustness\n  - Expect improved performance on instruction-driven image editing robustness versus prior watermarking baselines.\n    - InstructPix2Pix (default config: Euler sampler, 20 steps, sT=10, sI=1.5, 512×512):\n      - 64-bit message: BER ≈ 2.66% (vs. ≈40–51% BER for DWT-DCT, DWT-DCT-SVD, RivaGAN, MBRS, CIN, PIMoG, SepMark after editing).\n      - 256-bit message: BER ≈ 4.19%.\n      - Unedited watermarked images: BER ≈ 0.00%.\n      - Unwatermarked images (forensic integrity): BER ≈ 49–50% with and without editing (near-random), indicating no false positives.\n    - Real-world images + diverse instructions: 64-bit message: BER ≈ 2.61%.\n  - Fidelity and editability remain high:\n    - PSNR ≈ 41.9 dB and SSIM ≈ 0.991 (64-bit).\n    - Editability preserved: CLIP-I ≈ 0.8430 vs. 0.8402 (orig), CLIP-T ≈ 0.2148 vs. 0.2183 (orig) — differences within ~0.003 absolute.\n  - Robustness to other editing methods (zero-shot generalization):\n    - ControlNet-InstructPix2Pix: average BER ≈ 0.96%.\n    - MagicBrush: average BER ≈ 9.34% (degradation under stronger semantic edits but still extractable).\n    - Inpainting, DDIM Inversion (Stable Diffusion based): effective extraction (qualitative; low BER in examples).\n  - Robustness to pixel-level distortions not seen in training:\n    - Pre-process before editing (I): JPEG, blur, noise, color-shifts: typical BER ≈ 2.5–3.1%; worst cases — brightness ≈ 12.29%, noise+denoise ≈ 8.64%.\n    - Post-process watermarked image only (II): near-zero BER (0–0.92%); noise+denoise ≈ 3.91%.\n    - Post-process edited image (III): JPEG/blur ≈ 2.8–3.0%; Gaussian noise ≈ 6.05%; brightness ≈ 9.48%; noise+denoise ≈ 9.37%.\n  - Robustness across sampling configurations:\n    - Samplers (DDPM, DDIM, DPM-Solver, Euler): BER typically ≤4–5%.\n    - Number of inference steps (20→100): edited images become more detailed; BER increases slightly within ≈1% absolute.\n    - Guidance scales: higher text guidance (sT up to 15) and lower image guidance (sI near 1.0) increase edit strength; BER remains <5% except at sI=1.0 where degradation is noticeable.\n  - Continual editing:\n    - Accurate extraction through ≈3 sequential edit rounds (BER remains low); performance degrades progressively with more rounds, trending toward ≈40–50% by ~6 rounds (graceful failure under extreme cumulative edits).\n  - Scaling with watermark length (trade-off):\n    - 16 bits: BER ≈ 2.28%, PSNR ≈ 40.83 dB.\n    - 64 bits: BER ≈ 2.66%, PSNR ≈ 41.91 dB.\n    - 256 bits: BER ≈ 4.19%, PSNR ≈ 39.18 dB.\n    - 1024 bits: BER ≈ 6.30%, PSNR ≈ 36.60 dB.\n- Ablations that signal success/failure\n  - With PIDSG: BER ≈ 2.66% (64-bit). Without PIDSG: BER ≈ 50.16% (fails under editing).\n  - With Lex2 term: BER ≈ 0% (unedited) and ≈2.66% (edited). Without Lex2: ≈50% BER both before and after editing (decoder fails to localize watermark).\n  - Gradient backward steps k in PIDSG:\n    - k=1: BER ≈ 4.05%; k=2: ≈ 2.92%; k=3: ≈ 2.66% (best), with small PSNR decrease as k increases.\n- Expected efficiency/overhead patterns\n  - Training feasible on a single A6000, batch size 2, 20k steps; differentiating only the last k (≤3) denoising steps avoids OOM while preserving robustness.\n  - Inference cost for extraction is a single forward pass through a lightweight residual-block decoder; watermark embedding is a single U-Net pass; editing pipeline cost unchanged vs. base editor.\n- Non-target NLP/QA benchmarks (for completeness with named tasks)\n  - Language modeling (lambada_openai, wikitext, ptb), reading comprehension (squad_completion, squad_v2, narrativeqa), commonsense (hellaswag, piqa, social_iqa, commonsenseqa), factual QA (arc_easy, arc_challenge, boolq, openbookqa), context resolution (winogrande, winograd), other (swde, fda):\n    - Expect unchanged performance (not applicable). Any observed differences should be within measurement noise (≤0.1% absolute), as the method operates in vision watermarking and does not modify language model behavior.\n\nArchitectural_Symptoms:\n- Training characteristics\n  - With PIDSG enabled (gradients through last k denoising steps), total loss converges smoothly within ~20k steps; Lex1/Lex2 message MSE decline steadily; no NaNs observed.\n  - Without Lex2, training appears stable in pixel/feature reconstruction (very high PSNR up to ≈55–69 dB), but message extraction MSE fails to decrease (BER ≈50%) — a clear failure signature.\n  - Increasing k from 1→3 yields monotonic BER improvements and slight PSNR reductions — indicates effective gradient flow through partial denoising.\n  - Robust to moderate λ2 increases (message loss weight) with improved BER; overly large λ1 (feature-space fidelity weight ~0.1) yields near-random extraction (≈50% BER) — fidelity overpowering robustness is a negative symptom.\n- Runtime/memory behaviors\n  - Memory footprint during training scales with k, not with total sampling steps T; truncating gradients for first T−k steps prevents OOM at 512×512 with batch size 2 on A6000.\n  - GPU utilization peaks during the last k denoising steps (U-Net) and VAE encode/decode passes; CLIP text encoder contributes minimal overhead per iteration.\n  - Inference memory/time for extraction is roughly constant w.r.t. image content and instruction; comparable to a small CNN forward pass.\n- Profiling signatures and qualitative diagnostics\n  - Residual visualization N(|Iwm−Iori|) shows energy concentrated along semantic contours and secondary background structures; frequency-domain (DCT/DFT) energy biased toward infra-low to mid-low frequencies. Absence of these patterns indicates suboptimal embedding.\n  - Forensic integrity check: running the extractor on non-watermarked images yields BER ≈49–50% (random). Any systematic deviation toward low BER indicates leakage/overfitting.\n  - Robustness smoke tests:\n    - Pixel distortions (JPEG q∈[40,100], median/Gaussian blur, Gaussian noise σ small, brightness/contrast changes): post-process-watermarked (II) BER ≈0–1%; post-process-edited (III) BER typically ≤3–6%, with harder cases (brightness/noise+denoise) ≤9–10%.\n    - Sampler swaps (DDPM/DDIM/DPM-Solver/Euler), step counts (20–100): BER stable within ~1% absolute drift. Large degradations suggest guidance scale misconfiguration or broken gradient routing.\n    - Guidance sweeps: sT↑ or sI↓ should gently increase BER; catastrophic jumps near sI=1.0 flag extremely strong edits.\n  - Continual editing: BER should remain low for ≤3 rounds; rapid early escalation suggests insufficient semantic anchoring (tune k, λ2).\n  - Security/attack resilience: Noise+denoise (SDEdit-like) should yield BER ≈4–9% depending on when applied. Near-random BER here indicates failure to maintain semantic-anchored embedding.\n  - Convergence time: with default LR 1e−3, cosine schedule (400 warmup), batch size 2, expect stable convergence by ~15–20k steps; if not, verify frozen editor weights and gradient truncation boundaries.",
        "BACKGROUND": "**Title:** Robust-Wide: Robust Watermarking against Instruction-driven Image Editing\n\n**Historical Technical Context:**\nText-to-Image (T2I) generation has progressed from GAN-based and autoregressive CNN/Transformer systems to diffusion models (DMs), notably DDPMs and Latent Diffusion Models (LDMs). DDPMs learn a reverse denoising process to synthesize data, while LDMs move the computation to a VAE latent space to reduce training and inference cost. Instruction-driven image editing emerged by conditioning diffusion models on the input image and a natural-language instruction. InstructPix2Pix, trained on GPT-3- and Prompt2Prompt-generated triplets, established a fast, single-forward-pass editing paradigm. Subsequent systems—ControlNet-InstructPix2Pix (adding spatial controls), HIVE (human feedback alignment), MagicBrush (manual high-quality editing data), and MGIE (MLLM-guided instructions)—improved controllability, alignment, and quality. Related editing tools such as inpainting and DDIM/Null-text inversion provide other text-driven edit routes.\n\nRobust watermarking progressed from classical transform-domain methods (DWT-DCT, DWT-DCT-SVD) to deep encoder–noise–decoder frameworks (HiDDeN, RivaGAN, MBRS, CIN), with later work addressing physical-world robustness (StegaStamp, RIHOOP, PIMoG, LFM). These systems primarily simulate pixel-level distortions (e.g., JPEG, blur, color shifts) via differentiable or mixed noise layers. However, their robustness degrades under semantic edits produced by instruction-driven diffusion, which can significantly alter scene layout, style, and object identity—well beyond local, low-level perturbations.\n\nThis paper sits at the intersection of robust watermarking and instruction-driven diffusion editing. It adopts the deep encoder–noise–decoder paradigm but introduces a differentiable approximation to the denoising sampling process and injects diverse linguistic instructions during training to target semantic-level distortions.\n\n**Technical Limitations:**\n- Semantic robustness gap: Prior watermarking methods model pixel-level corruption (compression, blur, resampling) but fail under instruction-driven edits that induce high-level semantic changes (style transfer, object/attribute replacement), leading to ≈50% Bit Error Rate (BER), i.e., near random guessing.\n- Non-differentiable sampling bottleneck: The diffusion denoising sampling procedure is inherently non-differentiable across T steps. Backpropagating through all steps would require storing activations for each U-Net pass, incurring O(T) memory scaling and making end-to-end training impractical for 512×512 images.\n- Memory/compute scaling: A full-gradient path through T denoising steps scales time roughly O(T) U-Net evaluations and memory O(T·A), where A is activation footprint per step. With T≈20–50 and high-resolution latents, VRAM requirements exceed typical single-GPU budgets.\n- Training dynamics instability: Training only with edited images deprives the decoder of stable localization cues, causing extractor collapse (failure to discover watermark regions) and non-convergence; empirically BER≈50% without a pre-edit extraction constraint.\n- Generalization to sampling configurations: Watermarking robustness can depend on sampler choice, inference steps, and guidance scales. Prior methods are sensitive to these parameters; robustness must hold across samplers (DDPM, DDIM, DPM-Solver, Euler) and varying text/image guidance.\n- Editability/fidelity trade-off: Robust embeddings risk degrading visual quality or impairing editability. Prior methods may imprint conspicuous artifacts or alter edit behavior; maintaining high PSNR/SSIM and CLIP-based editability is non-trivial.\n\n**Paper Concepts:**\n- **Denoising Diffusion Probabilistic Model (DDPM):** A generative model that minimizes\n  \\[\n  \\mathcal{L}_{\\text{DM}}=\\mathbb{E}_{x,\\epsilon\\sim\\mathcal{N}(0,1),t}\\left[\\lVert \\epsilon-\\epsilon_\\theta(x_t,t)\\rVert_2^2\\right],\n  \\]\n  where \\(x_t\\) is a noised version of image \\(x\\) at timestep \\(t\\). The reverse process iteratively denoises \\(x_T\\sim\\mathcal{N}(0,I)\\) to \\(x_0\\). Intuitively, the model learns to remove Gaussian noise step-by-step to reconstruct data.\n- **Latent Diffusion Model (LDM):** Diffusion performed in VAE latent space \\(z=E(x)\\), optimizing\n  \\[\n  \\mathcal{L}_{\\text{LDM}}=\\mathbb{E}_{E(x),\\epsilon\\sim\\mathcal{N}(0,1),t}\\left[\\lVert \\epsilon-\\epsilon_\\theta(z_t,t)\\rVert_2^2\\right].\n  \\]\n  Operating in latent space reduces compute and memory versus pixel-space diffusion while maintaining visual fidelity after decoding with \\(D(z)\\).\n- **Instruction-driven Image Editing (InstructPix2Pix-style conditioning):** Conditioning on image features \\(E(c_I)\\) and a text instruction \\(c_T\\) via a CLIP text encoder, optimizing\n  \\[\n  \\mathcal{L}=\\mathbb{E}\\left[\\lVert \\epsilon-\\epsilon_\\theta(z_t,t,E(c_I),c_T)\\rVert_2^2\\right].\n  \\]\n  The model outputs an edited image \\(D(z_0)\\) in a single forward denoising trajectory guided by the instruction. Intuitively, the instruction steers edits ranging from style changes to object/attribute transformations.\n- **Partial Instruction-driven Denoising Sampling Guidance (PIDSG):** A training-time module that makes the otherwise non-differentiable sampling partially differentiable by allowing gradients only through the last \\(k\\) denoising steps. Let \\(Z_{\\text{wm}}=E(I_{\\text{wm}})\\) be the latent of the watermarked image, and \\(Z_T\\) pure noise. The pipeline concatenates \\(Z_{\\text{wm}}\\) and \\(Z_T\\), runs \\(T-k\\) steps with gradient detached to obtain \\(Z_k\\), then runs the final \\(k\\) “gradient-backward” steps conditioned on the instruction embedding to produce \\(Z_0\\). This reduces memory from O(T) to O(k) steps of activations while exposing the encoder to semantically guided distortions, encouraging watermark placement in concept-aware, robust regions.\n- **Watermark Embedding/Extraction Networks (Em, Ex):** The encoder \\(E_m\\) (U-Net) embeds an L-bit message \\(m\\in\\{0,1\\}^L\\) into image \\(I_{\\text{ori}}\\) to produce \\(I_{\\text{wm}}=E_m(I_{\\text{ori}},m)\\). The extractor \\(E_x\\) (residual-block CNN) predicts \\(\\hat{m}=E_x(I)\\) from either watermarked or edited images. Losses:\n  - Pixel fidelity: \\(\\mathcal{L}_{\\text{em1}}=\\lVert I_{\\text{ori}}-I_{\\text{wm}}\\rVert_2^2\\).\n  - Latent fidelity (VAE encoder \\(E\\)): \\(\\mathcal{L}_{\\text{em2}}=\\lVert E(I_{\\text{ori}})-E(I_{\\text{wm}})\\rVert_2^2\\).\n  - Post-edit extraction: \\(\\mathcal{L}_{\\text{ex1}}=\\text{MSE}(m,E_x(I^{\\text{edit}}_{\\text{wm}}))\\).\n  - Pre-edit extraction: \\(\\mathcal{L}_{\\text{ex2}}=\\text{MSE}(m,E_x(I_{\\text{wm}}))\\).\n  Total loss: \\(\\mathcal{L}_{\\text{total}}=\\mathcal{L}_{\\text{em1}}+\\lambda_1\\mathcal{L}_{\\text{em2}}+\\lambda_2\\mathcal{L}_{\\text{ex1}}+\\lambda_3\\mathcal{L}_{\\text{ex2}}\\).\n  Intuitively, the extractor learns watermark localization and decoding both before and after semantic edits; \\(\\mathcal{L}_{\\text{ex2}}\\) stabilizes training.\n- **Bit Error Rate (BER):** The fraction of incorrect bits in the extracted watermark,\n  \\[\n  \\text{BER}(X,Y)=\\frac{\\sum_{i=1}^{L}\\mathbb{1}[X_i\\neq Y_i]}{L},\\quad X,Y\\in\\{0,1\\}^L.\n  \\]\n  BER≈50% corresponds to chance performance for balanced bits; lower is better. It quantifies robustness under edits and distortions.\n\n**Experimental Context:**\nThe evaluation emphasizes robustness of watermark extraction under semantic edits generated by instruction-driven diffusion, while preserving visual fidelity and editability. Core metrics include BER (primary robustness metric), PSNR and SSIM (watermark imperceptibility), and CLIP-based measures—CLIP-I (image similarity) and CLIP-T (text–image direction similarity)—to gauge whether watermarking preserves the editability behavior of the original image. Robustness to conventional pixel-level distortions (JPEG, blur, noise, color/contrast/brightness shifts, sharpness) and to noise–denoise attacks (SDEdit-style) is also assessed.\n\nTraining uses 20k image–instruction pairs from InstructPix2Pix data; default evaluation uses 1.2k non-overlapping samples plus a curated 30-image real-world set (6 categories × 5 images) edited with 6 instructions each to yield 1.44k edited samples. The default PIDSG setup uses Euler sampling with 20 steps, text guidance scale \\(s_T=10.0\\), image guidance scale \\(s_I=1.5\\). Robust-Wide is compared against DWT-DCT, DWT-DCT-SVD, RivaGAN, MBRS, CIN, PIMoG, and SepMark (with necessary resizing or official constraints). The paper reports that all baselines fail under instruction-driven edits (BER≈49–51%), while Robust-Wide attains BER≈2.66% (64-bit) and ≈4.19% (256-bit) with high fidelity (PSNR≈41.9 dB, SSIM≈0.991). Integrity tests show it does not falsely extract watermarks from unwatermarked images; editability is preserved (CLIP-I and CLIP-T comparable to originals). Extensive stress tests vary samplers (DDPM, DDIM, DPM-Solver, Euler), inference steps, and guidance scales; robustness generalizes with BER typically <5% across settings. Additional tests demonstrate resilience to ControlNet-InstructPix2Pix (≈0.96% BER), MagicBrush (≈9.34% BER), inpainting, DDIM inversion, pixel-level perturbations, and even multi-round (continual) editing, while ablations confirm the necessity of PIDSG and the pre-edit extraction loss \\(\\mathcal{L}_{\\text{ex2}}\\).",
        "ALGORITHMIC_INNOVATION": "Core_Algorithm:\n- Replace the conventional pixel-level “noise layer” in deep watermarking with a Partial Instruction-driven Denoising Sampling Guidance (PIDSG) module that embeds a partially differentiable, instruction-guided diffusion sampling process inside the encoder–noise–decoder training loop.\n- Computational steps:\n  1) Em: U-Net-based embedding network takes an image Iori and an L-bit message m, reshapes m to a feature map, concatenates it with Iori, and outputs Iwm. Pixel- and latent-level L2 penalties preserve fidelity.\n  2) PIDSG: Encode Iwm to latent Zwm via the frozen VAE encoder; start from a pure noise latent ZT; run T−k denoising steps with gradients detached, then run the last k steps with gradients enabled under classifier-free dual guidance using both instruction text and image conditions; decode the final latent Z0 to obtain the edited image Iedit_wm.\n  3) Ex: a residual CNN extracts the message from both Iwm and Iedit_wm; training uses two extraction losses (pre- and post-edit).\n- Fundamental change: watermark robustness is no longer trained against pixel-level distortions (e.g., JPEG) but against the semantic transformations induced by an instruction-driven editor, made differentiable in the last k diffusion steps so the encoder learns to place watermarks in concept-aware, edit-resilient regions.\n- Scope: PIDSG operates per edited sample and per diffusion trajectory; Em and Ex are trainable, while the instruction-driven editor (VAE, U-Net, CLIP) is frozen.\n\nKey_Mechanism:\n- The key insight is to expose the encoder and extractor to the true editing dynamics by differentiating through the final denoising steps of an instruction-conditioned diffusion sampler. This forces watermark energy into semantic structures (object contours, layout, mid-/infra-low frequency content) that persist after instruction-driven edits.\n- Diverse instruction injections and guidance scaling (text vs. image CFG) act as adversarial, semantic-level perturbations, so the learned embedding maximizes persistence across a wide spectrum of edits while preserving image fidelity.\n- By truncating gradients for the first T−k steps, the method remains tractable in memory while retaining sufficient signal for end-to-end optimization toward semantic robustness.\n\nMathematical_Formulation:\n- Embedding and fidelity constraints:\n  - Watermarked image: Iwm = Em(Iori, m)\n  - Pixel-level fidelity: Lem1 = ∥Iori − Iwm∥_2^2\n  - Latent-level fidelity (VAE encoder E): Lem2 = ∥E(Iori) − E(Iwm)∥_2^2\n- PIDSG with partial gradients:\n  - VAE latent of Iwm: Zwm = E(Iwm), pure noise latent ZT ∼ N(0, I)\n  - Let εθ be the frozen editor U-Net; denote unconditional, text-only, and image-only predictions εu, εt, εi, respectively. Dual classifier-free guidance:\n    \\[\n    \\hat{\\epsilon}_\\theta(z_t, t; s_T, s_I) \\;=\\; \\epsilon_u \\;+\\; s_T\\,(\\epsilon_t - \\epsilon_u) \\;+\\; s_I\\,(\\epsilon_i - \\epsilon_u)\n    \\]\n    where s_T, s_I ≥ 0 are the text and image guidance scales.\n  - Generic sampler update (DDIM/Euler-style):\n    \\[\n    z_{t-1} \\;=\\; \\phi\\big(z_t,\\, t,\\, \\hat{\\epsilon}_\\theta(z_t,t; s_T,s_I),\\, Z_{wm}\\big)\n    \\]\n    where φ implements the chosen update rule and concatenation/conditioning on Zwm.\n  - Truncated gradient flow:\n    \\[\n    z_k \\;=\\; \\Phi_{\\perp}(Z_T,\\, Z_{wm},\\, \\mathrm{Ins};\\, T\\!\\to\\!k+1),\\quad\n    z_0 \\;=\\; \\Phi(z_k,\\, Z_{wm},\\, \\mathrm{Ins};\\, k\\!\\to\\!1)\n    \\]\n    where Φ⊥ denotes T−k steps with stop-gradient (⊥) and Φ denotes the last k differentiable steps. Final edited image:\n    \\[\n    I^{edit}_{wm} \\;=\\; D(z_0)\n    \\]\n- Extraction losses (Ex is trainable; D is frozen VAE decoder):\n  \\[\n  L_{ex1} \\;=\\; \\mathrm{MSE}\\big(m,\\, Ex(I^{edit}_{wm})\\big), \\qquad\n  L_{ex2} \\;=\\; \\mathrm{MSE}\\big(m,\\, Ex(I_{wm})\\big)\n  \\]\n- Total objective:\n  \\[\n  L_{total} \\;=\\; L_{em1} \\;+\\; \\lambda_1 L_{em2} \\;+\\; \\lambda_2 L_{ex1} \\;+\\; \\lambda_3 L_{ex2}\n  \\]\n  with typical values λ1=1e−3, λ2=0.1, λ3=1.\n- Complexity:\n  - Let H×W be image size, latent size be (H/8)×(W/8) with Cℓ channels, U denote FLOPs of a single editor U-Net call at latent resolution, and T be diffusion steps. Training per sample:\n    - Forward-only denoising: O((T−k)·U)\n    - Backpropagated denoising: O(k·U)\n    - Em/Ex passes: O(HW·C_e) and O(HW·C_x) respectively\n  - Overall per-iteration time complexity: O(T·U + HW(C_e + C_x))\n  - Memory dominated by k differentiable steps: O(k·A_U + A_Em + A_Ex), where A_U are U-Net activations for one step.\n\nExample Format:\n- Guided noise prediction:\n  \\[\n  \\hat{\\epsilon}_\\theta \\;=\\; \\epsilon_\\theta(z_t,t,\\varnothing) \\;+\\; s_T\\big[\\epsilon_\\theta(z_t,t,c_T)-\\epsilon_\\theta(z_t,t,\\varnothing)\\big] \\;+\\; s_I\\big[\\epsilon_\\theta(z_t,t,c_I)-\\epsilon_\\theta(z_t,t,\\varnothing)\\big]\n  \\]\n- Partially differentiable trajectory:\n  \\[\n  z_k = \\Phi_{\\perp}(Z_T, Z_{wm}, \\mathrm{Ins}),\\quad z_{t-1} = \\phi(z_t, t, \\hat{\\epsilon}_\\theta, Z_{wm}),\\; t=k,\\dots,1\n  \\]\n- Losses:\n  \\[\n  L_{total} = \\|I_{ori}-I_{wm}\\|_2^2 + \\lambda_1 \\|E(I_{ori})-E(I_{wm})\\|_2^2 + \\lambda_2 \\mathrm{MSE}(m, Ex(D(z_0))) + \\lambda_3 \\mathrm{MSE}(m, Ex(I_{wm}))\n  \\]\n- Complexity comparison:\n  - Proposed training: O(T·U) time, O(k·A_U) memory\n  - Standard pixel-noise watermarking: O(1) “noise layer” time, O(1) memory but no semantic robustness.\n\nComputational_Properties:\n- Time Complexity:\n  - Training per sample: O(T·U + HW(C_e + C_x)). Since U dominates, time scales linearly with diffusion steps T. Partial backprop confines gradient costs to O(k·U) while still incurring O((T−k)·U) forward-only cost.\n  - Embedding inference (Em only): O(HW·C_e), no diffusion. Extraction inference (Ex only): O(HW·C_x).\n  - Editing robustness verification requires no editor calls; only Ex is used on the edited image.\n- Space Complexity:\n  - Activations: O(k·A_U + A_Em + A_Ex); first T−k steps are stateless w.r.t. gradients (activations discarded), enabling large T with small k (typically k∈{1,2,3}).\n  - Parameters in training graph are only Em and Ex; editor and CLIP/VAEs are frozen and excluded from optimizer states.\n- Parallelization:\n  - Em/Ex are standard CNNs and fully GPU-parallelizable across batch, channels, and spatial tiles.\n  - The editor denoising steps are sequential in t but intra-step U-Net computation is highly parallel on GPUs and supports tensor/model/data parallelism. Multiple instructions per batch are trivially data-parallel.\n  - Mixed precision (fp16/bf16) is effective because the frozen editor behaves stably; keep loss accumulation for MSE in fp32 for precision.\n- Hardware Compatibility:\n  - Best efficiency on GPUs with high memory bandwidth; partial backprop (small k) lowers activation footprint. Activation checkpointing can further cut memory at small runtime overhead during the k differentiable steps.\n  - CPU execution is possible for Em/Ex, but training with PIDSG is GPU-oriented due to editor U-Net cost.\n- Training vs. Inference:\n  - Training uses the editor forward T times per sample, but only the last k steps retain activations; inference for watermark embedding/extraction does not invoke the editor, yielding fast deployment.\n  - The extraction model Ex can be quantized (e.g., INT8) with negligible BER impact; Em quantization affects visual quality and should be tested case-by-case.\n- Parameter Count:\n  - Trainable parameters are those of Em and Ex only; the diffusion editor (VAE encoder/decoder, U-Net) and CLIP text encoder are frozen. Total trainable size depends on chosen U-Net width/depth for Em and residual depth for Ex; typical configurations are orders of magnitude smaller than the frozen editor.\n- Numerical Stability:\n  - Stability is ensured by L2 pixel/latent penalties and frozen editor weights. Partial unrolling (small k) reduces gradient explosion risk through the sampler.\n  - Classifier-free guidance is numerically stable for moderate s_T, s_I; extreme scales can induce overshooting—mitigate via gradient clipping and limiting s_T, s_I ranges during training.\n- Scaling Behavior:\n  - Training cost scales linearly with T and roughly quadratically with latent spatial size (∝(H/8·W/8)), as in diffusion U-Nets.\n  - Larger k improves gradient signal but increases memory linearly; empirical sweet spot k∈[2,3].\n  - Longer payloads (L) increase embedding difficulty: BER rises and PSNR drops; balance via λ2, λ3 and Em capacity.\n- Implementation-critical details and pitfalls:\n  - Use explicit stop-gradient for steps t=T,…,k+1; an accidental graph retention negates memory gains.\n  - Ensure dual CFG is implemented with three forward calls (uncond, text-cond, image-cond) or equivalent cached heads.\n  - Concatenate Zwm with z_t consistently across all denoising steps (frozen editor expects fixed channel layout).\n  - Lex2 is essential: without Ex(Iwm) supervision, the extractor fails to localize watermark regions (training collapses to ≈50% BER).",
        "IMPLEMENTATION_GUIDANCE": "IMPLEMENTATION_GUIDANCE\n\nIntegration_Strategy:\n- Where to integrate\n  - Add a watermarking stage before publishing images and an extraction stage for forensics.\n  - Embedder Em: a U-Net that takes the original RGB image Iori (3×H×W) and a reshaped L-bit message m, outputs Iwm.\n  - PIDSG module: wrap the InstructPix2Pix denoising loop so only the last k steps are differentiable; freeze VAE, U-Net, and CLIP text encoder.\n  - Extractor Ex: a residual CNN that maps an edited RGB image to an L-dimensional bit vector.\n- Code-level changes (PyTorch + Diffusers recommended)\n  - Start from diffusers StableDiffusionInstructPix2PixPipeline (SD-IP2P). Freeze all SD-IP2P parameters: for p in pipe.unet.parameters(): p.requires_grad=False; same for pipe.vae, pipe.text_encoder, pipe.tokenizer.\n  - Subclass the pipeline to expose a manual denoising loop with partial gradients:\n    - Implement a new method forward_pidsg(image_latents, prompt_embeds, k, scheduler, num_inference_steps, sT, sI).\n    - Set timesteps: scheduler.set_timesteps(num_inference_steps).\n    - Sample z_T ~ N(0,I) with shape of VAE latents; compute zt according to scheduler.\n    - For steps t0..t_{T-k-1}: with torch.no_grad(): latents = unet(latents, t, prompt_embeds, image_embeds, guidance_scales) and scheduler step.\n    - Detach latents explicitly before gradient-enabled segment: latents = latents.detach().requires_grad_(True).\n    - For steps t_{T-k}..t_{T-1}: run unet forward with grad enabled; propagate through to Em.\n    - Decode with VAE decoder to produce Iedit_wm.\n  - Image conditioning (image guidance) in IP2P\n    - Use built-in IP2P conditioning (image_guidance_scale sI). Ensure the original image latent (Zwm) is passed to the image conditioning branch used by SD-IP2P (pipe.image_proj_model if present). If using older SD-IP2P, concatenate image embeddings to text context as per pipeline implementation.\n  - Embedder Em\n    - Implement a U-Net with input channels = 3 + Cmsg. Construct message tensor by reshaping m ∈ {0,1}^L to (1, sqrt(L), sqrt(L)) then upsample to H×W via transposed convolutions to Cmsg channels (recommend Cmsg=8–16).\n    - Concatenate with Iori and predict a residual Δ; set Iwm = clamp(Iori + Δ, [-1,1] if using VAE-normalized space).\n  - Extractor Ex\n    - Residual CNN: 6–8 residual blocks with 3×3 convs, channels 64→128→256 with two downsampling stages (stride-2 convs). Global average pool, MLP to L logits; apply sigmoid for [0,1] outputs.\n  - Losses\n    - Lem1 = L2(Iori, Iwm) on pixel-normalized range (match SD preprocessing).\n    - Lem2 = L2(E(Iori), E(Iwm)) using the frozen VAE encoder E from SD-IP2P to compute latents Zori and Zwm.\n    - Lex1 = MSE(m, Ex(Iedit_wm)), Lex2 = MSE(m, Ex(Iwm)).\n    - Ltotal = Lem1 + λ1·Lem2 + λ2·Lex1 + λ3·Lex2.\n  - Training loop\n    - For each batch: sample random message m, run Em to produce Iwm, run PIDSG(edit with instruction Ins) to get Iedit_wm, compute losses, backprop only through Em and Ex.\n  - Compatibility\n    - PyTorch 2.0+, diffusers >=0.20 recommended; use accelerate for multi-GPU or CPU offload.\n    - TensorFlow/JAX: mirror the partial-grad denoising by splitting the loop; in JAX, stop_gradient for first T−k steps and enable grad for last k.\n  - Migration path\n    - From existing watermarking (e.g., MBRS/CIN): reuse their Em/Ex backbones but insert PIDSG between them and add Lex2; switch objective to Ltotal above; freeze diffusion components.\n  - No custom CUDA kernels required. Optional: xFormers or PyTorch scaled-dot attention for UNet memory savings (affects only inference path which is frozen).\n\nParameter_Settings:\n- Core training\n  - Image size: 512×512 (default). For 768×768, expect +2–2.5× VRAM and latency.\n  - Batch size: 2 on 48 GB A6000; 1 on 24 GB 3090/4090; 4–8 on 80 GB A100.\n  - Optimizer: AdamW, lr=1e-3, betas=(0.9,0.999), weight_decay=0.01, cosine schedule with 400 warmup steps, total steps 20,000.\n  - Mixed precision: bf16 or fp16; enable gradient scaling.\n- PIDSG and sampling\n  - Sampler: Euler (ancestral) recommended; also works with DDPM, DDIM, DPM-Solver.\n  - Num inference steps T: 20 default. Robust range: 20–50. BER may increase by ~0.2–1.0% when T increases beyond 20 due to stronger edits.\n  - Backprop steps k: 2–3 on 48 GB; 1–2 on 24 GB; 3–4 on 80 GB. Empirical: k=3 gives best robustness vs. PSNR trade-off.\n  - Guidance scales: sT (text) = 7.5–12.5; default 10.0. sI (image) = 1.2–2.5; default 1.5. Lower sI (≈1.0) yields heavier semantic edits and higher BER.\n- Messages\n  - Length L: 64 (default, 8×8), 256 (16×16). 64 gives BER ≈2.6% and PSNR ≈41.9; 256 gives BER ≈4.2% and PSNR ≈39.2.\n  - Keep L as a perfect square to use transposed conv upsampling; avoid L=16 unless you implement proper learned upsampling (interpolation degraded PSNR in ablation).\n  - Threshold for decoding: 0.5 on sigmoid outputs; compute BER over L bits.\n- Loss weights\n  - λ1 (VAE-latent L2): 1e-4–3e-3; default 1e-3. Too high (≥0.1) collapses training (BER ≈50%).\n  - λ2 (Lex1): 0.05–0.2; default 0.1. Larger improves post-edit extraction but harms PSNR slightly.\n  - λ3 (Lex2): 1.0 fixed; essential for convergence and localizing watermark regions.\n- Architectures\n  - Em U-Net: base channels 64; depths 4; residual output Δ with tanh scaled by α∈[0.01,0.05] initialized to small value (e.g., final conv weight init std=1e-3) to stabilize early training.\n  - Ex: channels [64,128,256], 6 residual blocks, dropout 0.0–0.1 optional; initialize final MLP bias to logit(0.5)=0.\n- Initialization and normalization\n  - Use Kaiming normal for convs; zero-init final residual block convs in Em to start near identity.\n  - Normalize images to SD preprocessing: [-1,1] range after transforms.\n- Hardware-dependent tips\n  - 24 GB GPUs: k=2, T=20, batch=1, gradient accumulation=2–4; enable xformers memory-efficient attention; offload text encoder to CPU via accelerate.\n  - 48 GB A6000: k=3, T=20, batch=2.\n  - 80 GB A100: k=3–4, T=20–30, batch=4; try L=256 for higher payload.\n- Robust vs. critical parameters\n  - Critical: k, λ3 (must be 1.0), Lex2 presence, sampler loop correctness (stop_grad then enable_grad), sI around 1.5.\n  - Robust: sampler choice (Euler/DDIM/DPMSolver), sT in 7.5–12.5, optimizer settings, base channels.\n\nApplication_Conditions:\n- When to use\n  - If images will undergo instruction-driven edits (IP2P, ControlNet-IP2P, MagicBrush) or text-driven edits (inpainting, DDIM inversion).\n  - Content provenance and IP tracing where pixel-level watermarking fails under semantic edits.\n- Hardware requirements\n  - Minimum: 24 GB GPU for 512×512, k=2, batch=1 (mixed precision).\n  - Recommended: 48 GB GPU for k=3 and batch=2 to match reported robustness.\n- Scale considerations\n  - Training set: ≥20k image-instruction pairs; eval on ≈1.2k to measure BER/PSNR/SSIM.\n  - Larger datasets of instructions improve generalization to unseen edit styles.\n- Task compatibility\n  - Best for photographic and artistic images at 512×512; extends to architectures, landscapes, objects, animals, and paintings.\n  - Robust to pixel-level perturbations even if not seen during training (JPEG, blur, noise, color shifts).\n  - Neutral/limited benefit for tasks that never use diffusion-based edits; consider classic robust watermarking instead for pure JPEG/crop pipelines.\n- Choosing vs. alternatives\n  - Prefer Robust-Wide over MBRS/CIN/RivaGAN when semantic edits are expected; those baselines show ≈40–50% BER after IP2P edits.\n  - If only physical screen-shooting robustness is needed, PIMoG/StegaStamp may be more targeted.\n- Resource constraints\n  - For tight latency or low VRAM, reduce k to 1–2 and L to 64; accept a minor BER increase.\n\nExpected_Outcomes:\n- Quantitative performance (at 512×512, T=20, k=3, sT=10, sI=1.5, L=64)\n  - Watermark integrity: BER ≈0% before editing; ≈2.6% after IP2P edits.\n  - Image quality: PSNR ≈41.9 dB, SSIM ≈0.991; visual differences mainly along contours/backgrounds.\n  - Editability: CLIP-I difference ≈+0.003; CLIP-T roughly unchanged (≈0.218→0.215).\n- Generalization\n  - ControlNet-IP2P: BER ≈0.96% average.\n  - MagicBrush: BER ≈9.3% average (heavier stylistic changes).\n  - Inpainting and DDIM inversion: extraction remains successful on tested cases.\n  - Pixel-level distortions (pre/post edit): BER typically 0–6%; color/brightness/contrast shifts up to ≈12% in worst cases; SDEdit noise+denoise ≈3.9–9.4% depending on insertion point.\n- Timeline\n  - Convergence in 15–20k steps (single A6000 ~1–2 days). Early signs by 5k steps: BER<10% on edited samples, BER~0% on unedited.\n- Trade-offs\n  - Increasing k lowers BER but reduces PSNR by ~1–3 dB.\n  - Increasing L from 64→256 increases BER by ~1.5–2% and lowers PSNR by ~2–3 dB.\n  - Increasing T beyond 20 slightly increases BER (<1%) due to stronger semantic deviation but yields more detailed edits.\n- Failure modes and mitigations\n  - Missing Lex2: BER ≈50% before/after edits; fix by enabling Lex2 term and ensuring Ex(Iwm) path is trained.\n  - Incorrect gradient gating: if no detach for first T−k steps, OOM; if detach in last k steps, no improvement in robustness and poor convergence; verify by checking requires_grad on latents in last k steps.\n  - Excessive λ1 (≥0.1): model collapses to over-preserve image; BER ≈50%; lower λ1 to 1e-3.\n  - Very low sI (≈1.0) or extremely strong edits: BER rises (>5–10%); advise sI≥1.3 for validation; accept that re-creations can exceed robustness limits.\n  - Very long messages (≥1024 bits): BER increases (≈6–10%) and PSNR drops; keep L≤256 for reliable extraction.\n- Debugging indicators\n  - Ex on original, non-watermarked images should yield ≈50% BER (integrity check).\n  - Ex(Iwm) BER should quickly approach 0% during training (<2k steps).\n  - Residual maps should highlight semantic contours and some background, not only high-frequency noise; if residuals look like uniform speckle, Em is not learning semantic placement—reduce λ1, increase k, or increase Lex2 weight slightly.\n  - Ablate k: observe monotonic BER decrease from k=1→3 and PSNR decrease; if not, gradient gating is incorrect.\n- Hardware-specific expectations\n  - 24 GB GPU: training speed ~0.4–0.6 it/s (batch=1, k=2); inference embed ~80–120 ms/image for Em; PIDSG forward during training dominates time due to SD-IP2P loop.\n  - 48 GB GPU: ~0.8–1.2 it/s (batch=2, k=3).\n  - 80 GB GPU: batch=4, k=3–4 may reach ~1.5–2.0 it/s.\n\nValidation procedures:\n- Dataset-level\n  - Train on 20k IP2P pairs; evaluate on 1.2k held-out pairs with 6–8 edited images per instruction; report BER mean±std.\n- Metrics to log every 500 steps\n  - BER on: (i) Iwm, (ii) Iedit_wm, (iii) original unwatermarked images (integrity).\n  - PSNR/SSIM between Iori and Iwm.\n  - Optional: CLIP-I/CLIP-T to verify editability.\n- Stress tests\n  - Distortions before and after edits (JPEG qualities 50–95; noise σ=5–25; blur ksize=3–7).\n  - Different samplers and guidance scales (sT=5–15, sI=1–3).\n  - Continual editing (3 rounds): expect successful extraction up to 3 rounds with rising BER; monitor curve shape similar to monotonic increase.\n\nTroubleshooting checklist:\n- OOM: lower k, enable fp16/bf16, activate xformers, reduce batch, or accumulate grads.\n- High BER after edits (>15%): confirm Lex2 present, λ1 in [1e-4,3e-3], k≥2; verify pipeline uses Euler/consistent scheduler between train/eval.\n- Poor image quality (PSNR<35 dB): reduce λ2 or message length L; lower Em residual scale α; ensure Em outputs residual not full image.\n- No robustness to alternative editors: diversify instruction set and optionally mix ControlNet-IP2P and inpainting during training (keep them frozen with same PIDSG gating)."
    }
]