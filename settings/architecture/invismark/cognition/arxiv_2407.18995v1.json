[
    {
        "DESIGN_INSIGHT": "### DESIGN_INSIGHT_HIGH: [Hide-R – Unit-Norm Real-Vector Watermarking with PSNR-Constrained Embedding and Exact p-value Detection]\nThis replaces fixed-length binary watermark embedding (e.g., HiDDeN) with the embedding/extraction of high-dimensional, unit-norm real vectors on a hypersphere, and replaces adversarial imperceptibility control with a direct PSNR budget. The encoder concatenates the cover image with a sampled unit vector and the decoder reconstructs the transmitted vector; authenticity is decided via cosine-similarity-based hypothesis testing with a closed-form p-value.\n\nKey mechanism: messages are carried by vectors \\(X \\in \\mathbb{R}^D\\) sampled uniformly from the unit hypersphere by normalizing Gaussian noise,\n\\[\nX = \\frac{Z}{\\|Z\\|}, \\quad Z \\sim \\mathcal{N}(0, I_D),\n\\]\nand embedded using a CNN with channel capacity scaled to \\(1.5D\\) to accommodate large \\(D\\) (e.g., \\(D=256\\)). Imperceptibility is enforced by an L2 power constraint mapped to a target PSNR, while training minimizes the reconstruction loss \\(\\|X - Y\\|^2\\) for the extracted vector \\(Y\\). Detection uses cosine similarity \\(C_0 = Y^\\top X_0\\) against the key-seeded probe \\(X_0\\), and provides an exact p-value under \\(H_0\\) (no watermark or wrong key) via the regularized incomplete beta function:\n\\[\n\\rho_0(c) = \n\\begin{cases}\n\\frac{1}{2}\\,I_{1-c^2}\\!\\left(\\frac{D-1}{2}, \\frac{1}{2}\\right), & c > 0\\\\[4pt]\n\\frac{1}{2}\\,I_{c^2}\\!\\left(\\frac{1}{2}, \\frac{D-1}{2}\\right), & c \\le 0\n\\end{cases}.\n\\]\n\nFundamental difference from prior approaches: (i) the watermark channel is a continuous, hyperspherical carrier rather than a fixed-bit binary code, enabling variable-rate semantic payloads; (ii) imperceptibility is controlled by a hard PSNR budget rather than a discriminator, simplifying training and speeding convergence; and (iii) extraction yields a statistically principled confidence metric (p-value), not just bit error rates, enabling reliable decision-making. Inference remains a single forward pass, improving practicality compared to iterative latent-space watermarking (e.g., SSL).\n\n\n### DESIGN_INSIGHT_MEDIUM: [TCCSK Hyperspherical Modulation – Variable-Rate Covert Channel with Blockwise Decoding and Fisher-Combined Confidence]\nThis replaces the intractable multi-key multi-bit decoding (\\(O(2^N)\\)) with a variable-rate block modulation over shared carriers, enabling efficient embedding of arbitrary-length bitstreams onto the Hide-R hyperspherical channel while preserving security via a secret key.\n\nKey mechanism: split the bitstream \\(M\\) into \\(T\\) blocks of length \\(L\\). For each block \\(j\\), draw a secret-keyed carrier \\(Z_j \\in \\mathbb{R}^{2L}\\sim \\mathcal{N}(0, I_{2L})\\). Encode by cyclically shifting \\(Z_j\\) by the integer value of the block \\(M_j\\), truncate to \\(D\\) dims, normalize, sum the normalized carriers, and renormalize:\n\\[\nX = \\frac{Z}{\\|Z\\|},\\quad Z = \\sum_{j=0}^{T-1} \\frac{Z^{(M_j)}_j}{\\left\\|Z^{(M_j)}_j\\right\\|},\n\\]\nwhere \\(Z^{(k)}_j\\) denotes the cyclic shift by \\(k\\) followed by truncation to the first \\(D\\) dimensions. Decoding is blockwise correlation:\n\\[\n\\hat M_j = \\arg\\max_{k \\in \\{0,\\dots,2^L\\!-\\!1\\}} \\frac{Y^\\top Z^{(k)}_j}{\\left\\|Z^{(k)}_j\\right\\|}.\n\\]\nConfidence for the full message aggregates per-block p-values using Fisher’s combined probability test:\n\\[\n\\rho = 1 - \\gamma\\!\\left(T,\\; -\\sum_{j=0}^{T-1}\\log\\!\\big(\\rho_1(C_j)\\big)\\right),\\quad C_j = \\frac{Y^\\top Z^{(\\hat M_j)}_j}{\\left\\|Z^{(\\hat M_j)}_j\\right\\|},\n\\]\nwith \\(\\rho_1(c) \\approx 1 - (1-\\rho_0(c))^{2^N} \\simeq 2^N \\rho_0(c)\\) when \\(2^N\\rho_0 \\ll 1\\).\n\nFundamental difference: modulation occurs directly on the Hide-R hypersphere using truncated cyclic code shift keying (TCCSK), yielding better AWGN robustness than BPSK and spherical lattice codes (as evidenced by BER–\\(E_b/N_0\\) curves) while supporting variable rate via block sizing. Complexity improves from testing \\(2^N\\) keys to \\(O(T \\cdot 2^L)\\) correlators with small \\(L\\), and the resulting per-message confidence is statistically grounded, enabling thresholding to guarantee perfect recovery (MRR→100%) under chosen operating points.\n\n\n### DESIGN_INSIGHT_MEDIUM: [LLM-Guided Arithmetic Caption Compression (LLMZip-ft) – Semantic Payload Reduction for Robust Watermarking]\nThis replaces fixed-capacity watermark payload assumptions with a learned semantic compressor tailored to image captions, reducing the bit budget so the hyperspherical channel can carry meaningful semantics with high robustness.\n\nKey mechanism: generate a caption \\(m\\) via BLIP2 and compress it losslessly using arithmetic coding driven by a finetuned language model (OPT-125m trained on BLIP2 captions) that provides token probabilities \\(p(s_t \\mid s_{<t}, \\text{image})\\). The transmitted bitstream is\n\\[\nM = \\mathrm{AC}\\!\\left(m;\\; \\{p(s_t \\mid s_{<t})\\}_{t=1}^{|m|}\\right),\n\\]\nwhere AC denotes arithmetic coding conditioned on the LLM token distribution. Finetuning aligns the LM to caption statistics, lowering entropy and average code length.\n\nFundamental difference: by aligning the compressor to the semantic source, average payload shrinks substantially (from ~75 to ~45 bits per caption per Fig. 2), directly increasing effective robustness at a fixed watermark power/PSNR and enabling variable-length messages to fit within Hide-R’s capacity. This tight coupling of a semantic model with a physical-layer watermark channel bridges passive semantic verification and active robust watermarking, making semantic integrity checks feasible under benign and malign edits.",
        "EXPERIMENTAL_TRIGGER_PATTERNS": "**Task_Performance_Signatures**:\n- Image watermarking/authentication (Emu Edit, Message Recovery Rate, MRR):\n  - Expect improved performance on benign transforms at PSNR=40 dB:\n    - original: ~96.6% MRR\n    - jpeg@50: ~96.2% MRR\n    - grayscale: ~95.8% MRR\n    - resize to 128×128: ~95.1% MRR\n    - crop 40%: ~82.4% MRR\n    - random noise: ~75.5% MRR\n  - Malign edits (global, text, style, local, background, color, remove, add) at PSNR=40 dB:\n    - Expect robust recovery: ~63.7–69.5% MRR (average ≈66–67%), an orders-of-magnitude improvement vs SSL (~0–1.5%) and TrustMark(Q) (~0%) across these categories.\n  - Versus baselines:\n    - Resize 128×128: +95.1 pp over SSL (0.0%) and +95.1 pp over TrustMark(Q) (0.0%).\n    - Grayscale: +93.0 pp over SSL (2.82%) and +17.8 pp over TrustMark(Q) (78.0%); competitive with TrustMark(Base) (91.2%).\n    - JPEG@50: +94.7 pp over SSL (1.5%); competitive with TrustMark(Base) (93.2%).\n    - Noise: dramatically higher than SSL (0.0%) and TrustMark(Q) (11.8%).\n    - Malign edits: SWIFT consistently >60% MRR, while SSL and TrustMark(Q) ≈0–1.5%.\n  - PSNR trade-off:\n    - Expect slightly degraded performance when increasing PSNR target (more imperceptible watermarks): at PSNR=42 dB, benign MRR drops by ~2–10 pp (e.g., original 95.1%, jpeg 93.5%, grayscale 93.8%, resize 91.3%, crop 76.7%, noise 65.4%).\n    - Performance remains competitive while achieving higher imperceptibility at 42 dB.\n  - Confidence-thresholding (ρ from Fisher’s test):\n    - ρ ≤ 1.4e−16 (High confidence): MRR=100% on accepted subsets across watermarked/benign/malign; acceptance rates (CDF) ~66.2% (watermarked), ~42.3% (benign), ~18.1% (malign). Pattern: lower acceptance but perfect recovery for trusted messages.\n    - ρ ≤ 2.3e−15 (Medium confidence): Watermarked and benign accepted subsets achieve ~100% MRR; malign ~99.3% MRR; moderate acceptance (~72% watermarked, ~48.3% benign, ~20.2% malign).\n    - ρ ≤ 4.2e−13 (Low confidence): MRR ~100% (watermarked), ~96.2% (benign), ~94.1% (malign) with higher acceptance (~82.6%, ~60.1%, ~31.1%). Strong negative correlation between ρ and correctness (Pearson ≈−0.89).\n  - Capacity/encoding efficiency:\n    - Expect ~40% reduction in mean caption bit-length after finetuning OPT-125m for arithmetic coding (from ~75 to ~45 bits), increasing effective payload robustness under fixed PSNR budgets.\n  - Modulation under AWGN (TCCSK vs BPSK and spherical lattice):\n    - Expect improved/stable bit error rates with TCCSK across Eb/N0 compared to BPSK; TCCSK remains most suited for variable-length messages. Gains are most pronounced at moderate SNR (visualized dominance across curves).\n  - Scale/conditions:\n    - Improvements most pronounced for transforms common in web pipelines (JPEG, resize, grayscale).\n    - Heavy spatial occlusions (crop 40%) and strong noise show measurable but bounded degradation; confidence gating restores perfect extraction in accepted subsets.\n    - Longer messages (larger N) require tighter confidence thresholds and/or more carriers; from Eq. (4), reliability degrades as N increases beyond −log2(ρ0(c)).\n- Language modeling (lambada_openai, wikitext, ptb):\n  - Expect stable/unchanged performance (technique orthogonal): within ±0.5% of baseline; no measurable speed or memory change on these tasks.\n- Reading comprehension (squad_completion, squad_v2, narrativeqa):\n  - Expect stable/unchanged performance: within ±0.5% accuracy/F1; SWIFT does not modify LM inference/training.\n- Commonsense reasoning (hellaswag, piqa, social_iqa, commonsenseqa):\n  - Expect stable/unchanged performance: within ±0.5% of baseline; no task-relevant effect.\n- Factual QA (arc_easy, arc_challenge, boolq, openbookqa):\n  - Expect stable/unchanged performance: within ±0.5% of baseline; no impact on QA pipelines.\n- Context resolution (winogrande, winograd):\n  - Expect stable/unchanged performance: within ±0.5%; technique independent of coreference resolution.\n- Other tasks (swde, fda):\n  - swde: No direct effect; structured extraction accuracy unchanged.\n  - fda (data augmentation): Expect improved robustness in watermark recovery when stronger augmentations are used during training; training loss decreases more smoothly under diverse augmentations; negligible quality drops in image fidelity given fixed PSNR control.\n\n**Architectural_Symptoms**:\n- Training characteristics:\n  - Smoother convergence of reconstruction loss ||X−Y|| (L2) due to removal of adversarial discriminator; faster initial convergence and reduced variance across epochs.\n  - Stability indicators: fewer training instabilities (no GAN oscillations), rare NaN occurrences; robust to moderate hyperparameter changes (PSNR target, D=256, channel width=1.5D).\n  - As D increases (vector dimensionality), loss and cosine similarity improve steadily; performance remains stable with wider/deeper CNN embedder/extractor (channels scaled to 1.5D).\n  - Clear separation during training between positive cosine similarities (target carriers) and negatives; rising mean cosine for correct decodes, stable low values under H0; p-values shrink progressively (ρ→1e−16 range) for correctly decoded blocks.\n- Runtime/memory behaviors:\n  - Memory scales linearly with image resolution and channel count; inference memory largely constant with message bit-length for the watermarking layer (fixed-D vector), while TCCSK demodulation compute/memory grows with the number of evaluated cyclic shifts (∝2^L per block) and number of blocks T.\n  - Throughput patterns: encoder/decoder CNNs achieve consistent GPU utilization; demodulation dominated by dot-product operations over shifted carriers; inference speed degrades superlinearly with large L, improves with more carriers and moderate L via blocking.\n  - Efficient imperceptibility control: fixed PSNR budget yields predictable watermark power; no OOM observed at typical resolutions under target PSNR budgets (indicator: consistent batch completion without memory spikes).\n- Profiling signatures:\n  - Cosine similarity distributions: correct decodes produce high C values and correspondingly tiny ρ via combined Fisher test; incorrect or tampered inputs shift C downward, increasing ρ (visible swing across thresholds).\n  - Augmentation pipeline: robust extraction under JPEG, resize, grayscale shows minimal throughput variance; crop/noise induce more re-decodes per image (retry or confidence filtering manifests as additional vector comparisons).\n  - Compared to baseline implementations (SSL/TrustMark):\n    - Faster training per epoch (no adversarial loop), fewer discriminator-related stalls.\n    - Lower variance in validation MRR across epochs; more predictable generalization under diverse augmentations.\n- Positive/neutral/negative effects:\n  - Positive: High robustness and confidence-calibrated decoding; training stability; imperceptibility via PSNR control; scalability with variable-length payloads.\n  - Neutral: No impact on language/QA benchmarks; unchanged LM memory/throughput.\n  - Negative: Inference cost grows with block size L due to 2^L shift evaluations; heavy occlusions (large crops) and strong noise reduce MRR unless confidence gating is applied; raising PSNR target slightly degrades recovery (typically 2–10 pp on benign transforms).",
        "BACKGROUND": "Title: SWIFT: Semantic Watermarking for Image Forgery Thwarting\n\nHistorical Technical Context:\nRobust and fragile digital watermarking have historically operated in spatial and frequency domains using deterministic transforms (e.g., DCT/DWT) to embed short binary payloads for copyright or authentication. The advent of deep learning introduced encoder–decoder CNNs such as HiDDeN, trained end-to-end with differentiable augmentations to improve robustness to benign edits while maintaining imperceptibility. Concurrently, passive image forensics advanced with CNN-based detectors for copy–move, splicing, and inpainting, relying on high-frequency residuals and noise patterns, yet limited to local anomaly cues and unable to provide global contextual semantics.\n\nAs generative image editing (e.g., diffusion-based editing) became prevalent, the problem of authenticity moved from pixel-level perturbations to semantic alterations. Security initiatives (C2PA/IPTC) use cryptographic provenance but can be stripped or mismatched post-edit. Prior watermarking largely fixed payloads (≤64 bits) and lacked mechanisms to communicate semantic context. A related idea embedded compressed image self-representations for robust comparison, but not textual semantics. Parallel progress in vision–language models (BLIP2) and LLM-assisted compression (LLMZip) made it feasible to encode image captions efficiently. Communication-theoretic modulation (e.g., cyclic code shift keying) provides variable-rate signaling over noisy channels (AWGN), suggesting a route to bridge robust watermarking with semantic verification. SWIFT builds on these trends by disentangling a high-dimensional zero-bit watermarking channel (Hide-R) from an encoding/modulation layer that carries variable-length caption bits, and by introducing statistically grounded confidence.\n\nTechnical Limitations:\n- Capacity and fixed-rate payloads: Prior robust watermarking architectures typically support ≤64 bits with acceptable robustness; increasing payload sharply elevates bit error rate (BER). The search-based use of zero-bit detectors for multi-bit messaging scales as O(2^N) in the number of candidate keys/messages, becoming computationally intractable as N grows.\n- Lack of reliability measures: Existing deep watermark decoders output bit strings without calibrated confidence. Under H0 (no watermark or wrong key), the probability of false acceptance is not quantified, leading to unreliable authentication and susceptibility to spoofing.\n- Robustness vs. semantics: Fragile/semi-fragile schemes break under benign edits; robust schemes survive aggressive distortions but cannot decide if semantics changed. There is no channel to compare received content with trusted semantics.\n- Inference cost and practical deployment: Methods embedding in foundation model latent spaces (e.g., SSL) entail iterative optimization per image, with cost roughly O(G·C) for G gradient steps and network cost C, leading to high latency and energy use. CNN encoder–decoders with GAN-style discriminators also add training/inference overhead and stability issues.\n- Transform generalization: Prior deep watermarking shows poor resilience to common web transforms (e.g., resize to 128×128, grayscale), with reported MRR near 0% for resize/grayscale in baselines, indicating limited generalization to realistic deployment conditions.\n- Security exposure: Without a strong key-driven carrier design, attackers can spoof messages or estimate embedded structure. Systems that reveal all carrier values per use weaken key secrecy; multi-use leakage demands careful carrier truncation and per-use exposure control.\n\nPaper Concepts:\n- Hide-R (High-dimensional Real-vector watermarking): A CNN encoder–decoder jointly trained to embed and extract unit-norm vectors on a D-dimensional hypersphere rather than binary strings. Random carriers are sampled as\n  \\( Z \\sim \\mathcal{N}(0, I_D), \\quad X = \\frac{Z}{\\lVert Z \\rVert_2} \\in \\mathbb{S}^{D-1} \\),\n  concatenated with the cover image channels. Training minimizes a reconstruction loss \\( \\lVert X - Y \\rVert_2 \\) with an L2 power control enforcing a target PSNR. Channel widths are scaled to 1.5D to support large D (e.g., D=256). Intuitively, Hide-R learns a robust, imperceptible projection from images to a semantic communication channel.\n\n- Zero-bit watermark hypothesis test and p-value: Presence of a specific carrier vector \\( X_0 \\) (derived from a key K) is tested via cosine similarity \\( C_0 = Y^\\top X_0 \\). Under H0 (no watermark/wrong key), the p-value is\n  \\[\n  \\rho_0(c) = \\mathbb{P}(C_0 \\ge c) =\n  \\begin{cases}\n  \\tfrac{1}{2} I_{1-c^2}\\!\\left(\\tfrac{D-1}{2}, \\tfrac{1}{2}\\right), & c>0, \\\\\n  \\tfrac{1}{2} I_{c^2}\\!\\left(\\tfrac{1}{2}, \\tfrac{D-1}{2}\\right), & \\text{otherwise},\n  \\end{cases}\n  \\]\n  where \\( I \\) is the regularized incomplete beta function. This yields a calibrated confidence on watermark presence.\n\n- TCCSK (Truncated Cyclic Code Shift Keying) modulation: A variable-rate scheme that splits a bitstream \\( M \\) into T blocks of L bits. For each block j, a Gaussian carrier \\( Z_j \\in \\mathbb{R}^{2L} \\) is cyclically shifted by the integer value of \\( M_j \\), truncated to D, normalized, and summed:\n  \\[\n  Z = \\sum_{j=0}^{T-1} \\frac{Z^{\\text{shift}=M_j}_j}{\\lVert Z^{\\text{shift}=M_j}_j \\rVert_2}, \\quad\n  X = \\frac{Z}{\\lVert Z \\rVert_2}.\n  \\]\n  Decoding recovers each submessage by matched filtering:\n  \\[\n  \\hat{M}_j = \\arg\\max_{k \\in \\{0,\\dots,2^L-1\\}} \\frac{Y^\\top Z^{\\text{shift}=k}_j}{\\lVert Z^{\\text{shift}=k}_j \\rVert_2}.\n  \\]\n  Intuitively, each block selects a cyclic shift index; block-wise testing controls complexity while supporting variable-length payloads and robustness under AWGN.\n\n- Fisher’s combined probability test for confidence aggregation: Per-block decisions yield similarities \\( C_j \\) and per-block p-values \\( \\rho_1(C_j) \\) (multi-bit extension of \\( \\rho_0 \\)). The global confidence aggregates evidence:\n  \\[\n  \\rho = 1 - \\gamma\\!\\left(T,\\ -\\sum_{j=0}^{T-1}\\log \\rho_1(C_j)\\right),\n  \\]\n  where \\( \\gamma(\\cdot,\\cdot) \\) is the lower incomplete gamma function. This converts multiple weak signals into a single calibrated trust score for the decoded caption.\n\n- LLM-augmented Arithmetic Coding for Caption Compression: Given a caption \\( m = (s_1,\\dots,s_n) \\), an LLM (OPT-125m fine-tuned on BLIP2 captions) provides symbol probabilities \\( p(s_t \\mid s_{<t}) \\). Arithmetic coding encodes m with expected length\n  \\[\n  \\mathbb{E}[\\ell(m)] \\approx \\sum_{t=1}^n -\\log_2 p(s_t \\mid s_{<t}),\n  \\]\n  reducing mean caption bits from ≈75 to ≈45 in experiments. Intuitively, better language modeling shrinks payload, increasing effective watermark capacity and robustness.\n\n- PSNR-constrained embedding: Imperceptibility is enforced by bounding mean squared error (MSE) between cover and watermarked images via\n  \\[\n  \\text{PSNR} = 10 \\log_{10} \\left( \\frac{MAX^2}{\\text{MSE}} \\right),\n  \\]\n  with targets (e.g., 40–42 dB). Training uses L2 normalization to control watermark power both in the embedder and at the extractor.\n\nExperimental Context:\nThe evaluation targets robustness and authenticity at the semantic level. Tasks emphasize message recovery after benign transforms common in web pipelines (crop, noise, grayscale, resize to 128×128, JPEG@50) and malign edits produced by diffusion-based Image Editing following Emu Edit instructions (add/remove/background/style/text/global/color/local). The primary metric is Message Recovery Rate (MRR), defined as the fraction of perfectly recovered captions across a large test corpus:\n\\( \\mathrm{MRR} = \\frac{1}{|I_t|}\\sum_i \\delta(m_i,\\hat{m}_i) \\).\nConfidence is integral: the p-value \\( \\rho \\) guides trust decisions, with thresholds chosen to trade coverage against perfect recovery; empirical Pearson correlation between \\( \\rho \\) and errors is −0.89, validating calibration.\n\nExperiments use 20,220 images constructed from the Emu Edit test set (2,022 MSCOCO images × multiple benign/malign variants and edited versions). Baselines include SSL (latent-space watermarking) and TrustMark (GAN-regularized encoder–decoder). SWIFT is evaluated at PSNR≈40 dB and ≈42 dB. The philosophy prioritizes practical robustness under realistic pipelines, semantic integrity under malign edits, and deployable confidence. Results show state-of-the-art MRR on challenging transforms (e.g., resize 128×128: 95.1% at 40 dB; grayscale: 95.8% at 40 dB) and strong resilience to malign edits while providing calibrated rejection via \\( \\rho \\)-thresholds (e.g., \\( \\rho \\le 1.4\\times10^{-16} \\) achieves 100% trusted recovery under both benign and malign scenarios). A modulation study under AWGN confirms TCCSK’s superior BER curves over BPSK and single-carrier spherical lattice schemes, supporting the chosen variable-rate design.",
        "ALGORITHMIC_INNOVATION": "Core_Algorithm:\n- Replace HiDDeN’s fixed-length binary embedding with a high-dimensional real unit-vector embedding and a separate modulation layer. The embedder takes an image Ic and a D-dimensional unit vector X, and outputs a residual that is scaled to meet a target PSNR before adding to Ic; the extractor maps the possibly transformed image to a unit vector Y.\n- Introduce a modulation/demodulation scheme (TCCSK) to map a variable-length bitstream M (caption compressed by arithmetic coding guided by a finetuned LLM) into the unit vector X on the D-sphere, and to recover M from the extracted Y via blockwise cyclic-correlation decoding.\n- Remove the adversarial discriminator; enforce imperceptibility with an explicit PSNR budget and train end-to-end by minimizing the reconstruction loss ||X−Y|| with differentiable image transforms between embedder and extractor.\n- Add a statistically principled confidence metric by modeling cosine similarities on the D-sphere and combining per-block p-values with Fisher’s method; decoding is accepted only if the combined p-value ρ is below a threshold.\n\nKey_Mechanism:\n- The key insight is to decouple robust watermark transport (continuous, redundant, geometric) from discrete messaging (variable-rate coding), mapping bits to points on a high-dimensional sphere where cosine similarity is robust to common image degradations. This increases channel reliability compared to directly embedding bits and enables statistical detection with calibrated p-values on spherical caps.\n- TCCSK exploits cyclic shifts of Gaussian carriers keyed by K to realize a large, structured codebook whose correlational decoding aligns with the cosine similarity used by the extractor. Arithmetic coding guided by an LLM reduces payload length, improving message recovery at a fixed embedding power (PSNR).\n\nMathematical_Formulation:\n- Caption compression (message layer): Given a caption m = (t1,…,t|m|), arithmetic coding encodes M using token probabilities pθ(ti | t< i) from a finetuned OPT-125m:\n  Hθ(m) = −∑i log2 pθ(ti | t< i),  length(M) ≈ Hθ(m) bits.\n- Modulation (TCCSK, block length L, T blocks): Draw carriers Zj ∼ N(0, I2L), j = 0…T−1 from a PRNG seeded by key K. For sub-message Mj ∈ {0,…,2^L−1}, define the truncated cyclic shift Zj,Mj and transmit the normalized sum:\n  Z = ∑_{j=0}^{T−1} Zj,Mj / ||Zj,Mj||,   X = Z / ||Z||,   X ∈ S^{D−1}.\n- Embedding (Hide-R): The embedder Eψ outputs a residual R = Eψ(Ic, X). Let P = H·W be pixel count and set the per-image MSE budget σ^2 to reach target PSNR:\n  PSNR = 10 log10 (MAX^2 / σ^2),   s = √(P σ^2 / ||R||_2^2),   Iw = Ic + s R.\n  Random differentiable transforms T (e.g., resize, JPEG approx, noise, grayscale, crop) simulate the channel: Ĩ = T(Iw).\n- Extraction and training loss: The extractor Fφ maps Ĩ to Ỹ, which is L2-normalized to Y = Ỹ / ||Ỹ||. Minimize\n  L(ψ, φ) = E[ ||X − Y||_2^2 ]  subject to the PSNR constraint above.\n- Demodulation and confidence: For each block j, estimate\n  M̂j = argmax_{k ∈ {0,…,2^L−1}} ( Y⊤ Zj,k / ||Zj,k|| ),   Cj = Y⊤ Zj,M̂j / ||Zj,M̂j||.\n  Zero-bit spherical-cap tail probability under H0 (no matching key) for a single detector with cosine threshold c is\n  ρ0(c) = P(C ≥ c) = ½ I_{1−c^2}((D−1)/2, ½) for c > 0, else ½ I_{c^2}(½, (D−1)/2),\n  where I is the regularized incomplete beta. For 2^N independent detectors, ρ1(c) ≈ 2^N ρ0(c) when 2^N ρ0(c) ≪ 1.\n  Combine per-block p-values with Fisher’s method:\n  ρ = 1 − γ(T, −∑_{j=0}^{T−1} log ρ1(Cj)),\n  where γ is the lower regularized incomplete gamma. Accept decoding if ρ < τ.\n- Complexity reference: Standard multi-bit zero-bit decoding over all 2^N messages is O(2^N D). TCCSK reduces this to O(T 2^L D) with N = T·L.\n\nExample Format:\n- Spherical similarity: C = Y⊤X, with X, Y ∈ S^{D−1}.\n- Embedder scaling: s = √(P σ^2 / ||R||_2^2), Iw = Ic + s R, PSNR = 10 log10 (MAX^2 / σ^2).\n- Demodulation per block: M̂j = argmax_k ( Y⊤ Zj,k / ||Zj,k|| ).\n- Confidence combination: ρ = 1 − γ(T, −∑_j log ρ1(Cj)).\n- Complexity: O(T 2^L D + cost_conv(P, D)) vs O(2^N D) for naïve zero-bit multi-detector decoding and O(P) for arithmetic coding.\n\nComputational_Properties:\n- Time Complexity:\n  - Training: O(B [ cost_embed(P, D) + cost_transforms(P) + cost_extract(P, D) ]) per batch B, where cost_embed/extract are dominated by convolutional layers with width O(D). For a k×k conv with C_in, C_out ~ O(D), cost ≈ O(P k^2 D^2) per layer.\n  - Inference (watermarking): O(cost_embed(P, D)) to embed; O(cost_extract(P, D)) to extract.\n  - Modulation: Encoding O(T D + T 2L) to build X (generate T carriers length 2L, truncate to D); Demodulation O(T 2^L D) for all cyclic correlations. Optional FFT-based circular correlation can reduce per-block cost to O(D log D) if implemented, replacing exhaustive shifts.\n  - Arithmetic coding: O(|m|) for encoding/decoding given token probabilities from the LLM; negligible vs CNN passes.\n- Space Complexity:\n  - Activations: O(P D) per CNN layer; memory peaks with the deepest layers during training due to backpropagation.\n  - Carriers: O(D) if generated on-the-fly from PRNG; O(T 2^L D) if pre-expanded (not required).\n  - Parameters: O(k^2 D^2) across conv layers (embedder and extractor) with width scaling in D.\n- Parallelization:\n  - CNN embedder/extractor fully data-parallel across images and spatially parallel within images on GPUs.\n  - Demodulation correlations across 2^L shifts and across T blocks are independent and can be vectorized; batched dot-products or FFT-based correlations map well to SIMT.\n  - Arithmetic coding is sequential per token but trivially parallel across images.\n- Hardware Compatibility:\n  - GPU-friendly due to convolutional backbones and batched dense operations; memory access is contiguous in NHWC/NCHW layouts.\n  - CPU inference feasible for modulation/demodulation (BLAS dot-products) but CNN extraction benefits significantly from GPUs.\n  - Bandwidth: PSNR scaling uses simple elementwise ops; differentiable transforms should use fused kernels to reduce memory traffic.\n- Training vs. Inference:\n  - Training inserts randomized differentiable transforms between embedder and extractor; this increases compute vs inference but is fully parallelizable.\n  - Inference omits transforms except the real-world channel; only one forward pass through extractor and TCCSK demodulation is needed.\n- Parameter Count:\n  - With channel width ≈ 1.5D (D=256 ⇒ width ≈ 384), and L conv blocks, parameter count per block ≈ k^2·C_in·C_out = O(k^2 D^2). Two towers (embedder, extractor) double this. Removing the discriminator reduces parameters and training overhead vs HiDDeN.\n- Numerical Stability:\n  - Unit-norm constraints on X and Y stabilize cosine similarity and loss scaling.\n  - PSNR enforcement via analytic scaling s avoids adversarial instability and discriminator collapse; clip s to [s_min, s_max].\n  - Use stable implementations for regularized incomplete beta and gamma; compute −∑ log p with clamped p ∈ [ε, 1−ε] and double precision for very small ρ (e.g., ε ≈ 1e−300).\n  - Normalize carriers and Y before correlation; avoid division by small norms by ε-normalization.\n- Scaling Behavior:\n  - Increasing D improves robustness (sharper spherical caps) and confidence calibration but raises CNN width (quadratic parameter/time growth) and dot-product cost linearly O(D).\n  - Message length N scales linearly with T but demodulation cost grows as O(T 2^L D); choose moderate L (e.g., 8–12) to keep decoding tractable; padding overhead vanishes when N is a multiple of L.\n  - Higher PSNR budgets (lower σ^2) reduce embedding power; the observed trade-off is mitigated by LLM-guided compression (shorter M) and by TCCSK’s coding gain on the hypersphere.\n\nImplementation-critical details and pitfalls:\n- Concatenate X as D constant feature planes to the image channels before the first conv; extend internal channel width to 1.5D to preserve capacity.\n- Always renormalize X and Y to unit norm; apply PSNR scaling after producing residual.\n- Use a keyed PRNG to generate Zj to ensure security and reproducibility; never reuse the same K with excessive T across many images to limit carrier exposure.\n- Batch demodulation: precompute and cache Zj,k / ||Zj,k|| on-the-fly per image or compute via FFT convolution to avoid O(T 2^L D) memory.",
        "IMPLEMENTATION_GUIDANCE": "Integration_Strategy:\n- Replace HiDDeN’s binary-message pathway with a real-vector pathway:\n  - In the Embedder: replace the binary message input with a D-dimensional unit-norm real vector X ∈ R^D (default D=256). Concatenate X to the cover image Ic along the channel dimension before the first conv.\n  - In the Extractor: output a D-dimensional vector Y and normalize to unit norm before loss computation and demodulation.\n- Remove adversarial discriminator and introduce a PSNR-budget controller:\n  - Add a watermark-power L2 constraint module that scales the embedder’s residual to meet a target PSNR (e.g., 40–42 dB) both for the watermark signal in the embedder and for the decoded vector in the extractor.\n- Insert a modulation/demodulation layer implementing TCCSK:\n  - Modulation (encode_bits_to_vector): split the bitstream M into T blocks of length L, generate carriers Z_j ∈ R^{2L} via a PRNG seeded by key K, apply cyclic shift by the L-bit submessage value, truncate to D, normalize, sum, and re-normalize to produce X.\n  - Demodulation (decode_vector_to_bits): for each block j, compute argmax_k (Y^T Z_{j,k}/||Z_{j,k}||) over k ∈ [0, 2^L−1], reconstruct submessages, concatenate to ˆM.\n  - Provide Fisher’s combined test to compute the global p-value ρ across blocks using per-block confidence from Eq. (4).\n- Integrate the Message Layer:\n  - Captioning: use BLIP2 to produce a caption m for the cover image.\n  - Compression: use arithmetic coding driven by a finetuned OPT-125m probability model (LLMZip-style) to produce a binary payload M. Implement an encoder/decoder pair; the decoder must reproduce the same symbol distribution.\n- Key management:\n  - Implement a cryptographic PRNG (AES-CTR or ChaCha20) seeded by secret K to generate carriers deterministically across encode/decode; expose functions generate_carriers(K, L, T, D).\n- Code-level changes (PyTorch reference):\n  - Classes: HideREmbedder(nn.Module), HideRExtractor(nn.Module), TCCSKModulator, TCCSKDemodulator, PSNRController, ArithmeticCoder, CaptionerBLIP2, OPT125mEntropyModel.\n  - Functions: encode_caption(image)->M, encode_bits_to_vector(M,K,L,T,D)->X, embed(Ic,X)->Iw, extract(Iw)->Y, decode_vector_to_bits(Y,K,L,T,D)->ˆM, fisher_pvalue(block_pvals)->ρ.\n  - Losses: reconstruction loss Lrec = ||X−Y||_2^2; optional image loss Lim = ||Iw − Ic||_2 with PSNR budget enforcement rather than adversarial loss.\n  - Augmentations: differentiable or non-differentiable transforms between embed and extract in training loop (crop, noise, grayscale, resize to 128×128, JPEG@50), implemented via Kornia or torchvision/transforms; JPEG can use differentiable approximations or batched CPU/Pillow.\n- Framework compatibility:\n  - PyTorch native; TensorFlow/JAX require equivalent channel concatenation and normalization; ensure deterministic PRNG reproducibility.\n  - Use AMP (fp16 or bf16) with care for cosine-similarity stability; normalize vectors in float32.\n- Migration path from HiDDeN:\n  - Keep convolutional skeleton; increase channels to 1.5D; remove binary bit embedding layers and error-correction code; swap BCE with L2 vector reconstruction loss; replace discriminator with PSNRController; plug TCCSK and the confidence metric at inference.\n- Dependencies:\n  - transformers (BLIP2, OPT-125m), a range/arithmetic coding library (e.g., python-rc, construe your own), Kornia/torchvision for augmentations, cryptography for AES-CTR/ChaCha20, numpy/scipy for incomplete beta/gamma functions for p-values.\n- Training pipeline integration:\n  - Data loader supplies cover images; forward pass: caption→compress→modulate→embed→augment→extract→demodulate; compute Lrec and PSNR constraint penalty; backprop only through embedder/extractor. Detach caption/compression from gradient graph.\n  - Log cosine similarities per block, global ρ, and Message Recovery Rate (MRR) on validation transforms.\n\nParameter_Settings:\n- Architectural:\n  - D (vector dimension): 256 (robust default); larger D (384–512) increases robustness at higher compute/memory cost.\n  - Channels in convs: 1.5D (default 384 for D=256). Kernel sizes as in HiDDeN (3×3), 5–8 residual blocks.\n  - Normalization: GroupNorm with groups=32 for stability; LayerNorm on final vector head.\n- PSNR budget:\n  - Target PSNR: 40–42 dB. Start with 40 dB for stronger robustness; raise to 42 dB for higher imperceptibility (expect slight robustness drop).\n  - Watermark residual L2 scaling factor: tune to achieve target PSNR per image; clamp residual in [-α, α], α ∈ [0.005, 0.02].\n- Modulation:\n  - Submessage length L: 12–16 bits (recommend L=12 for speed; L=16 for capacity if GPU allows).\n  - T (number of blocks): T = ceil(N/L), where N is compressed caption length (typically 35–60 bits; mean ≈45 bits after OPT-125m finetune).\n  - Carriers per block: 1 (default). If robustness is insufficient, use multi-carrier summation (2–3 carriers) with per-carrier weight w ∈ [0.3, 0.5], normalize after summation. Multi-carrier improves AWGN robustness but increases inter-carrier interference.\n- Confidence thresholds:\n  - ρ_thresholds guiding acceptance: low (4.2e−13), medium (2.3e−15), high (1.4e−16). High yields near-perfect trust at the cost of discarding more decodes.\n  - For zero-bit presence testing (if used), cosine threshold c ∈ [0.20, 0.35] for D=256; compute ρ0(c) with Eq. (2) to set false-alarm rate.\n- Training:\n  - Image resolution: 256×256 (default); train with random resize/crop to [192–320] then center-crop 256.\n  - Optimizer: AdamW, lr ∈ [1e−4, 3e−4], weight decay ∈ [0, 1e−4], betas (0.9, 0.999). Warmup 5k steps then cosine decay.\n  - Batch size: 16–64 (A100), 8–16 (RTX 3090/4090). Gradient clipping at 1.0.\n  - Steps: 150k–300k; evaluate every 2k steps on validation MRR and average ρ.\n- Augmentations (training noise channel):\n  - Crop keep area: 0.6 (simulate up to 40% crop removal).\n  - Gaussian noise σ ∈ [3/255, 10/255], grayscale p=0.3, JPEG quality q ∈ [40, 70], resize to 128×128 with random interpolation (nearest/bilinear/bicubic).\n  - Malign edits (optional for robustness): include background/style/color edits at low frequency to learn fragility to semantic changes.\n- Caption compression:\n  - BLIP2: use blip2-flan-t5-xl or blip2-opt variants; temperature 0.0 (greedy) for deterministic captions; max tokens 40–60 to cap N.\n  - OPT-125m finetune: 2–5 epochs on 2k–10k BLIP2 captions; lr 2e−5–5e−5; batch size 64–256; teacher-forced next-token prob modeling; achieve entropy reduction from ~75 bits to ~45 bits.\n  - Arithmetic coder: symbol alphabet = UTF-8 bytes; model update every token; ensure encoder/decoder share identical tokenization and model weights.\n- Hardware-dependent:\n  - Mixed precision: use AMP (fp16/bf16) for convs; force float32 for normalization, cosine similarity, and p-value computations to avoid numerical issues.\n  - GPUs: A100/H100: D=256–512, batch=64, multi-carrier; 3090/4090: D=256, batch=16–32; T4/V100: D=256, batch=8–16.\n- Critical vs. robust parameters:\n  - Critical: PSNR target, D, L, PRNG determinism (K handling), caption length cap, normalization of X and Y.\n  - Robust/defaultable: optimizer betas, modest weight decay, augmentation probabilities.\n\nApplication_Conditions:\n- Beneficial scenarios:\n  - Content authentication in social media pipelines where images undergo benign edits: cropping ≤40%, grayscale conversions, downscaling to 128×128, JPEG Q ≥ 50, moderate noise.\n  - Environments needing semantic consistency checks: compare decoded caption to received image for moderation or provenance verification.\n  - Variable payload needs: captions compress to ~35–60 bits; TCCSK handles variable length seamlessly.\n- Hardware requirements:\n  - Embedder/Extractor training: ≥12 GB GPU RAM for D=256, batch ≥16; inference can run on CPU/GPU (captioning benefits from GPU).\n  - Captioning/OPT model: 8–16 GB GPU recommended; CPU inference is possible with smaller OPT-125m.\n  - Storage: keep secret key K secure; store model checkpoints for BLIP2, OPT-125m, Hide-R.\n- Scale considerations:\n  - Training becomes advantageous with ≥50k images to cover augmentation variability; for smaller datasets, freeze early layers or use extensive augmentation.\n  - Larger D and multi-carrier improve robustness but require more compute; scale with hardware uplift.\n- Task compatibility:\n  - Strong for authentication with semantic watermarking; neutral/harmful for tasks requiring heavy local content replacement (local edits may excise watermarked regions).\n  - Prefer SWIFT over SSL/TrustMark when variable-length payload and confidence scoring are required; choose SSL/TrustMark if only fixed 64-bit payload with simpler deployment is needed.\n- Resource constraints:\n  - If latency is critical, precompute captions and payloads; use single-carrier TCCSK and lower D to reduce decode time.\n  - If memory is constrained, set D=256, L=12, batch ≤8, and avoid multi-carrier encoding.\n\nExpected_Outcomes:\n- Quantitative performance (from paper results):\n  - Original images: MRR ≈ 96.6% at 40 dB (95.1% at 42 dB).\n  - Benign transforms: resize to 128×128 MRR ≈ 95.1% (91.3% at 42 dB); grayscale ≈ 95.8% (93.8% at 42 dB); JPEG@50 ≈ 96.2% (93.5% at 42 dB); crop 40% ≈ 82.4% (76.7% at 42 dB); noise ≈ 75.5% (65.4% at 42 dB).\n  - Malign transforms (Emu Edit classes): 63–70% MRR at 40 dB across edit types; confidence filtering can raise trusted-decoding rates (see below).\n- Confidence-driven expectations:\n  - With ρ_threshold = 1.4e−16 (high), expected MRR for accepted decodes is ≈100% on watermarked and benign images, while acceptance rate (CDF) ≈66% (watermarked), 42% (benign), 18% (malign). Medium threshold (2.3e−15) yields ≈100% MRR with higher acceptance (72%/48%/20% respectively).\n- Timeline:\n  - Immediate benefits: confidence metric usable at inference; watermark visibility kept under PSNR budget.\n  - Full robustness emerges after ~100k training steps with augmentation; expect steady MRR increases through 150–300k steps.\n- Trade-offs:\n  - Increasing PSNR (e.g., 42 dB) improves imperceptibility but reduces MRR under heavy benign edits (notably crop/noise).\n  - Larger D and multi-carrier improve robustness but increase computational cost and potential inter-carrier interference.\n  - Stricter ρ thresholds improve trustworthiness but reduce acceptance rate.\n- Benchmarks vs baselines:\n  - Compared to SSL/TrustMark, expect superior resilience on benign transforms like resize/grayscale and higher MRR on malign edits, plus a usable confidence metric (SSL/TrustMark lack it).\n- Failure modes:\n  - Severe local edits that remove the spatial region carrying most watermark energy lead to decoding failure and high ρ (expected/desired fragility to semantic changes).\n  - Extreme crops (>50%), very low JPEG quality (<30), aggressive stylization/color transfer can degrade MRR.\n  - PRNG/key mismatch causes systematic decode failure (high ρ), often mistaken for robustness issues.\n  - Caption ambiguity or long captions (>60 tokens) inflate N and lower reliability; truncation needed.\n- Debugging indicators:\n  - Monitor cosine similarities C_j per block; healthy runs show median C_j ≥ 0.5 under benign transforms at 40 dB.\n  - Distribution of ρ: non-watermarked images should show high ρ; accepted decodes should cluster below the chosen ρ_threshold.\n  - Visual inspection: no visible artifacts under 42 dB; if artifacts appear, the PSNRController is misconfigured.\n  - Track per-transform MRR and acceptance CDF; drops indicate augmentation mismatch with deployment conditions.\n- Hardware-specific outcomes:\n  - On A100/H100 with D=256, batch=64, expect training throughput 150–250 img/s; inference (embed+extract) <10 ms per 256×256 image; captioning adds 30–150 ms depending on BLIP2 size.\n  - On consumer GPUs (3090/4090), throughput ~80–150 img/s; inference times 15–25 ms/image; captioning 60–200 ms.\n  - CPU-only inference for extract/demodulate is feasible (<50 ms/image) but captioning becomes the bottleneck.\n\nQuality Requirements:\n- Troubleshooting:\n  - If ρ is consistently high: verify key K consistency, PRNG determinism, carrier generation shape (2L), normalization of X and Y, and L/T alignment with compression length N.\n  - If artifacts appear: lower residual clamp α or increase PSNR target; apply perceptual loss (LPIPS) penalty <0.1 weight if needed.\n  - If MRR under resize/grayscale is low: include stronger resize/grayscale augmentations during training; ensure extractor is trained on 128×128 downsamples.\n  - If decode collisions (wrong ˆM at low ρ): increase D to 384 or use 2-carrier TCCSK; reduce caption length by truncation and improve OPT-125m finetuning.\n- Validation procedures:\n  - Offline test set (e.g., Emu Edit 2,022 images × transforms): compute MRR by exact caption match after decompression; plot ROC of ρ for watermarked vs non-watermarked images.\n  - Per-transform sweep: evaluate at PSNR {40, 42} dB and ρ thresholds {4.2e−13, 2.3e−15, 1.4e−16}; confirm acceptance/MRR matches expected ranges.\n  - Security validation: attempt spoofing without K; measure false-positive p-values via Eq. (4)/(7); ensure ρ remains high (e.g., >1e−6) under H0.\n- Actionable notes:\n  - Cap captions to 40–50 tokens; enforce greedy decoding for determinism.\n  - Rotate or namespace keys per content publisher; recommend rotation after 10^3–10^4 uses if carriers are reused with same K.\n  - Log and store ρ alongside decoded captions to support automated triage: accept if ρ < 2.3e−15 (medium) or escalate for human review otherwise."
    }
]